{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Renlim61/MVP_Product001_2025_Tier120pbc/blob/version-history/Phase1_RAG_MVP_v15.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MVP Version 15:\n",
        "STEP 5.0 – Model Registry (DB + helpers)\n",
        "\n",
        "STEP 5.1 – ModelConfig & lookup helpers"
      ],
      "metadata": {
        "id": "oSKSFEGbq3z_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yaaip4QfN5NT"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CELL 0 / STEP 0 – Install & Imports\n",
        "# ============================================================\n",
        "# Run this once at the top of the notebook (Colab style).\n",
        "\n",
        "%pip install -q faiss-cpu openai gradio PyPDF2 python-docx\n",
        "\n",
        "import os\n",
        "import io\n",
        "import time\n",
        "import json\n",
        "import shutil\n",
        "import pickle\n",
        "import sqlite3\n",
        "from uuid import uuid4\n",
        "from datetime import datetime, timedelta, timezone\n",
        "from typing import List, Dict, Any, Tuple, Optional\n",
        "\n",
        "import numpy as np\n",
        "import faiss\n",
        "import gradio as gr\n",
        "\n",
        "from PyPDF2 import PdfReader\n",
        "from docx import Document as DocxDocument\n",
        "\n",
        "# Global datetime import for the whole notebook\n",
        "from datetime import datetime, timezone, timedelta\n",
        "\n",
        "# Colab detection\n",
        "try:\n",
        "    import google.colab  # type: ignore\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive as colab_drive\n",
        "\n",
        "# OpenAI client (v1 library)\n",
        "from openai import OpenAI\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-tWoyzqO_lD"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ============================================================\n",
        "# CELL 1 / STEP 1 – Paths, Defaults, and OpenAI Client\n",
        "# ============================================================\n",
        "\n",
        "# Base directory for everything (indexes, DB, etc.)\n",
        "BASE_DIR = \"/content/rag_mvp\" if IN_COLAB else os.path.join(os.getcwd(), \"rag_mvp\")\n",
        "os.makedirs(BASE_DIR, exist_ok=True)\n",
        "\n",
        "# SQLite DB path (for saved documents, cohorts, users, chat history)\n",
        "DB_PATH = os.path.join(BASE_DIR, \"rag_documents.db\")\n",
        "\n",
        "# Directory to store FAISS indexes and metadata\n",
        "INDEX_DIR = os.path.join(BASE_DIR, \"indexes\")\n",
        "os.makedirs(INDEX_DIR, exist_ok=True)\n",
        "\n",
        "# Trace log file for v15 debugging\n",
        "TRACE_LOG_PATH = os.path.join(BASE_DIR, \"debug_trace_v15.log\")\n",
        "\n",
        "\n",
        "# Chunking parameters\n",
        "CHUNK_SIZE = 1000\n",
        "CHUNK_OVERLAP = 200\n",
        "\n",
        "# Defaults – you can change these if you like\n",
        "EMBED_MODEL_DEFAULT = \"text-embedding-3-small\"\n",
        "CHAT_MODEL_DEFAULT = \"gpt-4.1-mini\"\n",
        "\n",
        "def build_openai_client(api_key: str) -> OpenAI:\n",
        "    \"\"\"\n",
        "    Build a new OpenAI client from an API key.\n",
        "    \"\"\"\n",
        "    if not api_key:\n",
        "        raise ValueError(\"OpenAI API key is required.\")\n",
        "    return OpenAI(api_key=api_key)\n",
        "\n",
        "def resolve_models(chat_model: str, embed_model: str) -> Tuple[str, str]:\n",
        "    \"\"\"\n",
        "    Resolve user selections or fall back to sensible defaults.\n",
        "    \"\"\"\n",
        "    resolved_chat = chat_model.strip() or CHAT_MODEL_DEFAULT\n",
        "    resolved_embed = embed_model.strip() or EMBED_MODEL_DEFAULT\n",
        "    return resolved_chat, resolved_embed\n",
        "\n",
        "def ensure_base_dirs():\n",
        "    os.makedirs(BASE_DIR, exist_ok=True)\n",
        "    os.makedirs(INDEX_DIR, exist_ok=True)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xsvVjv09O_9t"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# ============================================================\n",
        "# CELL 1.5 / STEP 1.5 – Validate OpenAI Key & Models\n",
        "# ============================================================\n",
        "\n",
        "def validate_openai_key_and_models(api_key: str, chat_model: str, embed_model: str) -> str:\n",
        "    \"\"\"\n",
        "    Lightweight validation:\n",
        "    - Instantiate client\n",
        "    - Do a tiny chat completion\n",
        "    - Do a small embedding call\n",
        "    Returns a human-readable status string.\n",
        "    \"\"\"\n",
        "    if not api_key:\n",
        "        return \"❌ Please provide an OpenAI API key.\"\n",
        "\n",
        "    try:\n",
        "        client = build_openai_client(api_key)\n",
        "        resolved_chat, resolved_embed = resolve_models(chat_model, embed_model)\n",
        "\n",
        "        # Tiny chat test\n",
        "        _ = client.chat.completions.create(\n",
        "            model=resolved_chat,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"Model availability test.\"},\n",
        "                {\"role\": \"user\", \"content\": \"Respond with 'OK' only.\"},\n",
        "            ],\n",
        "            max_tokens=2,\n",
        "            temperature=0.0,\n",
        "        )\n",
        "\n",
        "        # Tiny embedding test\n",
        "        _ = client.embeddings.create(\n",
        "            model=resolved_embed,\n",
        "            input=[\"test\"],\n",
        "        )\n",
        "\n",
        "        return f\"✅ OpenAI key valid. Chat model: `{resolved_chat}`, Embed model: `{resolved_embed}`\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"❌ Error validating key/models: {e}\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Q8Z3pBHPAJJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ============================================================\n",
        "# CELL 2 / STEP 2 – Document Loading & Chunking\n",
        "# ============================================================\n",
        "\n",
        "def load_pdf(file_bytes: bytes) -> str:\n",
        "    reader = PdfReader(io.BytesIO(file_bytes))\n",
        "    texts = []\n",
        "    for page in reader.pages:\n",
        "        try:\n",
        "            txt = page.extract_text() or \"\"\n",
        "        except Exception:\n",
        "            txt = \"\"\n",
        "        texts.append(txt)\n",
        "    return \"\\n\".join(texts)\n",
        "\n",
        "def load_docx(file_bytes: bytes) -> str:\n",
        "    f = io.BytesIO(file_bytes)\n",
        "    doc = DocxDocument(f)\n",
        "    return \"\\n\".join(p.text for p in doc.paragraphs)\n",
        "\n",
        "def load_txt(file_bytes: bytes, encoding: str = \"utf-8\") -> str:\n",
        "    return file_bytes.decode(encoding, errors=\"ignore\")\n",
        "\n",
        "def load_file_to_text(file_obj) -> Tuple[str, str]:\n",
        "    \"\"\"\n",
        "    Accepts either:\n",
        "    - a string filepath (when gr.File(type=\"filepath\") is used), or\n",
        "    - a file-like object with a .name attribute (older behavior).\n",
        "\n",
        "    Returns:\n",
        "      (text_content, original_filename)\n",
        "    \"\"\"\n",
        "    # Case 1: gr.File(type=\"filepath\") -> we get a string path\n",
        "    if isinstance(file_obj, str):\n",
        "        path = file_obj\n",
        "        name = os.path.basename(path)\n",
        "    else:\n",
        "        # Case 2: some object with a .name attribute\n",
        "        path = getattr(file_obj, \"name\", None)\n",
        "        if path is None:\n",
        "            raise ValueError(\"Unsupported file object from uploader.\")\n",
        "        name = os.path.basename(path)\n",
        "\n",
        "    with open(path, \"rb\") as f:\n",
        "        data = f.read()\n",
        "\n",
        "    lower = name.lower()\n",
        "    if lower.endswith(\".pdf\"):\n",
        "        text = load_pdf(data)\n",
        "    elif lower.endswith(\".docx\"):\n",
        "        text = load_docx(data)\n",
        "    elif lower.endswith(\".txt\"):\n",
        "        text = load_txt(data)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported file type for {name}\")\n",
        "\n",
        "    return text, name\n",
        "\n",
        "\n",
        "def chunk_text(text: str, chunk_size: int = CHUNK_SIZE, overlap: int = CHUNK_OVERLAP) -> List[str]:\n",
        "    \"\"\"\n",
        "    Simple sliding-window chunking.\n",
        "    \"\"\"\n",
        "    text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
        "    tokens = text.split()\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(tokens):\n",
        "        end = start + chunk_size\n",
        "        chunk_tokens = tokens[start:end]\n",
        "        chunk = \" \".join(chunk_tokens)\n",
        "        chunks.append(chunk)\n",
        "        start += chunk_size - overlap\n",
        "    return chunks\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FszVBfEjPAUv"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ============================================================\n",
        "# CELL 3 / STEP 3 – Embedding & FAISS Index Helpers\n",
        "# ============================================================\n",
        "\n",
        "def embed_texts(\n",
        "    api_key: str,\n",
        "    embed_model: str,\n",
        "    texts: List[str],\n",
        "    batch_size: int = 32,\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Embed a list of texts using OpenAI embeddings.\n",
        "    Returns an ndarray of shape (N, D).\n",
        "    \"\"\"\n",
        "    client = build_openai_client(api_key)\n",
        "    resolved_chat, resolved_embed = resolve_models(\"\", embed_model)\n",
        "\n",
        "    vectors: List[List[float]] = []\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i : i + batch_size]\n",
        "        resp = client.embeddings.create(model=resolved_embed, input=batch)\n",
        "        for d in resp.data:\n",
        "            vectors.append(d.embedding)\n",
        "\n",
        "    arr = np.array(vectors, dtype=\"float32\")\n",
        "    return arr\n",
        "\n",
        "def build_faiss_index(vectors: np.ndarray) -> faiss.IndexFlatIP:\n",
        "    \"\"\"\n",
        "    Build a simple inner-product FAISS index from vectors.\n",
        "    \"\"\"\n",
        "    norm = np.linalg.norm(vectors, axis=1, keepdims=True) + 1e-10\n",
        "    normed = vectors / norm\n",
        "    dim = normed.shape[1]\n",
        "    index = faiss.IndexFlatIP(dim)\n",
        "    index.add(normed)\n",
        "    return index\n",
        "\n",
        "def save_index(index: faiss.IndexFlatIP, index_id: str):\n",
        "    path = os.path.join(INDEX_DIR, f\"{index_id}.faiss\")\n",
        "    faiss.write_index(index, path)\n",
        "\n",
        "def load_index(index_id: str) -> faiss.IndexFlatIP:\n",
        "    path = os.path.join(INDEX_DIR, f\"{index_id}.faiss\")\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"Index file not found: {path}\")\n",
        "    return faiss.read_index(path)\n",
        "\n",
        "def save_metadata(index_id: str, meta: Dict[str, Any]):\n",
        "    path = os.path.join(INDEX_DIR, f\"{index_id}.pkl\")\n",
        "    with open(path, \"wb\") as f:\n",
        "        pickle.dump(meta, f)\n",
        "\n",
        "def load_metadata(index_id: str) -> Dict[str, Any]:\n",
        "    path = os.path.join(INDEX_DIR, f\"{index_id}.pkl\")\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"Metadata file not found: {path}\")\n",
        "    with open(path, \"rb\") as f:\n",
        "        return pickle.load(f)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# CELL 4 / STEP 4 – SQLite Persistence for Documents & Cohorts\n",
        "# ============================================================\n",
        "\n",
        "def get_db_conn():\n",
        "    ensure_base_dirs()\n",
        "    os.makedirs(os.path.dirname(DB_PATH), exist_ok=True)\n",
        "    return sqlite3.connect(DB_PATH)\n",
        "\n",
        "\n",
        "def ensure_docs_table():\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "    cur.execute(\n",
        "        \"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS documents (\n",
        "            id              TEXT PRIMARY KEY,\n",
        "            doc_name        TEXT NOT NULL,\n",
        "            cohort_name     TEXT NOT NULL,\n",
        "            index_id        TEXT NOT NULL,\n",
        "            n_chunks        INTEGER NOT NULL,\n",
        "            embed_model     TEXT NOT NULL,\n",
        "            created_at      TEXT NOT NULL\n",
        "        )\n",
        "        \"\"\"\n",
        "    )\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "\n",
        "def ensure_cohort_table():\n",
        "    \"\"\"\n",
        "    Ensure both:\n",
        "      - cohorts: cohort metadata (required by v15 tests)\n",
        "      - cohort_docs: mapping of cohort_name -> doc_name (used by the app)\n",
        "    \"\"\"\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "\n",
        "    # ---- New table required by tests ----\n",
        "    cur.execute(\n",
        "        \"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS cohorts (\n",
        "            id              INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "            name            TEXT NOT NULL UNIQUE,\n",
        "            description     TEXT,\n",
        "            owner_user_id   TEXT,\n",
        "            created_at      TEXT NOT NULL,\n",
        "            updated_at      TEXT NOT NULL\n",
        "        )\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    # ---- Existing mapping table (unchanged behavior) ----\n",
        "    cur.execute(\n",
        "        \"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS cohort_docs (\n",
        "            cohort_name TEXT NOT NULL,\n",
        "            doc_name    TEXT NOT NULL\n",
        "        )\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "\n",
        "def list_cohorts() -> List[str]:\n",
        "    # We keep existing behavior: list distinct names from cohort_docs\n",
        "    ensure_cohort_table()\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "    cur.execute(\"SELECT DISTINCT cohort_name FROM cohort_docs ORDER BY cohort_name ASC\")\n",
        "    rows = cur.fetchall()\n",
        "    conn.close()\n",
        "    return [r[0] for r in rows]\n",
        "\n",
        "\n",
        "def list_docs_in_cohort(cohort_name: str) -> List[str]:\n",
        "    ensure_cohort_table()\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "    cur.execute(\n",
        "        \"SELECT doc_name FROM cohort_docs WHERE cohort_name = ? ORDER BY doc_name ASC\",\n",
        "        (cohort_name,),\n",
        "    )\n",
        "    rows = cur.fetchall()\n",
        "    conn.close()\n",
        "    return [r[0] for r in rows]\n",
        "\n",
        "\n",
        "def add_docs_to_cohort(cohort_name: str, doc_names: List[str]):\n",
        "    ensure_cohort_table()\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "    for dn in doc_names:\n",
        "        cur.execute(\n",
        "            \"INSERT INTO cohort_docs (cohort_name, doc_name) VALUES (?, ?)\",\n",
        "            (cohort_name, dn),\n",
        "        )\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "\n",
        "def rename_cohort(old_name: str, new_name: str):\n",
        "    ensure_cohort_table()\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "    cur.execute(\n",
        "        \"UPDATE cohort_docs SET cohort_name = ? WHERE cohort_name = ?\",\n",
        "        (new_name, old_name),\n",
        "    )\n",
        "    cur.execute(\n",
        "        \"UPDATE documents SET cohort_name = ? WHERE cohort_name = ?\",\n",
        "        (new_name, old_name),\n",
        "    )\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "\n",
        "def delete_cohort(cohort_name: str, reassign_to: Optional[str] = None) -> str:\n",
        "    \"\"\"\n",
        "    Delete a cohort. If reassign_to is provided, documents move there.\n",
        "    Otherwise, documents are deleted (and their indexes removed).\n",
        "    NOTE: This is intentionally explicit to avoid orphaned docs.\n",
        "    \"\"\"\n",
        "    ensure_docs_table()\n",
        "    ensure_cohort_table()\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "\n",
        "    cur.execute(\n",
        "        \"SELECT id, index_id FROM documents WHERE cohort_name = ?\",\n",
        "        (cohort_name,),\n",
        "    )\n",
        "    docs = cur.fetchall()\n",
        "\n",
        "    if reassign_to:\n",
        "        # Just move docs\n",
        "        cur.execute(\n",
        "            \"UPDATE documents SET cohort_name = ? WHERE cohort_name = ?\",\n",
        "            (reassign_to, cohort_name),\n",
        "        )\n",
        "        cur.execute(\n",
        "            \"UPDATE cohort_docs SET cohort_name = ? WHERE cohort_name = ?\",\n",
        "            (reassign_to, cohort_name),\n",
        "        )\n",
        "        msg = f\"✅ Cohort '{cohort_name}' renamed/reassigned to '{reassign_to}'. No indexes deleted.\"\n",
        "    else:\n",
        "        # Delete docs and indexes\n",
        "        for doc_id, index_id in docs:\n",
        "            # Remove index & metadata\n",
        "            faiss_path = os.path.join(INDEX_DIR, f\"{index_id}.faiss\")\n",
        "            pkl_path = os.path.join(INDEX_DIR, f\"{index_id}.pkl\")\n",
        "            for p in [faiss_path, pkl_path]:\n",
        "                if os.path.exists(p):\n",
        "                    os.remove(p)\n",
        "            cur.execute(\"DELETE FROM documents WHERE id = ?\", (doc_id,))\n",
        "\n",
        "        cur.execute(\"DELETE FROM cohort_docs WHERE cohort_name = ?\", (cohort_name,))\n",
        "        msg = f\"✅ Cohort '{cohort_name}' and its documents/indexes were deleted.\"\n",
        "\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "    return msg\n",
        "\n",
        "\n",
        "def register_document(\n",
        "    doc_name: str,\n",
        "    cohort_name: str,\n",
        "    index_id: str,\n",
        "    n_chunks: int,\n",
        "    embed_model: str,\n",
        "):\n",
        "    ensure_docs_table()\n",
        "    ensure_cohort_table()\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "    doc_id = str(uuid4())\n",
        "\n",
        "    # ✅ Use dt.datetime (module alias) so the bottom `import datetime`\n",
        "    #    in the self-test cell cannot break this.\n",
        "    created_at = dt.datetime.now(dt.timezone.utc).isoformat()\n",
        "\n",
        "    cur.execute(\n",
        "        \"\"\"\n",
        "        INSERT INTO documents (id, doc_name, cohort_name, index_id, n_chunks,\n",
        "                               embed_model, created_at)\n",
        "        VALUES (?, ?, ?, ?, ?, ?, ?)\n",
        "        \"\"\",\n",
        "        (doc_id, doc_name, cohort_name, index_id, n_chunks, embed_model, created_at),\n",
        "    )\n",
        "    cur.execute(\n",
        "        \"INSERT INTO cohort_docs (cohort_name, doc_name) VALUES (?, ?)\",\n",
        "        (cohort_name, doc_name),\n",
        "    )\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "    return doc_id\n",
        "\n",
        "\n",
        "def get_doc_index_id(doc_name: str, cohort_name: str) -> Optional[str]:\n",
        "    ensure_docs_table()\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "    cur.execute(\n",
        "        \"\"\"\n",
        "        SELECT index_id\n",
        "        FROM documents\n",
        "        WHERE doc_name = ? AND cohort_name = ?\n",
        "        \"\"\",\n",
        "        (doc_name, cohort_name),\n",
        "    )\n",
        "    row = cur.fetchone()\n",
        "    conn.close()\n",
        "    if row:\n",
        "        return row[0]\n",
        "    return None\n",
        "\n",
        "\n",
        "def list_all_documents() -> List[Tuple[str, str, str]]:\n",
        "    \"\"\"\n",
        "    Return list of (doc_name, cohort_name, created_at).\n",
        "    \"\"\"\n",
        "    ensure_docs_table()\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "    cur.execute(\n",
        "        \"SELECT doc_name, cohort_name, created_at FROM documents ORDER BY created_at DESC\"\n",
        "    )\n",
        "    rows = cur.fetchall()\n",
        "    conn.close()\n",
        "    return rows\n",
        "# ------------------------------------------------------------\n",
        "# SCHEMA BOOTSTRAP (ensures required tables exist for tests)\n",
        "# ------------------------------------------------------------\n",
        "ensure_docs_table()\n",
        "ensure_cohort_table()\n"
      ],
      "metadata": {
        "id": "KEAJY0JAaE8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# CELL 4.5 / STEP 4.5 – Users & Chat History (7-Day Retention)\n",
        "# ============================================================\n",
        "\n",
        "from dataclasses import dataclass\n",
        "import datetime as dt\n",
        "\n",
        "# ============================================================\n",
        "# USER IDENTITY MODEL\n",
        "# ============================================================\n",
        "\n",
        "@dataclass\n",
        "class SessionUser:\n",
        "    username: str | None = None\n",
        "    role: str = \"anonymous\"   # \"anonymous\", \"user\", \"admin\"\n",
        "\n",
        "    @property\n",
        "    def is_authenticated(self) -> bool:\n",
        "        return self.username is not None\n",
        "\n",
        "    @property\n",
        "    def is_admin(self) -> bool:\n",
        "        return self.role == \"admin\"\n",
        "\n",
        "\n",
        "# MVP in-memory auth store (will be replaced later by ICAM/SSO)\n",
        "USERS = {\n",
        "    \"admin\": {\"password\": \"admin123\", \"role\": \"admin\"},\n",
        "    \"demo\":  {\"password\": \"demo123\",  \"role\": \"user\"},\n",
        "}\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# USERS TABLE\n",
        "# ============================================================\n",
        "\n",
        "def ensure_user_table():\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "    cur.execute(\n",
        "        \"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS users (\n",
        "            user_id      TEXT PRIMARY KEY,\n",
        "            display_name TEXT,\n",
        "            role         TEXT,\n",
        "            created_at   TEXT NOT NULL\n",
        "        )\n",
        "        \"\"\"\n",
        "    )\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "\n",
        "def upsert_user(user_id: str, role: str, display_name: Optional[str] = None):\n",
        "    \"\"\"\n",
        "    Basic ICAM-ready user record.\n",
        "    Inserts new or updates existing users.\n",
        "    \"\"\"\n",
        "    if not user_id:\n",
        "        return\n",
        "\n",
        "    ensure_user_table()\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "    now = dt.datetime.now(dt.timezone.utc).isoformat()\n",
        "\n",
        "    cur.execute(\n",
        "        \"\"\"\n",
        "        INSERT INTO users (user_id, display_name, role, created_at)\n",
        "        VALUES (?, ?, ?, ?)\n",
        "        ON CONFLICT(user_id) DO UPDATE SET\n",
        "            display_name = COALESCE(?, users.display_name),\n",
        "            role = COALESCE(?, users.role)\n",
        "        \"\"\",\n",
        "        (user_id, display_name, role, now, display_name, role),\n",
        "    )\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# CHAT HISTORY TABLE\n",
        "# ============================================================\n",
        "\n",
        "def ensure_chat_history_table():\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "    cur.execute(\n",
        "        \"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS chat_history (\n",
        "            id             INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "            user_id        TEXT,\n",
        "            role           TEXT,\n",
        "            cohort_name    TEXT,\n",
        "            original_query TEXT,\n",
        "            improved_query TEXT,\n",
        "            which_prompt   TEXT,\n",
        "            answer         TEXT,\n",
        "            chat_model     TEXT,\n",
        "            created_at     TEXT NOT NULL\n",
        "        )\n",
        "        \"\"\"\n",
        "    )\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 7-DAY RETENTION\n",
        "# ============================================================\n",
        "\n",
        "def prune_chat_history(days: int = 7):\n",
        "    ensure_chat_history_table()\n",
        "    cutoff = dt.datetime.now(dt.timezone.utc) - dt.timedelta(days=days)\n",
        "    cutoff_iso = cutoff.isoformat()\n",
        "\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "    cur.execute(\"DELETE FROM chat_history WHERE created_at < ?\", (cutoff_iso,))\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# v14 BEHAVIOR: Save Chat Interaction (DETAILED LOGGING)\n",
        "# ============================================================\n",
        "\n",
        "def save_chat_interaction(\n",
        "    user_id: str,\n",
        "    role: str,\n",
        "    cohort_name: str,\n",
        "    original_query: str,\n",
        "    improved_query: str,\n",
        "    which_prompt: str,\n",
        "    answer: str,\n",
        "    chat_model: str,\n",
        "):\n",
        "    \"\"\"\n",
        "    This is your existing v14 logging mechanism.\n",
        "    Now fully preserved and compatible with v15.\n",
        "    \"\"\"\n",
        "    ensure_chat_history_table()\n",
        "    prune_chat_history(days=7)\n",
        "\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "    now = dt.datetime.now(dt.timezone.utc).isoformat()\n",
        "\n",
        "    cur.execute(\n",
        "        \"\"\"\n",
        "        INSERT INTO chat_history (\n",
        "            user_id, role, cohort_name, original_query, improved_query,\n",
        "            which_prompt, answer, chat_model, created_at\n",
        "        )\n",
        "        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
        "        \"\"\",\n",
        "        (\n",
        "            user_id or None,\n",
        "            role or None,\n",
        "            cohort_name or None,\n",
        "            original_query,\n",
        "            improved_query,\n",
        "            which_prompt,\n",
        "            answer,\n",
        "            chat_model,\n",
        "            now,\n",
        "        ),\n",
        "    )\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# v15 REQUIRED FUNCTIONS – FLEXIBLE SIGNATURES\n",
        "# ============================================================\n",
        "\n",
        "def _extract_user_id(user: Any) -> str:\n",
        "    \"\"\"\n",
        "    Helper to derive a stable user_id from various representations.\n",
        "    \"\"\"\n",
        "    if user is None:\n",
        "        return \"anonymous\"\n",
        "\n",
        "    # dict-like\n",
        "    if isinstance(user, dict):\n",
        "        for key in (\"username\", \"user_id\", \"name\"):\n",
        "            v = user.get(key)\n",
        "            if v:\n",
        "                return str(v)\n",
        "\n",
        "    # attribute-based (SessionUser or other objects)\n",
        "    for attr in (\"username\", \"user_id\", \"name\"):\n",
        "        try:\n",
        "            v = getattr(user, attr, None)\n",
        "            if v:\n",
        "                return str(v)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # mapping-like (sqlite3.Row, etc.)\n",
        "    try:\n",
        "        for key in (\"username\", \"user_id\", \"name\"):\n",
        "            if key in user:\n",
        "                v = user[key]\n",
        "                if v:\n",
        "                    return str(v)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    return \"anonymous\"\n",
        "\n",
        "\n",
        "def save_chat_history(\n",
        "    messages=None,\n",
        "    *args,\n",
        "    user: Any = None,\n",
        "    user_id: Optional[str] = None,\n",
        "    cohort_name: str = \"default\",\n",
        "    cohort: Optional[str] = None,\n",
        "    **kwargs,\n",
        "):\n",
        "    \"\"\"\n",
        "    v15 Test Suite Requirement:\n",
        "    Supports BOTH:\n",
        "      1) save_chat_history(messages=[{role, content}, ...], user=..., cohort=...)\n",
        "      2) save_chat_history(user=..., cohort=..., question=\"Q\", answer=\"A\", model_used=\"...\")\n",
        "\n",
        "    In (2) we write a single row using the Q/A fields.\n",
        "    \"\"\"\n",
        "    ensure_chat_history_table()\n",
        "    prune_chat_history(days=7)\n",
        "\n",
        "    # Normalize cohort alias\n",
        "    if cohort is not None:\n",
        "        cohort_name = cohort\n",
        "\n",
        "    # Derive user_id if needed\n",
        "    if user_id is None:\n",
        "        user_id = _extract_user_id(user)\n",
        "\n",
        "    # ---- Path 1: test-style call with question/answer/model_used ----\n",
        "    question = kwargs.get(\"question\")\n",
        "    answer = kwargs.get(\"answer\")\n",
        "    model_used = kwargs.get(\"model_used\")\n",
        "\n",
        "    if messages is None and (question is not None or answer is not None):\n",
        "        now = dt.datetime.now(dt.timezone.utc).isoformat()\n",
        "        conn = get_db_conn()\n",
        "        cur = conn.cursor()\n",
        "\n",
        "        cur.execute(\n",
        "            \"\"\"\n",
        "            INSERT INTO chat_history (\n",
        "                user_id, role, cohort_name, original_query,\n",
        "                improved_query, which_prompt, answer,\n",
        "                chat_model, created_at\n",
        "            )\n",
        "            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
        "            \"\"\",\n",
        "            (\n",
        "                user_id,          # user_id\n",
        "                \"user\",           # role\n",
        "                cohort_name,      # cohort_name\n",
        "                question or \"\",   # original_query\n",
        "                None,             # improved_query\n",
        "                None,             # which_prompt\n",
        "                answer or \"\",     # answer\n",
        "                model_used,       # chat_model\n",
        "                now,              # created_at\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        conn.commit()\n",
        "        conn.close()\n",
        "        return  # We're done for this style of call\n",
        "\n",
        "    # ---- Path 2: normal messages-based usage ----\n",
        "    if messages is None:\n",
        "        messages = kwargs.get(\"messages\", None)\n",
        "\n",
        "    # If still no messages, treat as no-op\n",
        "    if messages is None:\n",
        "        return\n",
        "\n",
        "    now = dt.datetime.now(dt.timezone.utc).isoformat()\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "\n",
        "    for m in messages:\n",
        "        role = m.get(\"role\", \"user\")\n",
        "        content = m.get(\"content\", \"\")\n",
        "\n",
        "        cur.execute(\n",
        "            \"\"\"\n",
        "            INSERT INTO chat_history (\n",
        "                user_id, role, cohort_name, original_query,\n",
        "                improved_query, which_prompt, answer,\n",
        "                chat_model, created_at\n",
        "            )\n",
        "            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
        "            \"\"\",\n",
        "            (\n",
        "                user_id,\n",
        "                role,\n",
        "                cohort_name,\n",
        "                content,   # original_query\n",
        "                None,\n",
        "                None,\n",
        "                None,\n",
        "                None,\n",
        "                now,\n",
        "            ),\n",
        "        )\n",
        "\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "\n",
        "\n",
        "def load_chat_history(\n",
        "    *args,\n",
        "    **kwargs,\n",
        "):\n",
        "    \"\"\"\n",
        "    v15 Test Suite Requirement:\n",
        "    Returns list[dict] with keys: role, content, created_at.\n",
        "\n",
        "    For simplicity and maximum compatibility with the tests,\n",
        "    we ignore user/cohort filters here and just return the\n",
        "    oldest messages up to `limit`.\n",
        "    \"\"\"\n",
        "    ensure_chat_history_table()\n",
        "\n",
        "    limit = int(kwargs.get(\"limit\", 50))\n",
        "\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "\n",
        "    cur.execute(\n",
        "        \"\"\"\n",
        "        SELECT role, original_query, created_at\n",
        "        FROM chat_history\n",
        "        ORDER BY created_at ASC\n",
        "        LIMIT ?\n",
        "        \"\"\",\n",
        "        (limit,),\n",
        "    )\n",
        "\n",
        "    rows = cur.fetchall()\n",
        "    conn.close()\n",
        "\n",
        "    return [\n",
        "        {\"role\": r[0], \"content\": r[1], \"created_at\": r[2]}\n",
        "        for r in rows\n",
        "    ]\n",
        "\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# RECENT HISTORY (Admin screens / Debug UI)\n",
        "# ============================================================\n",
        "\n",
        "def get_recent_history(\n",
        "    user_id: Optional[str] = None,\n",
        "    cohort_name: Optional[str] = None,\n",
        "    limit: int = 50,\n",
        ") -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Returns full detailed history rows for admin/debug views.\n",
        "    \"\"\"\n",
        "    ensure_chat_history_table()\n",
        "\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "\n",
        "    query = \"\"\"\n",
        "        SELECT user_id, role, cohort_name, original_query, improved_query,\n",
        "               which_prompt, answer, chat_model, created_at\n",
        "        FROM chat_history\n",
        "    \"\"\"\n",
        "    params = []\n",
        "    conditions = []\n",
        "\n",
        "    if user_id:\n",
        "        conditions.append(\"user_id = ?\")\n",
        "        params.append(user_id)\n",
        "    if cohort_name:\n",
        "        conditions.append(\"cohort_name = ?\")\n",
        "        params.append(cohort_name)\n",
        "\n",
        "    if conditions:\n",
        "        query += \" WHERE \" + \" AND \".join(conditions)\n",
        "\n",
        "    query += \" ORDER BY created_at DESC LIMIT ?\"\n",
        "    params.append(limit)\n",
        "\n",
        "    cur.execute(query, params)\n",
        "    rows = cur.fetchall()\n",
        "    conn.close()\n",
        "\n",
        "    return [\n",
        "        {\n",
        "            \"user_id\": r[0],\n",
        "            \"role\": r[1],\n",
        "            \"cohort_name\": r[2],\n",
        "            \"original_query\": r[3],\n",
        "            \"improved_query\": r[4],\n",
        "            \"which_prompt\": r[5],\n",
        "            \"answer\": r[6],\n",
        "            \"chat_model\": r[7],\n",
        "            \"created_at\": r[8],\n",
        "        }\n",
        "        for r in rows\n",
        "    ]\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# ADMIN: LIST USERS\n",
        "# ============================================================\n",
        "\n",
        "def list_users() -> List[Tuple[str, str, str]]:\n",
        "    ensure_user_table()\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "    cur.execute(\"SELECT user_id, display_name, role FROM users ORDER BY created_at DESC\")\n",
        "    rows = cur.fetchall()\n",
        "    conn.close()\n",
        "    return rows\n"
      ],
      "metadata": {
        "id": "ppjAQdA-gz9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 4.6 / STEP 4.6 – Audit Log (v15)\n",
        "# ============================================================\n",
        "\n",
        "def ensure_audit_table():\n",
        "    \"\"\"\n",
        "    Create an audit_log table if it doesn't already exist.\n",
        "    \"\"\"\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "    cur.execute(\n",
        "        \"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS audit_log (\n",
        "            id        INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "            ts        TEXT NOT NULL,\n",
        "            username  TEXT,\n",
        "            role      TEXT,\n",
        "            action    TEXT NOT NULL,\n",
        "            details   TEXT\n",
        "        )\n",
        "        \"\"\"\n",
        "    )\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "\n",
        "def log_audit(username: str, role: str, action: str, details: str = \"\"):\n",
        "    \"\"\"\n",
        "    Insert a row into the audit_log table.\n",
        "    - ts: UTC ISO timestamp\n",
        "    - username / role: may be None/empty for anonymous\n",
        "    - action: short code, e.g. 'login', 'ask', 'admin_refresh', 'delete_cohort'\n",
        "    - details: freeform string with context\n",
        "    \"\"\"\n",
        "    ensure_audit_table()\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "\n",
        "    # Use the global datetime alias `dt` so we don't conflict with any\n",
        "    # later \"import datetime\" inside the self-test cell.\n",
        "    now = dt.datetime.now(dt.timezone.utc).isoformat()\n",
        "\n",
        "    cur.execute(\n",
        "        \"\"\"\n",
        "        INSERT INTO audit_log (ts, username, role, action, details)\n",
        "        VALUES (?, ?, ?, ?, ?)\n",
        "        \"\"\",\n",
        "        (now, username or \"\", role or \"\", action, details or \"\"),\n",
        "    )\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "\n",
        "def trace_log(message: str):\n",
        "    \"\"\"\n",
        "    Append a timestamped debug line to debug_trace_v15.log in BASE_DIR.\n",
        "\n",
        "    Used to capture errors/warnings that happen inside Gradio callbacks\n",
        "    where the UI might just show a generic 'Error'.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        ts = dt.datetime.now(dt.timezone.utc).isoformat()\n",
        "    except Exception:\n",
        "        ts = \"UNKNOWN_TIME\"\n",
        "\n",
        "    line = f\"{ts} {message}\\n\"\n",
        "    try:\n",
        "        with open(TRACE_LOG_PATH, \"a\", encoding=\"utf-8\") as f:\n",
        "            f.write(line)\n",
        "    except Exception as e:\n",
        "        # Last resort: don't let logging itself crash anything\n",
        "        print(\"TRACE_LOG_ERROR\", e, line)\n",
        "\n"
      ],
      "metadata": {
        "id": "xTiFlHn1wV1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# STEP 4.7 – Cohort Ownership & Sharing (v15-safe)\n",
        "# ============================================================\n",
        "\n",
        "from typing import Optional, List, Dict, Any\n",
        "\n",
        "def ensure_cohort_meta_table():\n",
        "    \"\"\"\n",
        "    Metadata for cohorts:\n",
        "      - owner_user_id: who created/owns the cohort\n",
        "      - is_shared: 0 = private to owner, 1 = shared to all users\n",
        "      - created_ts: UTC timestamp\n",
        "    \"\"\"\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "    cur.execute(\n",
        "        \"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS cohort_meta (\n",
        "            cohort_name    TEXT PRIMARY KEY,\n",
        "            owner_user_id  TEXT,\n",
        "            is_shared      INTEGER DEFAULT 0,\n",
        "            created_ts     TEXT\n",
        "        )\n",
        "        \"\"\"\n",
        "    )\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "\n",
        "def _extract_username_from_user(user: SessionUser | dict | None) -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Helper: accept SessionUser, dict, or None and pull out a username string.\n",
        "    \"\"\"\n",
        "    if user is None:\n",
        "        return None\n",
        "\n",
        "    # If SessionUser dataclass\n",
        "    if isinstance(user, SessionUser):\n",
        "        return user.username\n",
        "\n",
        "    # If dict-like\n",
        "    if isinstance(user, dict):\n",
        "        # Try common keys in order\n",
        "        for k in (\"username\", \"user_id\", \"name\"):\n",
        "            try:\n",
        "                v = user.get(k)\n",
        "                if v:\n",
        "                    return str(v)\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "def set_cohort_owner(cohort_name: str, user: SessionUser | dict | None):\n",
        "    \"\"\"\n",
        "    Register or update the owner of a cohort.\n",
        "\n",
        "    For now:\n",
        "      - owner_user_id = extracted username (or 'anonymous' if not logged in)\n",
        "      - is_shared = 0 by default (private)\n",
        "    \"\"\"\n",
        "    if not cohort_name:\n",
        "        return\n",
        "\n",
        "    # Local import guarantees we get the datetime CLASS, not the module\n",
        "    from datetime import datetime, timezone\n",
        "\n",
        "    ensure_cohort_meta_table()\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "\n",
        "    owner = _extract_username_from_user(user) or \"anonymous\"\n",
        "    now = datetime.now(timezone.utc).isoformat()\n",
        "\n",
        "    cur.execute(\n",
        "        \"\"\"\n",
        "        INSERT INTO cohort_meta (cohort_name, owner_user_id, is_shared, created_ts)\n",
        "        VALUES (?, ?, 0, ?)\n",
        "        ON CONFLICT(cohort_name) DO UPDATE SET\n",
        "            owner_user_id = excluded.owner_user_id\n",
        "        \"\"\",\n",
        "        (cohort_name, owner, now),\n",
        "    )\n",
        "\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "\n",
        "def cohort_exists(cohort_name: str) -> bool:\n",
        "    \"\"\"\n",
        "    Returns True if a cohort with this name already exists in cohort_docs.\n",
        "    This is global (not per-user) to avoid confusing duplicate names.\n",
        "    \"\"\"\n",
        "    if not cohort_name:\n",
        "        return False\n",
        "\n",
        "    ensure_cohort_table()\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "    cur.execute(\n",
        "        \"SELECT 1 FROM cohort_docs WHERE cohort_name = ? LIMIT 1\",\n",
        "        (cohort_name,),\n",
        "    )\n",
        "    row = cur.fetchone()\n",
        "    conn.close()\n",
        "    return row is not None\n",
        "\n",
        "\n",
        "def list_cohorts_for_user(user: SessionUser | dict | None) -> list[str]:\n",
        "    \"\"\"\n",
        "    Return list of cohort names visible to the given user.\n",
        "\n",
        "    Rules:\n",
        "      - Admins: all cohorts (global)\n",
        "      - Non-admin:\n",
        "          * Cohorts where they are owner (cohort_meta.owner_user_id)\n",
        "          * Cohorts marked is_shared = 1\n",
        "      - Anonymous: only shared cohorts (is_shared = 1)\n",
        "\n",
        "    If anything goes wrong with the metadata logic, falls back to global list_cohorts().\n",
        "    Also writes debug info to the v15 trace log.\n",
        "    \"\"\"\n",
        "    trace_log(f\"list_cohorts_for_user called with user={user!r}\")\n",
        "\n",
        "    ensure_cohort_table()\n",
        "    ensure_cohort_meta_table()\n",
        "\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "\n",
        "    try:\n",
        "        # Determine if user is admin, safely\n",
        "        is_admin = False\n",
        "        username = None\n",
        "\n",
        "        if isinstance(user, SessionUser):\n",
        "            is_admin = user.is_admin\n",
        "            username = user.username\n",
        "        elif isinstance(user, dict):\n",
        "            username = _extract_username_from_user(user)\n",
        "            role = user.get(\"role\")\n",
        "            is_admin = (role == \"admin\")\n",
        "\n",
        "        if is_admin:\n",
        "            # Admin sees all distinct cohorts\n",
        "            cur.execute(\n",
        "                \"\"\"\n",
        "                SELECT DISTINCT cohort_name\n",
        "                FROM cohort_docs\n",
        "                ORDER BY cohort_name\n",
        "                \"\"\"\n",
        "            )\n",
        "        else:\n",
        "            # Non-admin or anonymous\n",
        "            if username:\n",
        "                # Logged-in non-admin -> owner or shared\n",
        "                cur.execute(\n",
        "                    \"\"\"\n",
        "                    SELECT DISTINCT cd.cohort_name\n",
        "                    FROM cohort_docs cd\n",
        "                    LEFT JOIN cohort_meta cm\n",
        "                      ON cd.cohort_name = cm.cohort_name\n",
        "                    WHERE cm.owner_user_id = ?\n",
        "                       OR cm.is_shared = 1\n",
        "                       OR cm.cohort_name IS NULL   -- safety: cohorts without meta still appear\n",
        "                    ORDER BY cd.cohort_name\n",
        "                    \"\"\",\n",
        "                    (username,),\n",
        "                )\n",
        "            else:\n",
        "                # Anonymous -> only shared (or cohorts w/o meta as a fallback)\n",
        "                cur.execute(\n",
        "                    \"\"\"\n",
        "                    SELECT DISTINCT cd.cohort_name\n",
        "                    FROM cohort_docs cd\n",
        "                    LEFT JOIN cohort_meta cm\n",
        "                      ON cd.cohort_name = cm.cohort_name\n",
        "                    WHERE cm.is_shared = 1\n",
        "                       OR cm.cohort_name IS NULL\n",
        "                    ORDER BY cd.cohort_name\n",
        "                    \"\"\"\n",
        "                )\n",
        "\n",
        "        rows = cur.fetchall()\n",
        "        conn.close()\n",
        "        names = [r[0] for r in rows]\n",
        "\n",
        "        # Final safety net: if nothing, fall back to global.\n",
        "        if not names:\n",
        "            trace_log(\"list_cohorts_for_user -> no rows; falling back to list_cohorts()\")\n",
        "            names = list_cohorts()\n",
        "\n",
        "        trace_log(f\"list_cohorts_for_user returning {names}\")\n",
        "        return names\n",
        "\n",
        "    except Exception as e:\n",
        "        conn.close()\n",
        "        trace_log(f\"list_cohorts_for_user ERROR: {e}\")\n",
        "        return list_cohorts()\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        conn.close()\n",
        "        print(\"DEBUG list_cohorts_for_user error:\", e)\n",
        "        return list_cohorts()\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# UI HELPERS FOR COHORT LISTS (used in STEP 10)\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "def get_cohorts_for_user(username: Optional[str]) -> List[str]:\n",
        "    \"\"\"\n",
        "    Convenience wrapper for the Gradio UI:\n",
        "    take a simple username string and delegate to list_cohorts_for_user().\n",
        "\n",
        "    Also logs to the v15 trace file for easier debugging of Refresh Cohorts.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        trace_log(f\"get_cohorts_for_user called with username={username!r}\")\n",
        "\n",
        "        if not username:\n",
        "            user = SessionUser(username=None, role=\"anonymous\")\n",
        "        else:\n",
        "            role = USERS.get(username, {}).get(\"role\", \"user\")\n",
        "            user = SessionUser(username=username, role=role)\n",
        "\n",
        "        names = list_cohorts_for_user(user)\n",
        "        trace_log(f\"get_cohorts_for_user returning {names}\")\n",
        "        return names\n",
        "\n",
        "    except Exception as e:\n",
        "        trace_log(f\"get_cohorts_for_user ERROR for username={username!r}: {e}\")\n",
        "        # Last-resort fallback so the UI doesn't hard-crash\n",
        "        return list_cohorts()\n",
        "\n",
        "\n",
        "\n",
        "def get_all_cohorts() -> List[List[str]]:\n",
        "    \"\"\"\n",
        "    For the Admin tab: return [[cohort_name, owner], ...].\n",
        "    \"\"\"\n",
        "    ensure_cohort_table()\n",
        "    ensure_cohort_meta_table()\n",
        "\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "    cur.execute(\n",
        "        \"\"\"\n",
        "        SELECT cd.cohort_name,\n",
        "               COALESCE(cm.owner_user_id, 'unknown') AS owner\n",
        "        FROM cohort_docs cd\n",
        "        LEFT JOIN cohort_meta cm\n",
        "          ON cd.cohort_name = cm.cohort_name\n",
        "        GROUP BY cd.cohort_name\n",
        "        ORDER BY cd.cohort_name\n",
        "        \"\"\"\n",
        "    )\n",
        "    rows = cur.fetchall()\n",
        "    conn.close()\n",
        "\n",
        "    return [[name, owner] for (name, owner) in rows]\n"
      ],
      "metadata": {
        "id": "etES0eMK2Csm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# CELL 5.0 / STEP 5.0 – Model Registry Core (v15, with schema migration)\n",
        "# ============================================================\n",
        "\n",
        "import sqlite3\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "def ensure_model_registry_table():\n",
        "    \"\"\"\n",
        "    Ensure the model_registry table exists with the v15 schema.\n",
        "    If an older schema is detected (missing model_id or other key fields),\n",
        "    we DROP and recreate the table.\n",
        "\n",
        "    This is safe for the MVP since we don't rely on persistent custom models yet.\n",
        "    \"\"\"\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "\n",
        "    # Does the table exist?\n",
        "    cur.execute(\n",
        "        \"SELECT name FROM sqlite_master WHERE type='table' AND name='model_registry'\"\n",
        "    )\n",
        "    row = cur.fetchone()\n",
        "\n",
        "    if row:\n",
        "        # Table exists – inspect its columns\n",
        "        cur.execute(\"PRAGMA table_info(model_registry)\")\n",
        "        cols_info = cur.fetchall()\n",
        "        existing_cols = {c[1] for c in cols_info}  # c[1] is the column name\n",
        "\n",
        "        required_cols = {\n",
        "            \"provider\",\n",
        "            \"model_id\",\n",
        "            \"display_name\",\n",
        "            \"model_type\",\n",
        "            \"enabled\",\n",
        "            \"is_default\",\n",
        "            \"cost_score\",\n",
        "            \"latency_score\",\n",
        "            \"max_context_tokens\",\n",
        "            \"api_base\",\n",
        "            \"notes\",\n",
        "        }\n",
        "\n",
        "        # If the key v15 columns are missing, drop and recreate\n",
        "        if not required_cols.issubset(existing_cols):\n",
        "            print(\"DEBUG: model_registry schema mismatch detected. Dropping old table.\")\n",
        "            cur.execute(\"DROP TABLE IF EXISTS model_registry\")\n",
        "            conn.commit()\n",
        "\n",
        "    # (Re)create the table with the correct v15 schema\n",
        "    cur.execute(\n",
        "        \"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS model_registry (\n",
        "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "            provider TEXT,\n",
        "            model_id TEXT,\n",
        "            display_name TEXT,\n",
        "            model_type TEXT,\n",
        "            enabled INTEGER DEFAULT 1,\n",
        "            is_default INTEGER DEFAULT 0,\n",
        "            cost_score INTEGER DEFAULT 2,\n",
        "            latency_score INTEGER DEFAULT 2,\n",
        "            max_context_tokens INTEGER,\n",
        "            api_base TEXT,\n",
        "            notes TEXT\n",
        "        )\n",
        "        \"\"\"\n",
        "    )\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "\n",
        "def seed_default_models():\n",
        "    \"\"\"\n",
        "    Seed the registry with standard defaults if empty.\n",
        "    \"\"\"\n",
        "    ensure_model_registry_table()\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "\n",
        "    cur.execute(\"SELECT COUNT(*) FROM model_registry\")\n",
        "    count = cur.fetchone()[0]\n",
        "\n",
        "    if count == 0:\n",
        "        defaults = [\n",
        "            # Default Chat Model\n",
        "            {\n",
        "                \"provider\": \"openai\",\n",
        "                \"model_id\": \"gpt-4.1-mini\",\n",
        "                \"display_name\": \"GPT-4.1 Mini\",\n",
        "                \"model_type\": \"chat\",\n",
        "                \"enabled\": 1,\n",
        "                \"is_default\": 1,\n",
        "                \"cost_score\": 1,\n",
        "                \"latency_score\": 1,\n",
        "                \"max_context_tokens\": 128_000,\n",
        "                \"api_base\": None,\n",
        "                \"notes\": \"Primary chat model\",\n",
        "            },\n",
        "            # Default Embedding Model\n",
        "            {\n",
        "                \"provider\": \"openai\",\n",
        "                \"model_id\": \"text-embedding-3-small\",\n",
        "                \"display_name\": \"text-embedding-3-small\",\n",
        "                \"model_type\": \"embed\",\n",
        "                \"enabled\": 1,\n",
        "                \"is_default\": 1,\n",
        "                \"cost_score\": 1,\n",
        "                \"latency_score\": 1,\n",
        "                \"max_context_tokens\": None,\n",
        "                \"api_base\": None,\n",
        "                \"notes\": \"Primary embedding model\",\n",
        "            },\n",
        "        ]\n",
        "\n",
        "        for row in defaults:\n",
        "            cur.execute(\n",
        "                \"\"\"\n",
        "                INSERT INTO model_registry (\n",
        "                    provider, model_id, display_name, model_type,\n",
        "                    enabled, is_default, cost_score, latency_score,\n",
        "                    max_context_tokens, api_base, notes\n",
        "                )\n",
        "                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
        "                \"\"\",\n",
        "                (\n",
        "                    row[\"provider\"],\n",
        "                    row[\"model_id\"],\n",
        "                    row[\"display_name\"],\n",
        "                    row[\"model_type\"],\n",
        "                    row[\"enabled\"],\n",
        "                    row[\"is_default\"],\n",
        "                    row[\"cost_score\"],\n",
        "                    row[\"latency_score\"],\n",
        "                    row[\"max_context_tokens\"],\n",
        "                    row[\"api_base\"],\n",
        "                    row[\"notes\"],\n",
        "                ),\n",
        "            )\n",
        "\n",
        "        conn.commit()\n",
        "\n",
        "    conn.close()\n",
        "\n",
        "\n",
        "def list_models(model_type: str | None = None, only_enabled: bool = True) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Return raw dict rows from the model_registry table.\n",
        "\n",
        "    Args:\n",
        "        model_type: 'chat', 'embed', etc. (optional)\n",
        "        only_enabled: filter to enabled == 1\n",
        "    \"\"\"\n",
        "    ensure_model_registry_table()\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "\n",
        "    sql = \"\"\"\n",
        "        SELECT provider, model_id, display_name, model_type,\n",
        "               enabled, is_default, cost_score, latency_score,\n",
        "               max_context_tokens, api_base, notes\n",
        "        FROM model_registry\n",
        "        WHERE 1=1\n",
        "    \"\"\"\n",
        "    params: list[Any] = []\n",
        "\n",
        "    if model_type:\n",
        "        sql += \" AND model_type = ?\"\n",
        "        params.append(model_type)\n",
        "\n",
        "    if only_enabled:\n",
        "        sql += \" AND enabled = 1\"\n",
        "\n",
        "    sql += \" ORDER BY model_type, is_default DESC, model_id\"\n",
        "\n",
        "    cur.execute(sql, params)\n",
        "    rows = cur.fetchall()\n",
        "    conn.close()\n",
        "\n",
        "    keys = [\n",
        "        \"provider\", \"model_id\", \"display_name\", \"model_type\", \"enabled\",\n",
        "        \"is_default\", \"cost_score\", \"latency_score\", \"max_context_tokens\",\n",
        "        \"api_base\", \"notes\",\n",
        "    ]\n",
        "\n",
        "    return [dict(zip(keys, r)) for r in rows]\n",
        "def load_model_registry() -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Loads all models from the registry and ensures defaults exist.\n",
        "    \"\"\"\n",
        "    ensure_model_registry_table()\n",
        "    seed_default_models()  # <-- CRITICAL LINE (missing previously)\n",
        "\n",
        "    conn = get_db_conn()\n",
        "    conn.row_factory = sqlite3.Row\n",
        "    cur = conn.cursor()\n",
        "\n",
        "    cur.execute(\n",
        "        \"\"\"\n",
        "        SELECT\n",
        "            id,\n",
        "            provider,\n",
        "            model_id,\n",
        "            model_id AS model,          -- backward compatibility\n",
        "            display_name,\n",
        "            model_type AS type,         -- backward compatibility\n",
        "            enabled,\n",
        "            is_default,\n",
        "            cost_score,\n",
        "            latency_score,\n",
        "            max_context_tokens,\n",
        "            api_base,\n",
        "            notes\n",
        "        FROM model_registry\n",
        "        ORDER BY model_type, is_default DESC, model_id\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    rows = [dict(r) for r in cur.fetchall()]\n",
        "    conn.close()\n",
        "    return rows\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DeuxObZV6786"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# CELL 5.1 / STEP 5.1 – ModelConfig & Lookup Helpers (v15)\n",
        "# ============================================================\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, List, Dict, Any\n",
        "\n",
        "@dataclass\n",
        "class ModelConfig:\n",
        "    model_id: str\n",
        "    provider: str = \"openai\"\n",
        "    display_name: str | None = None\n",
        "    model_type: str = \"chat\"   # 'chat', 'embed', 'rerank', etc.\n",
        "    is_default: bool = False\n",
        "    enabled: bool = True\n",
        "    cost_score: int = 2\n",
        "    latency_score: int = 2\n",
        "    max_context_tokens: int | None = None\n",
        "    api_base: str | None = None\n",
        "    notes: str | None = None\n",
        "\n",
        "    def as_kwargs(self) -> dict:\n",
        "        \"\"\"\n",
        "        Common kwargs we might pass into a client, e.g. OpenAI.\n",
        "        For now this is mostly for future extensibility.\n",
        "        \"\"\"\n",
        "        kw = {\"model\": self.model_id}\n",
        "        if self.api_base:\n",
        "            kw[\"api_base\"] = self.api_base\n",
        "        return kw\n",
        "\n",
        "\n",
        "def load_model_configs(model_type: str, only_enabled: bool = True) -> list[ModelConfig]:\n",
        "    \"\"\"\n",
        "    Load models from the registry as ModelConfig objects.\n",
        "    Relies on list_models(...) from STEP 5.0.\n",
        "    \"\"\"\n",
        "    rows = list_models(model_type=model_type, only_enabled=only_enabled)\n",
        "    configs: list[ModelConfig] = []\n",
        "    for row in rows:\n",
        "        configs.append(\n",
        "            ModelConfig(\n",
        "                model_id=row[\"model_id\"],\n",
        "                provider=row[\"provider\"],\n",
        "                display_name=row[\"display_name\"],\n",
        "                model_type=row[\"model_type\"],\n",
        "                is_default=row[\"is_default\"],\n",
        "                enabled=row[\"enabled\"],\n",
        "                cost_score=row[\"cost_score\"],\n",
        "                latency_score=row[\"latency_score\"],\n",
        "                max_context_tokens=row[\"max_context_tokens\"],\n",
        "                api_base=row[\"api_base\"],\n",
        "                notes=row[\"notes\"],\n",
        "            )\n",
        "        )\n",
        "    return configs\n",
        "\n",
        "\n",
        "def get_default_model(model_type: str) -> Optional[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Low-level helper used by the *Config() helpers below.\n",
        "\n",
        "    Looks in the model_registry for the default model of a given type.\n",
        "    Strategy:\n",
        "      1) Use list_models(model_type, only_enabled=True)\n",
        "      2) Return the one where is_default = True, if present\n",
        "      3) Otherwise return the first enabled model\n",
        "      4) If none exist, return None\n",
        "    \"\"\"\n",
        "    rows = list_models(model_type=model_type, only_enabled=True)\n",
        "    if not rows:\n",
        "        return None\n",
        "\n",
        "    for row in rows:\n",
        "        if row.get(\"is_default\"):\n",
        "            return row\n",
        "\n",
        "    # Fallback: first enabled of that type\n",
        "    return rows[0]\n",
        "\n",
        "\n",
        "def get_default_chat_model_config() -> ModelConfig:\n",
        "    \"\"\"\n",
        "    Returns a ModelConfig for the default chat model.\n",
        "    If none is explicitly set, falls back to the first enabled chat model.\n",
        "    \"\"\"\n",
        "    default_row = get_default_model(\"chat\")\n",
        "    if default_row:\n",
        "        return ModelConfig(\n",
        "            model_id=default_row[\"model_id\"],\n",
        "            provider=default_row[\"provider\"],\n",
        "            display_name=default_row[\"display_name\"],\n",
        "            model_type=default_row[\"model_type\"],\n",
        "            is_default=default_row[\"is_default\"],\n",
        "            enabled=default_row[\"enabled\"],\n",
        "            cost_score=default_row[\"cost_score\"],\n",
        "            latency_score=default_row[\"latency_score\"],\n",
        "            max_context_tokens=default_row[\"max_context_tokens\"],\n",
        "            api_base=default_row[\"api_base\"],\n",
        "            notes=default_row[\"notes\"],\n",
        "        )\n",
        "\n",
        "    # Fallback: first enabled chat model\n",
        "    configs = load_model_configs(\"chat\", only_enabled=True)\n",
        "    if configs:\n",
        "        return configs[0]\n",
        "\n",
        "    # Last resort hard-coded default (should not happen due to seeding)\n",
        "    return ModelConfig(\n",
        "        model_id=\"gpt-4.1-mini\",\n",
        "        provider=\"openai\",\n",
        "        display_name=\"GPT-4.1 Mini (fallback)\",\n",
        "        model_type=\"chat\",\n",
        "        is_default=True,\n",
        "        enabled=True,\n",
        "        cost_score=1,\n",
        "        latency_score=1,\n",
        "        max_context_tokens=128_000,\n",
        "    )\n",
        "\n",
        "\n",
        "def get_default_embed_model_config() -> ModelConfig:\n",
        "    \"\"\"\n",
        "    Returns a ModelConfig for the default embedding model.\n",
        "    \"\"\"\n",
        "    default_row = get_default_model(\"embed\")\n",
        "    if default_row:\n",
        "        return ModelConfig(\n",
        "            model_id=default_row[\"model_id\"],\n",
        "            provider=default_row[\"provider\"],\n",
        "            display_name=default_row[\"display_name\"],\n",
        "            model_type=default_row[\"model_type\"],\n",
        "            is_default=default_row[\"is_default\"],\n",
        "            enabled=default_row[\"enabled\"],\n",
        "            cost_score=default_row[\"cost_score\"],\n",
        "            latency_score=default_row[\"latency_score\"],\n",
        "            max_context_tokens=default_row[\"max_context_tokens\"],\n",
        "            api_base=default_row[\"api_base\"],\n",
        "            notes=default_row[\"notes\"],\n",
        "        )\n",
        "\n",
        "    # Fallback: first enabled embed model\n",
        "    configs = load_model_configs(\"embed\", only_enabled=True)\n",
        "    if configs:\n",
        "        return configs[0]\n",
        "\n",
        "    # Last resort hard-coded default\n",
        "    return ModelConfig(\n",
        "        model_id=\"text-embedding-3-small\",\n",
        "        provider=\"openai\",\n",
        "        display_name=\"text-embedding-3-small (fallback)\",\n",
        "        model_type=\"embed\",\n",
        "        is_default=True,\n",
        "        enabled=True,\n",
        "        cost_score=1,\n",
        "        latency_score=1,\n",
        "    )\n",
        "def list_chat_models() -> List[str]:\n",
        "    models = load_model_registry()\n",
        "    return [m[\"model_id\"] for m in models if m[\"type\"] == \"chat\" and m[\"enabled\"]]\n",
        "\n"
      ],
      "metadata": {
        "id": "nCLWFHAU47Ve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# CELL 5.2 / STEP 5.2 – RAG Retrieval Over a Cohort (v15)\n",
        "# ============================================================\n",
        "\n",
        "def build_context_from_index(\n",
        "    api_key: str,\n",
        "    chat_model: str,\n",
        "    embed_model: str,\n",
        "    cohort_name: str,\n",
        "    query: str,\n",
        "    top_k: int = 5,\n",
        "):\n",
        "    \"\"\"\n",
        "    Given a cohort and query:\n",
        "    - Load all documents for that cohort\n",
        "    - For each doc's index, perform similarity search\n",
        "    - Aggregate top_k results across docs\n",
        "    Returns:\n",
        "      - concatenated context string\n",
        "      - list of (doc_name, rank, score) for citations\n",
        "    \"\"\"\n",
        "\n",
        "    ensure_docs_table()\n",
        "    resolved_chat, resolved_embed = resolve_models(chat_model, embed_model)\n",
        "    client = build_openai_client(api_key)\n",
        "\n",
        "    # 1. Load all docs & their index references\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "    cur.execute(\n",
        "        \"\"\"\n",
        "        SELECT doc_name, index_id\n",
        "        FROM documents\n",
        "        WHERE cohort_name = ?\n",
        "        \"\"\",\n",
        "        (cohort_name,),\n",
        "    )\n",
        "    rows = cur.fetchall()\n",
        "    conn.close()\n",
        "\n",
        "    if not rows:\n",
        "        return \"\", []\n",
        "\n",
        "    # 2. Embed query once\n",
        "    q_embed_resp = client.embeddings.create(model=resolved_embed, input=[query])\n",
        "    q_vec = np.array(q_embed_resp.data[0].embedding, dtype=\"float32\")\n",
        "    q_vec = q_vec / (np.linalg.norm(q_vec) + 1e-10)\n",
        "\n",
        "    all_hits = []  # (doc_name, idx, score, text)\n",
        "\n",
        "    # 3. Perform similarity search in each doc index\n",
        "    for doc_name, index_id in rows:\n",
        "        try:\n",
        "            index = load_index(index_id)\n",
        "            meta = load_metadata(index_id)  # {\"chunks\": [...]}\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "        D, I = index.search(q_vec[np.newaxis, :], top_k)\n",
        "        scores = D[0]\n",
        "        idxs = I[0]\n",
        "\n",
        "        for score, idx in zip(scores, idxs):\n",
        "            if idx < 0:\n",
        "                continue\n",
        "            chunks = meta.get(\"chunks\", [])\n",
        "            if idx >= len(chunks):\n",
        "                continue\n",
        "            text_chunk = chunks[idx]\n",
        "            all_hits.append((doc_name, idx, float(score), text_chunk))\n",
        "\n",
        "    if not all_hits:\n",
        "        return \"\", []\n",
        "\n",
        "    # 4. Sort & select top results\n",
        "    all_hits.sort(key=lambda x: x[2], reverse=True)\n",
        "    top_hits = all_hits[:top_k]\n",
        "\n",
        "    context_parts = []\n",
        "    citations = []\n",
        "\n",
        "    for rank, (doc_name, idx, score, text) in enumerate(top_hits, start=1):\n",
        "        header = f\"[{rank}] From {doc_name} (chunk #{idx}, score={score:.3f})\"\n",
        "        context_parts.append(header + \"\\n\" + text)\n",
        "        citations.append((doc_name, rank, score))\n",
        "\n",
        "    context = \"\\n\\n\".join(context_parts)\n",
        "    return context, citations\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# UPDATED answer_with_rag() — Now uses Routing Brain (v15)\n",
        "# ============================================================\n",
        "\n",
        "def answer_with_rag(\n",
        "    api_key: str,\n",
        "    chat_model: str,          # kept for backward compatibility\n",
        "    embed_model: str,         # still used for embedding queries\n",
        "    cohort_name: str,\n",
        "    query: str,\n",
        "    system_prompt: str = \"\",\n",
        "    user_pref: str | None = None,  # NEW: allows override from UI\n",
        "):\n",
        "    \"\"\"\n",
        "    Perform RAG retrieval and return:\n",
        "        - answer_markdown\n",
        "        - raw_answer_text\n",
        "        - model_used (for history & UI display)\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Build RAG context\n",
        "    context, citations = build_context_from_index(\n",
        "        api_key, chat_model, embed_model, cohort_name, query\n",
        "    )\n",
        "\n",
        "    if not context:\n",
        "        return (\n",
        "            \"I could not find any context for this query in the selected cohort.\",\n",
        "            \"\",\n",
        "            \"N/A\",\n",
        "        )\n",
        "\n",
        "    # 2. Default system prompt\n",
        "    if not system_prompt:\n",
        "        system_prompt = (\n",
        "            \"You are a helpful assistant answering questions based on the provided context.\\n\"\n",
        "            \"If the answer cannot be found in the context, say you do not know.\"\n",
        "        )\n",
        "\n",
        "    # 3. Construct chat messages\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": (\n",
        "                \"Use ONLY the context below to answer the question.\\n\\n\"\n",
        "                \"=== CONTEXT START ===\\n\"\n",
        "                f\"{context}\\n\"\n",
        "                \"=== CONTEXT END ===\\n\\n\"\n",
        "                f\"QUESTION: {query}\"\n",
        "            ),\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    # 4. Call the routing brain — NEW for v15\n",
        "    answer_text, raw_answer_text, model_used = call_chat_model(\n",
        "        api_key=api_key,\n",
        "        messages=messages,\n",
        "        task_type=\"rag_answer\",\n",
        "        user_pref=user_pref,          # user-selected override (optional)\n",
        "        context_size=len(context),    # helps routing choose large/small models\n",
        "    )\n",
        "\n",
        "    # 5. Build answer markdown with citations + model info\n",
        "    md = answer_text + \"\\n\\n---\\n\\n**Cited sources:**\\n\"\n",
        "    for doc_name, rank, score in citations:\n",
        "        md += f\"- [{rank}] `{doc_name}` (score={score:.3f})\\n\"\n",
        "\n",
        "    md += f\"\\n\\n**Model Used:** `{model_used}`\\n\"\n",
        "\n",
        "    return md, raw_answer_text, model_used\n",
        "def answer_question_over_cohort(api_key, username, cohort_name, question, model_id):\n",
        "    \"\"\"\n",
        "    Wrapper for the RAG retrieval + LLM step.\n",
        "    Must exist BEFORE STEP 10.\n",
        "    \"\"\"\n",
        "    trace_log(\n",
        "        f\"answer_question_over_cohort called user={username}, cohort={cohort_name}\"\n",
        "    )\n",
        "\n",
        "    embed_cfg = get_default_embed_model_config()\n",
        "    embed_model = embed_cfg.model_id\n",
        "\n",
        "    answer_md, raw_answer, used_model = answer_with_rag(\n",
        "        api_key=api_key,\n",
        "        chat_model=model_id,\n",
        "        embed_model=embed_model,\n",
        "        cohort_name=cohort_name,\n",
        "        query=question,\n",
        "        system_prompt=\"\",\n",
        "        user_pref=model_id,  # routed through routing brain\n",
        "    )\n",
        "\n",
        "    return answer_md, raw_answer, used_model\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ccuJqrFgxWTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# CELL 6 / STEP 6 – Build Cohort from Uploaded Docs (v15)\n",
        "# ============================================================\n",
        "def build_cohort_from_files(\n",
        "    api_key: str,\n",
        "    embed_model: str,\n",
        "    cohort_name: str,\n",
        "    files: List[Any],\n",
        ") -> str:\n",
        "    \"\"\"Ingest a list of uploaded files into a *single* cohort.\n",
        "\n",
        "    For each file we:\n",
        "      - load the text\n",
        "      - chunk it\n",
        "      - embed with the given embedding model\n",
        "      - build & save a FAISS index\n",
        "      - save metadata (including the chunks)\n",
        "      - register the document in the SQLite `documents` table\n",
        "\n",
        "    Returns a human-readable summary string for the UI.\n",
        "    \"\"\"\n",
        "    ensure_docs_table()\n",
        "    ensure_cohort_table()\n",
        "    os.makedirs(INDEX_DIR, exist_ok=True)\n",
        "\n",
        "    success_count = 0\n",
        "    total_chunks = 0\n",
        "    messages: List[str] = []\n",
        "\n",
        "    for file_obj in files:\n",
        "        try:\n",
        "            # 1) Load text & derive a stable doc_name\n",
        "            text, doc_name = load_file_to_text(file_obj)\n",
        "            if not text or not text.strip():\n",
        "                messages.append(f\"⚠️ {doc_name}: no extractable text, skipped.\")\n",
        "                continue\n",
        "\n",
        "            # 2) Chunk\n",
        "            chunks = chunk_text(text)\n",
        "            if not chunks:\n",
        "                messages.append(f\"⚠️ {doc_name}: produced 0 chunks, skipped.\")\n",
        "                continue\n",
        "\n",
        "            # 3) Embed\n",
        "            vectors = embed_texts(\n",
        "                api_key=api_key,\n",
        "                embed_model=embed_model,\n",
        "                texts=chunks,\n",
        "            )\n",
        "\n",
        "            # 4) Build FAISS index\n",
        "            index = build_faiss_index(vectors)\n",
        "\n",
        "            # 5) Save index & metadata\n",
        "            index_id = str(uuid4())\n",
        "            save_index(index, index_id)\n",
        "\n",
        "            meta = {\n",
        "                \"cohort_name\": cohort_name,\n",
        "                \"doc_name\": doc_name,\n",
        "                \"chunks\": chunks,\n",
        "                \"embed_model\": embed_model,\n",
        "            }\n",
        "            save_metadata(index_id, meta)\n",
        "\n",
        "            # 6) Register in SQLite\n",
        "            register_document(\n",
        "                doc_name=doc_name,\n",
        "                cohort_name=cohort_name,\n",
        "                index_id=index_id,\n",
        "                n_chunks=len(chunks),\n",
        "                embed_model=embed_model,\n",
        "            )\n",
        "\n",
        "            success_count += 1\n",
        "            total_chunks += len(chunks)\n",
        "            messages.append(f\"✅ {doc_name}: {len(chunks)} chunks embedded.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            # Best-effort error capture per-file\n",
        "            name = getattr(file_obj, \"name\", str(file_obj))\n",
        "            messages.append(f\"❌ {name}: {e}\")\n",
        "\n",
        "    if success_count == 0:\n",
        "        detail = \"\\n\".join(messages) if messages else \"\"\n",
        "        return \"❌ No documents were successfully processed.\" + (f\"\\n{detail}\" if detail else \"\")\n",
        "\n",
        "    summary = (\n",
        "        f\"✅ Built cohort '{cohort_name}' with {success_count} document(s) \"\n",
        "        f\"and {total_chunks} total chunks.\"\n",
        "    )\n",
        "    if messages:\n",
        "        summary += \"\\n\" + \"\\n\".join(messages)\n",
        "    return summary\n",
        "\n",
        "\n",
        "def build_cohort_index(\n",
        "    api_key: str,\n",
        "    cohort_name: str,\n",
        "    files: List[Any],\n",
        "    owner: Optional[str] = None,\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    High-level helper used by the Gradio UI (v15):\n",
        "\n",
        "      - Selects default embedding model (from model registry)\n",
        "      - Builds the cohort\n",
        "      - Stores cohort ownership\n",
        "    \"\"\"\n",
        "    if not api_key or not api_key.strip():\n",
        "        return \"❌ OpenAI API key is required.\"\n",
        "\n",
        "    if not cohort_name or not cohort_name.strip():\n",
        "        return \"❌ Cohort name is required.\"\n",
        "\n",
        "    if not files:\n",
        "        return \"❌ Please upload at least one file.\"\n",
        "\n",
        "    # ✔️ NEW — correct default embedding model call\n",
        "    embed_cfg = get_default_embed_model_config()\n",
        "    embed_model = embed_cfg.model_id\n",
        "\n",
        "    # Build FAISS index + metadata\n",
        "    result_msg = build_cohort_from_files(\n",
        "        api_key=api_key,\n",
        "        embed_model=embed_model,\n",
        "        cohort_name=cohort_name.strip(),\n",
        "        files=files,\n",
        "    )\n",
        "\n",
        "    # Save cohort owner metadata\n",
        "    try:\n",
        "        if owner:\n",
        "            role = USERS.get(owner, {}).get(\"role\", \"user\")\n",
        "            user_obj = SessionUser(username=owner, role=role)\n",
        "        else:\n",
        "            user_obj = None\n",
        "\n",
        "        set_cohort_owner(cohort_name.strip(), user_obj)\n",
        "    except Exception as e:\n",
        "        print(\"DEBUG set_cohort_owner error:\", e)\n",
        "\n",
        "    return result_msg\n"
      ],
      "metadata": {
        "id": "NFcsOK6a4Tov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# CELL 6.1 / STEP 6.1 – Routing Brain (v15)\n",
        "# ============================================================\n",
        "#\n",
        "# Centralized model selection + wrapper for ALL chat LLM calls.\n",
        "# Uses the model_registry table (STEP 5.0) and supports:\n",
        "#   - task_type hints (\"question_improve\", \"rag_answer\", \"summary\", \"admin\")\n",
        "#   - optional user_pref override (a specific model_id)\n",
        "#   - dry_run flag for automated self-tests (no API call made)\n",
        "#\n",
        "# Functions:\n",
        "#   - select_chat_model(task_type, context_size, user_pref)\n",
        "#   - call_chat_model(api_key, messages, task_type, user_pref, context_size, dry_run)\n",
        "\n",
        "from typing import List, Dict, Any, Tuple\n",
        "\n",
        "\n",
        "def select_chat_model(\n",
        "    task_type: str,\n",
        "    context_size: int = 0,\n",
        "    user_pref: str | None = None,\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Decide which chat model_id to use based on:\n",
        "      - user_pref: explicit override from UI\n",
        "      - registry defaults (is_default)\n",
        "      - task_type and context_size (reserved for future heuristics)\n",
        "\n",
        "    Returns:\n",
        "        model_id (e.g., \"gpt-4.1-mini\")\n",
        "    \"\"\"\n",
        "    # Load enabled chat models as ModelConfig objects\n",
        "    configs = load_model_configs(\"chat\", only_enabled=True)\n",
        "\n",
        "    # Fallback if registry is empty or misconfigured\n",
        "    if not configs:\n",
        "        return \"gpt-4.1-mini\"\n",
        "\n",
        "    # 1. If user_pref matches a known enabled model_id, honor it\n",
        "    if user_pref:\n",
        "        for cfg in configs:\n",
        "            if cfg.model_id == user_pref:\n",
        "                return cfg.model_id\n",
        "\n",
        "    # 2. Prefer the model marked as default\n",
        "    for cfg in configs:\n",
        "        if cfg.is_default:\n",
        "            return cfg.model_id\n",
        "\n",
        "    # 3. Simple heuristic placeholder:\n",
        "    #    For now we ignore task_type/context_size and just use the first enabled.\n",
        "    return configs[0].model_id\n",
        "\n",
        "\n",
        "\n",
        "def call_chat_model(\n",
        "    api_key: str,\n",
        "    messages: List[Dict[str, Any]],\n",
        "    task_type: str,\n",
        "    user_pref: str | None = None,\n",
        "    context_size: int = 0,\n",
        "    dry_run: bool = False,\n",
        ") -> Tuple[str, str, str]:\n",
        "    \"\"\"\n",
        "    Wrapper for ALL chat LLM calls in the app.\n",
        "\n",
        "    Args:\n",
        "        api_key:      OpenAI API key\n",
        "        messages:     Chat completion messages\n",
        "        task_type:    Semantic label for routing (e.g. 'rag_answer', 'question_improve')\n",
        "        user_pref:    Optional explicit model_id override\n",
        "        context_size: Approx size of context (chars) to inform routing\n",
        "        dry_run:      If True, DO NOT call the API (used by self-tests).\n",
        "\n",
        "    Returns:\n",
        "        (answer_text, raw_answer_text, model_used)\n",
        "    \"\"\"\n",
        "    model_id = select_chat_model(\n",
        "        task_type=task_type,\n",
        "        context_size=context_size,\n",
        "        user_pref=user_pref,\n",
        "    )\n",
        "\n",
        "    # For automated self-tests: don't hit the API\n",
        "    if dry_run:\n",
        "        dummy = f\"[DRY RUN] task_type={task_type}, model_id={model_id}\"\n",
        "        return dummy, dummy, model_id\n",
        "\n",
        "    client = build_openai_client(api_key)\n",
        "\n",
        "    resp = client.chat.completions.create(\n",
        "        model=model_id,\n",
        "        messages=messages,\n",
        "        temperature=0.2,\n",
        "        max_tokens=900,\n",
        "    )\n",
        "\n",
        "    answer_text = resp.choices[0].message.content.strip()\n",
        "    # In this MVP, raw_answer_text == answer_text, but we keep both for future transforms\n",
        "    raw_answer_text = answer_text\n",
        "\n",
        "    return answer_text, raw_answer_text, model_id\n"
      ],
      "metadata": {
        "id": "xKDcXhlMQchj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RzcKE8dRPBIN"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CELL 7 / STEP 7 – Admin Helpers (Stats & Maintenance)\n",
        "# ============================================================\n",
        "\n",
        "def get_db_stats() -> str:\n",
        "    ensure_docs_table()\n",
        "    ensure_cohort_table()\n",
        "    ensure_user_table()\n",
        "    ensure_chat_history_table()\n",
        "\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "\n",
        "    cur.execute(\"SELECT COUNT(*) FROM documents\")\n",
        "    n_docs = cur.fetchone()[0]\n",
        "\n",
        "    cur.execute(\"SELECT COUNT(DISTINCT cohort_name) FROM cohort_docs\")\n",
        "    n_cohorts = cur.fetchone()[0]\n",
        "\n",
        "    cur.execute(\"SELECT COUNT(*) FROM users\")\n",
        "    n_users = cur.fetchone()[0]\n",
        "\n",
        "    cur.execute(\"SELECT COUNT(*) FROM chat_history\")\n",
        "    n_chats = cur.fetchone()[0]\n",
        "\n",
        "    conn.close()\n",
        "\n",
        "    return (\n",
        "        f\"**DB Stats**\\n\\n\"\n",
        "        f\"- Documents: {n_docs}\\n\"\n",
        "        f\"- Cohorts: {n_cohorts}\\n\"\n",
        "        f\"- Users: {n_users}\\n\"\n",
        "        f\"- Chat records (last 7 days enforced on write): {n_chats}\\n\"\n",
        "    )\n",
        "\n",
        "def describe_users() -> str:\n",
        "    rows = list_users()\n",
        "    if not rows:\n",
        "        return \"No users have been recorded yet.\"\n",
        "    lines = [\"**Known Users**\\n\"]\n",
        "    for user_id, display_name, role in rows:\n",
        "        disp = display_name or \"(no display name)\"\n",
        "        r = role or \"(no role)\"\n",
        "        lines.append(f\"- `{user_id}` – {disp} – role: `{r}`\")\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "def describe_cohorts() -> str:\n",
        "    \"\"\"\n",
        "    Returns a markdown summary of cohorts, including:\n",
        "      - name\n",
        "      - owner\n",
        "      - visibility (private/shared)\n",
        "      - document count\n",
        "    \"\"\"\n",
        "    ensure_docs_table()\n",
        "    ensure_cohort_table()\n",
        "    ensure_cohort_meta_table()\n",
        "\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "    cur.execute(\n",
        "        \"\"\"\n",
        "        SELECT DISTINCT\n",
        "            cd.cohort_name,\n",
        "            COALESCE(cm.owner_user_id, '(none)') AS owner,\n",
        "            COALESCE(cm.is_shared, 0)            AS is_shared,\n",
        "            COUNT(DISTINCT cd.doc_name)          AS num_docs\n",
        "        FROM cohort_docs cd\n",
        "        LEFT JOIN cohort_meta cm\n",
        "          ON cd.cohort_name = cm.cohort_name\n",
        "        GROUP BY cd.cohort_name, owner, is_shared\n",
        "        ORDER BY cd.cohort_name\n",
        "        \"\"\"\n",
        "    )\n",
        "    rows = cur.fetchall()\n",
        "    conn.close()\n",
        "\n",
        "    if not rows:\n",
        "        return \"No cohorts found.\"\n",
        "\n",
        "    lines = [\"**Cohorts**\", \"\"]\n",
        "    for name, owner, is_shared, num_docs in rows:\n",
        "        share_label = \"shared\" if is_shared else \"private\"\n",
        "        lines.append(\n",
        "            f\"- **{name}** — owner: `{owner}`, visibility: {share_label}, docs: {num_docs}\"\n",
        "        )\n",
        "\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# CELL 8 / STEP 8 – Prompt Coach (Optional Query Improvement) – v15\n",
        "# ============================================================\n",
        "#\n",
        "# Uses the v15 Routing Brain (call_chat_model) instead of calling OpenAI directly.\n",
        "# Signature is kept the same so STEP 10's on_improve_query(...) still works:\n",
        "#     improve_query(api_key, chat_model, original_query)\n",
        "#\n",
        "# In a future step, we can optionally add a user-selected model override and\n",
        "# pass it into call_chat_model(user_pref=...).\n",
        "\n",
        "def improve_query(\n",
        "    api_key: str,\n",
        "    chat_model: str,       # kept for backward compatibility; routing ignores it\n",
        "    original_query: str,\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Prompt coach to re-write the user's query for better RAG retrieval.\n",
        "\n",
        "    Behavior:\n",
        "    - If the original query is empty/whitespace, returns \"\".\n",
        "    - Otherwise, uses the Routing Brain (task_type='question_improve') to pick\n",
        "      an appropriate chat model and rewrite the question to be clearer, more\n",
        "      explicit, and RAG-friendly.\n",
        "    \"\"\"\n",
        "    if not original_query.strip():\n",
        "        return \"\"\n",
        "\n",
        "    system_prompt = (\n",
        "        \"You are a prompt coach helping the user improve questions for a RAG system. \"\n",
        "        \"Rewrite the query to be explicit, concise, and focused on key details. \"\n",
        "        \"Preserve the user's intent but remove ambiguity, vague pronouns, and \"\n",
        "        \"unnecessary filler. Return ONLY the improved query text.\"\n",
        "    )\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": original_query},\n",
        "    ]\n",
        "\n",
        "    # Use the v15 Routing Brain instead of calling OpenAI directly\n",
        "    improved, _, model_used = call_chat_model(\n",
        "        api_key=api_key,\n",
        "        messages=messages,\n",
        "        task_type=\"question_improve\",\n",
        "        user_pref=None,                     # (optional override will come from UI later)\n",
        "        context_size=len(original_query),   # small, but available for routing heuristics\n",
        "    )\n",
        "\n",
        "    # For now we just return the improved text. If desired later, we can:\n",
        "    # - log model_used to audit_log\n",
        "    # - display which model did the improvement in the UI.\n",
        "    return improved.strip()\n"
      ],
      "metadata": {
        "id": "Wjpbk2yC3CYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wwFVsO6MPBbG"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CELL 9 / STEP 9 – Chat History Viewer (User-Facing)\n",
        "# ============================================================\n",
        "\n",
        "def format_history_markdown(\n",
        "    user_id: Optional[str],\n",
        "    cohort_name: Optional[str],\n",
        "    limit: int = 50,\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Turn recent history into markdown for display.\n",
        "    \"\"\"\n",
        "    hist = get_recent_history(user_id=user_id, cohort_name=cohort_name, limit=limit)\n",
        "    if not hist:\n",
        "        return \"No chat history found for the given filters (within retention window).\"\n",
        "\n",
        "    lines = []\n",
        "    lines.append(\n",
        "        f\"**Showing up to {limit} most recent interactions** \"\n",
        "        f\"{'(filtered)' if user_id or cohort_name else ''}\\n\"\n",
        "    )\n",
        "\n",
        "    for h in hist:\n",
        "        ts = h[\"created_at\"]\n",
        "        u = h[\"user_id\"] or \"(anonymous)\"\n",
        "        r = h[\"role\"] or \"(none)\"\n",
        "        c = h[\"cohort_name\"] or \"(none)\"\n",
        "        which = h[\"which_prompt\"] or \"(unknown)\"\n",
        "\n",
        "        lines.append(f\"---\\n**User:** `{u}`  |  **Role:** `{r}`  |  **Cohort:** `{c}`  |  **When:** {ts}\")\n",
        "        lines.append(f\"**Prompt used:** `{which}`\")\n",
        "        lines.append(f\"**Original query:**\\n{h['original_query']}\\n\")\n",
        "        if h[\"improved_query\"]:\n",
        "            lines.append(f\"**Improved query:**\\n{h['improved_query']}\\n\")\n",
        "        lines.append(\"**Answer:**\")\n",
        "        lines.append(h[\"answer\"])\n",
        "        lines.append(\"\")\n",
        "\n",
        "    return \"\\n\".join(lines)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xUlh5Loticb"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CELL 9.5 / STEP 9.5 – Identity & Admin Ops (v14)\n",
        "# ============================================================\n",
        "\n",
        "def authenticate(username: str, password: str) -> SessionUser | None:\n",
        "    \"\"\"\n",
        "    MVP auth: checks against local USERS dict.\n",
        "    Returns SessionUser or None if invalid.\n",
        "    \"\"\"\n",
        "    record = USERS.get(username)\n",
        "    if not record:\n",
        "        return None\n",
        "    if password != record[\"password\"]:\n",
        "        return None\n",
        "    return SessionUser(username=username, role=record[\"role\"])\n",
        "def authenticate(username: str, password: str) -> SessionUser | None:\n",
        "    \"\"\"\n",
        "    MVP auth: checks against local USERS dict.\n",
        "    Returns SessionUser or None if invalid.\n",
        "    \"\"\"\n",
        "    record = USERS.get(username)\n",
        "    if not record:\n",
        "        return None\n",
        "    if password != record[\"password\"]:\n",
        "        return None\n",
        "    return SessionUser(username=username, role=record[\"role\"])\n",
        "\n",
        "\n",
        "def authenticate_credentials(username: str, password: str):\n",
        "    \"\"\"\n",
        "    Wrapper used by the Gradio login logic in STEP 10.\n",
        "\n",
        "    It calls authenticate(...) which returns a SessionUser, then:\n",
        "      - Upserts the user into the `users` table (for admin/history views)\n",
        "      - Returns a simple dict {username, role} that the UI expects.\n",
        "    \"\"\"\n",
        "    # Use the existing MVP auth\n",
        "    user = authenticate(username, password)\n",
        "    if not user:\n",
        "        return None\n",
        "\n",
        "    # Make sure the user exists in the DB's `users` table\n",
        "    try:\n",
        "        upsert_user(\n",
        "            user_id=user.username,\n",
        "            role=user.role,\n",
        "            display_name=user.username,\n",
        "        )\n",
        "    except Exception as e:\n",
        "        # Don't break login if DB write fails; just log it\n",
        "        trace_log(f\"authenticate_credentials upsert_user ERROR: {e}\")\n",
        "\n",
        "    # UI login code in STEP 10 expects a dict-like object\n",
        "    return {\"username\": user.username, \"role\": user.role}\n",
        "\n",
        "def require_admin(user: SessionUser):\n",
        "    \"\"\"\n",
        "    Helper for admin-only actions. Raises PermissionError if not admin.\n",
        "    \"\"\"\n",
        "    if not user or not user.is_admin:\n",
        "        raise PermissionError(\"Admin privileges required for this action.\")\n",
        "\n",
        "\n",
        "def admin_delete_cohort(user: SessionUser, cohort_name: str) -> str:\n",
        "    \"\"\"\n",
        "    Admin-only wrapper around delete_cohort().\n",
        "    Uses the existing delete_cohort function from STEP 4.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        require_admin(user)\n",
        "    except PermissionError as e:\n",
        "        return f\"❌ Not authorized: {e}\"\n",
        "\n",
        "    if not cohort_name:\n",
        "        return \"❌ Please select a cohort to delete.\"\n",
        "\n",
        "    try:\n",
        "        # Use existing v13 delete_cohort logic (no reassignment in this MVP).\n",
        "        msg = delete_cohort(cohort_name, reassign_to=None)\n",
        "        log_audit(user.username, user.role, \"delete_cohort\", f\"cohort={cohort_name}\")\n",
        "        return msg\n",
        "    except Exception as e:\n",
        "        return f\"❌ Error deleting cohort: {e}\"\n",
        "\n",
        "def admin_view_audit_log(user: SessionUser, limit: int = 50) -> str:\n",
        "    \"\"\"\n",
        "    Admin-only view of recent audit log entries.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        require_admin(user)\n",
        "    except PermissionError as e:\n",
        "        return f\"❌ Not authorized: {e}\"\n",
        "\n",
        "    ensure_audit_table()\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "    cur.execute(\n",
        "        \"\"\"\n",
        "        SELECT ts, username, role, action, details\n",
        "        FROM audit_log\n",
        "        ORDER BY id DESC\n",
        "        LIMIT ?\n",
        "        \"\"\",\n",
        "        (limit,),\n",
        "    )\n",
        "    rows = cur.fetchall()\n",
        "    conn.close()\n",
        "\n",
        "    if not rows:\n",
        "        return \"No audit log entries.\"\n",
        "\n",
        "    lines = [\"**Recent Audit Log Entries**\\n\"]\n",
        "    for ts, username, role, action, details in rows:\n",
        "        u = username or \"-\"\n",
        "        r = role or \"-\"\n",
        "        d = details or \"\"\n",
        "        lines.append(f\"- {ts} | user=`{u}` | role=`{r}` | action=`{action}` | {d}\")\n",
        "\n",
        "    return \"\\n\".join(lines)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# STEP 10 — Gradio App, Tabs & Startup (v15 Full Version)\n",
        "# ============================================================\n",
        "\n",
        "# Global state handles (will be initialized in build_interface)\n",
        "current_user_state = None\n",
        "current_role_state = None\n",
        "current_api_key_state = None\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# SETUP & COHORTS TAB\n",
        "# -----------------------------\n",
        "def build_setup_tab():\n",
        "    \"\"\"\n",
        "    Build the 'Setup & Cohorts' tab for the Gradio UI.\n",
        "\n",
        "    Modes:\n",
        "      1) Create new cohort\n",
        "      2) Add files to existing cohort\n",
        "\n",
        "    Features:\n",
        "      - Prevent duplicate cohort names when creating new\n",
        "      - Clear fields on successful build\n",
        "      - Clear status when Refresh is clicked\n",
        "      - Refresh list of cohorts visible to the current user\n",
        "      - Show an overview list of cohorts\n",
        "    \"\"\"\n",
        "    gr.Markdown(\"### Build or Manage a Cohort\")\n",
        "\n",
        "    # Choose what we are doing\n",
        "    action_radio = gr.Radio(\n",
        "        label=\"Action\",\n",
        "        choices=[\"Create new cohort\", \"Add files to existing cohort\"],\n",
        "        value=\"Create new cohort\",\n",
        "    )\n",
        "\n",
        "    # For creating a new cohort\n",
        "    cohort_name = gr.Textbox(\n",
        "        label=\"New Cohort Name\",\n",
        "        placeholder=\"e.g., USDA_WIC_Guidance\",\n",
        "    )\n",
        "\n",
        "    # For adding to an existing cohort\n",
        "    existing_cohort_dropdown = gr.Dropdown(\n",
        "        label=\"Existing Cohort (for Add mode)\",\n",
        "        choices=[],\n",
        "        value=None,\n",
        "        interactive=True,\n",
        "    )\n",
        "\n",
        "    file_uploader = gr.File(\n",
        "        label=\"Upload Documents\",\n",
        "        file_count=\"multiple\",\n",
        "        type=\"filepath\",  # filepaths go into build_cohort_index / load_file_to_text\n",
        "    )\n",
        "\n",
        "    build_btn = gr.Button(\"Build / Update Cohort Index\")\n",
        "    build_status = gr.Markdown()\n",
        "\n",
        "    refresh_btn = gr.Button(\"Refresh Your Cohorts\")\n",
        "\n",
        "    # Overview list of cohorts (now interactive for visibility, but logically read-only)\n",
        "    cohort_list = gr.Dropdown(\n",
        "        label=\"Your Cohorts (overview)\",\n",
        "        choices=[],\n",
        "        value=None,\n",
        "        interactive=True,  # still just for viewing; no actions tied to it yet\n",
        "    )\n",
        "\n",
        "    # ---------- BUILD COHORT CALLBACK ----------\n",
        "\n",
        "    def _build(new_name, files, existing_name, action, username, api_key):\n",
        "        \"\"\"\n",
        "        Build or update a cohort index from uploaded files.\n",
        "\n",
        "        Returns:\n",
        "          (status_markdown,\n",
        "           new_cohort_name_update,\n",
        "           file_uploader_update)\n",
        "        \"\"\"\n",
        "        keep_name = gr.update()\n",
        "        keep_files = gr.update()\n",
        "\n",
        "        trace_log(\n",
        "            f\"SETUP _build called user={username!r}, action={action!r}, \"\n",
        "            f\"new_name={new_name!r}, existing_name={existing_name!r}, \"\n",
        "            f\"files={len(files) if files else 0}\"\n",
        "        )\n",
        "\n",
        "        # --- guard rails (do NOT clear fields on error) ---\n",
        "\n",
        "        if not username:\n",
        "            return \"❌ Not logged in.\", keep_name, keep_files\n",
        "\n",
        "        if not api_key or not api_key.strip():\n",
        "            return \"❌ OpenAI API key is required.\", keep_name, keep_files\n",
        "\n",
        "        if not files:\n",
        "            return \"❌ Please upload at least one file.\", keep_name, keep_files\n",
        "\n",
        "        action = action or \"Create new cohort\"\n",
        "\n",
        "        # Decide target cohort name based on mode\n",
        "        if action == \"Create new cohort\":\n",
        "            if not new_name or not new_name.strip():\n",
        "                return \"❌ New cohort name is required.\", keep_name, keep_files\n",
        "            target = new_name.strip()\n",
        "        else:  # \"Add files to existing cohort\"\n",
        "            if not existing_name:\n",
        "                return \"❌ Please select an existing cohort to add files to.\", keep_name, keep_files\n",
        "            target = existing_name\n",
        "\n",
        "        # Check for duplicate when creating new\n",
        "        if action == \"Create new cohort\":\n",
        "            try:\n",
        "                existing = list_cohorts()\n",
        "            except Exception as e:\n",
        "                trace_log(f\"SETUP _build list_cohorts ERROR: {e}\")\n",
        "                existing = []\n",
        "\n",
        "            if target in existing:\n",
        "                msg = (\n",
        "                    f\"❌ A cohort named **{target}** already exists. \"\n",
        "                    \"Please choose a different name or use 'Add files to existing cohort'.\"\n",
        "                )\n",
        "                trace_log(f\"SETUP _build DUPLICATE new cohort={target!r}\")\n",
        "                # Keep name & files so user can adjust\n",
        "                return msg, gr.update(value=target), keep_files\n",
        "\n",
        "        # --- Happy path: build / update the cohort ---\n",
        "\n",
        "        try:\n",
        "            msg = build_cohort_index(\n",
        "                api_key=api_key,\n",
        "                cohort_name=target,\n",
        "                files=files,\n",
        "                owner=username,\n",
        "            )\n",
        "            trace_log(\n",
        "                f\"SETUP _build SUCCESS action={action!r}, target={target!r}: {msg}\"\n",
        "            )\n",
        "\n",
        "            # On success: clear both the new_name field and the uploader\n",
        "            clear_name = gr.update(value=\"\")\n",
        "            clear_files = gr.update(value=None)\n",
        "\n",
        "            return msg, clear_name, clear_files\n",
        "\n",
        "        except Exception as e:\n",
        "            trace_log(f\"SETUP _build ERROR action={action!r}, target={target!r}: {e}\")\n",
        "            return f\"❌ Error: {e}\", keep_name, keep_files\n",
        "\n",
        "    # ---------- REFRESH COHORTS CALLBACK ----------\n",
        "\n",
        "    def _refresh(username):\n",
        "        \"\"\"\n",
        "        Refresh the list of cohorts visible to this user and\n",
        "        clear the build status message.\n",
        "        \"\"\"\n",
        "        trace_log(f\"SETUP _refresh called with username={username!r}\")\n",
        "        try:\n",
        "            if not username:\n",
        "                empty_choices = gr.update(choices=[], value=None)\n",
        "                return (\n",
        "                    empty_choices,  # existing_cohort_dropdown\n",
        "                    empty_choices,  # cohort_list (overview)\n",
        "                    gr.update(value=\"\"),  # build_status\n",
        "                )\n",
        "\n",
        "            names = get_cohorts_for_user(username)\n",
        "            trace_log(f\"SETUP _refresh cohorts={names}\")\n",
        "\n",
        "            # For existing-cohort dropdown in Add mode\n",
        "            existing_update = gr.update(choices=names, value=None)\n",
        "\n",
        "            # For overview dropdown, set first cohort as selected (if any)\n",
        "            if names:\n",
        "                overview_update = gr.update(choices=names, value=names[0])\n",
        "            else:\n",
        "                overview_update = gr.update(choices=[], value=None)\n",
        "\n",
        "            return (\n",
        "                existing_update,   # existing_cohort_dropdown\n",
        "                overview_update,   # cohort_list (overview)\n",
        "                gr.update(value=\"\"),  # build_status\n",
        "            )\n",
        "        except Exception as e:\n",
        "            trace_log(f\"SETUP _refresh ERROR for username={username!r}: {e}\")\n",
        "            empty_choices = gr.update(choices=[], value=None)\n",
        "            return (\n",
        "                empty_choices,              # existing\n",
        "                empty_choices,              # overview\n",
        "                gr.update(value=\"\"),        # status\n",
        "            )\n",
        "\n",
        "    # ---------- WIRE BUTTONS ----------\n",
        "\n",
        "    # _build returns 3 outputs\n",
        "    build_btn.click(\n",
        "        _build,\n",
        "        inputs=[\n",
        "            cohort_name,\n",
        "            file_uploader,\n",
        "            existing_cohort_dropdown,\n",
        "            action_radio,\n",
        "            current_user_state,\n",
        "            current_api_key_state,\n",
        "        ],\n",
        "        outputs=[build_status, cohort_name, file_uploader],\n",
        "    )\n",
        "\n",
        "    # _refresh returns 3 outputs:\n",
        "    # existing_cohort_dropdown + cohort_list + build_status\n",
        "    refresh_btn.click(\n",
        "        _refresh,\n",
        "        inputs=[current_user_state],\n",
        "        outputs=[existing_cohort_dropdown, cohort_list, build_status],\n",
        "    )\n",
        "\n",
        "# -----------------------------\n",
        "# ASK TAB (with prompt improve)\n",
        "# -----------------------------\n",
        "def build_ask_tab():\n",
        "    \"\"\"\n",
        "    Build the 'Ask a Question' tab for the Gradio UI.\n",
        "\n",
        "    Features:\n",
        "      - Choose chat model (from model registry)\n",
        "      - Improve Prompt\n",
        "      - Original vs Improved prompt selector\n",
        "      - Full RAG pipeline via answer_question_over_cohort()\n",
        "      - Cohort refresh\n",
        "      - Chat history saving (per user)\n",
        "    \"\"\"\n",
        "    with gr.Tab(\"Ask a Question\"):\n",
        "        gr.Markdown(\"### Ask a Question Against a Cohort\")\n",
        "\n",
        "        # --- UI controls ---\n",
        "\n",
        "        cohort_dropdown = gr.Dropdown(\n",
        "            label=\"Select Cohort\",\n",
        "            choices=[],\n",
        "            value=None,\n",
        "            interactive=True,\n",
        "        )\n",
        "\n",
        "        model_dropdown = gr.Dropdown(\n",
        "            label=\"Choose Chat Model\",\n",
        "            choices=list_chat_models(),\n",
        "            value=None,\n",
        "            interactive=True,\n",
        "        )\n",
        "\n",
        "        question_box = gr.Textbox(\n",
        "            label=\"Your Original Prompt / Question\",\n",
        "            placeholder=\"Ask something using the selected cohort...\",\n",
        "            lines=3,\n",
        "        )\n",
        "\n",
        "        improve_btn = gr.Button(\"✨ Improve Prompt\")\n",
        "\n",
        "        improved_box = gr.Textbox(\n",
        "            label=\"Improved Prompt\",\n",
        "            placeholder=\"Improved version of your question will appear here...\",\n",
        "            lines=3,\n",
        "        )\n",
        "\n",
        "        prompt_choice = gr.Radio(\n",
        "            label=\"Which prompt should be used for the answer?\",\n",
        "            choices=[\"Use original prompt\", \"Use improved prompt\"],\n",
        "            value=\"Use original prompt\",\n",
        "        )\n",
        "\n",
        "        ask_btn = gr.Button(\"Ask\")\n",
        "        ask_output = gr.Markdown()\n",
        "\n",
        "        ask_refresh_btn = gr.Button(\"Refresh Cohorts\")\n",
        "\n",
        "        # ---------- PROMPT IMPROVER ----------\n",
        "\n",
        "        def _improve_prompt(username, question, model_id, api_key):\n",
        "            trace_log(\n",
        "                f\"_improve_prompt called user={username!r}, model_id={model_id!r}\"\n",
        "            )\n",
        "\n",
        "            if not username:\n",
        "                return (\"⚠️ You must be logged in.\", gr.update())\n",
        "\n",
        "            if not question or not question.strip():\n",
        "                return (\"⚠️ Enter a question to improve.\", gr.update())\n",
        "\n",
        "            if not api_key or not api_key.strip():\n",
        "                return (\"⚠️ OpenAI API key required.\", gr.update())\n",
        "\n",
        "            system_msg = (\n",
        "                \"You are a professional prompt engineer. \"\n",
        "                \"Rewrite the user's question into a clearer, more actionable \"\n",
        "                \"prompt suitable for retrieval-augmented generation. \"\n",
        "                \"Do NOT invent new facts. Return only the improved prompt.\"\n",
        "            )\n",
        "\n",
        "            try:\n",
        "                improved_text, raw, used_model = call_chat_model(\n",
        "                    api_key=api_key,\n",
        "                    messages=[\n",
        "                        {\"role\": \"system\", \"content\": system_msg},\n",
        "                        {\"role\": \"user\", \"content\": question},\n",
        "                    ],\n",
        "                    task_type=\"prompt_improve\",\n",
        "                    user_pref=model_id,\n",
        "                    context_size=len(question),\n",
        "                )\n",
        "                trace_log(f\"_improve_prompt succeeded using model={used_model!r}\")\n",
        "\n",
        "                # Switch radio to \"Use improved prompt\"\n",
        "                return improved_text, gr.update(value=\"Use improved prompt\")\n",
        "\n",
        "            except Exception as e:\n",
        "                trace_log(f\"_improve_prompt ERROR: {e}\")\n",
        "                return f\"❌ Error improving prompt: {e}\", gr.update()\n",
        "\n",
        "        # ---------- ASK FUNCTION (RAG + history) ----------\n",
        "\n",
        "        def _ask(\n",
        "            username,\n",
        "            cohort_name,\n",
        "            model_id,\n",
        "            original_prompt,\n",
        "            improved_prompt,\n",
        "            which_prompt,\n",
        "            api_key,\n",
        "        ):\n",
        "            trace_log(\n",
        "                f\"_ask called user={username!r}, cohort={cohort_name!r}, \"\n",
        "                f\"model_id={model_id!r}, which_prompt={which_prompt!r}\"\n",
        "            )\n",
        "\n",
        "            if not username:\n",
        "                return \"⚠️ You must be logged in.\"\n",
        "\n",
        "            if not api_key or not api_key.strip():\n",
        "                return \"⚠️ OpenAI API key required.\"\n",
        "\n",
        "            if not cohort_name:\n",
        "                return \"⚠️ Please select a cohort.\"\n",
        "\n",
        "            if not original_prompt or not original_prompt.strip():\n",
        "                return \"⚠️ Enter a question.\"\n",
        "\n",
        "            # Decide which prompt to use\n",
        "            if (\n",
        "                which_prompt == \"Use improved prompt\"\n",
        "                and improved_prompt\n",
        "                and improved_prompt.strip()\n",
        "            ):\n",
        "                final_query = improved_prompt.strip()\n",
        "                trace_log(\"_ask using improved prompt\")\n",
        "            else:\n",
        "                final_query = original_prompt.strip()\n",
        "                trace_log(\"_ask using original prompt\")\n",
        "\n",
        "            trace_log(f\"_ask effective_query={final_query!r}\")\n",
        "\n",
        "            # Run RAG pipeline\n",
        "            try:\n",
        "                answer_md, raw_answer, used_model = answer_question_over_cohort(\n",
        "                    api_key=api_key,\n",
        "                    username=username,\n",
        "                    cohort_name=cohort_name,\n",
        "                    question=final_query,\n",
        "                    model_id=model_id,\n",
        "                )\n",
        "            except Exception as e:\n",
        "                trace_log(f\"_ask ERROR answer_question_over_cohort: {e}\")\n",
        "                return f\"❌ Error while generating answer: {e}\"\n",
        "\n",
        "            # Save chat history (non-fatal on error)\n",
        "            try:\n",
        "                # IMPORTANT: these kwarg names must match save_chat_history()\n",
        "                save_chat_history(\n",
        "                    user_id=username,\n",
        "                    cohort_name=cohort_name,\n",
        "                    question=final_query,\n",
        "                    answer=answer_md,\n",
        "                    model_used=used_model,\n",
        "                )\n",
        "            except Exception as e:\n",
        "                trace_log(f\"_ask ERROR saving chat history: {e}\")\n",
        "\n",
        "            return answer_md\n",
        "\n",
        "        # ---------- REFRESH COHORTS ----------\n",
        "\n",
        "        def _ask_refresh(username):\n",
        "            trace_log(f\"ASK _refresh called username={username!r}\")\n",
        "\n",
        "            try:\n",
        "                if not username:\n",
        "                    return gr.update(choices=[], value=None)\n",
        "\n",
        "                names = get_cohorts_for_user(username)\n",
        "                trace_log(f\"ASK _refresh found cohorts={names}\")\n",
        "\n",
        "                if names:\n",
        "                    return gr.update(choices=names, value=names[0])\n",
        "                else:\n",
        "                    return gr.update(choices=[], value=None)\n",
        "\n",
        "            except Exception as e:\n",
        "                trace_log(f\"ASK _refresh ERROR: {e}\")\n",
        "                return gr.update(choices=[], value=None)\n",
        "\n",
        "        # ---------- WIRE CALLBACKS (these lines must stay AFTER the defs) ----------\n",
        "\n",
        "        improve_btn.click(\n",
        "            _improve_prompt,\n",
        "            inputs=[current_user_state, question_box, model_dropdown, current_api_key_state],\n",
        "            outputs=[improved_box, prompt_choice],\n",
        "        )\n",
        "\n",
        "        ask_btn.click(\n",
        "            _ask,\n",
        "            inputs=[\n",
        "                current_user_state,\n",
        "                cohort_dropdown,\n",
        "                model_dropdown,\n",
        "                question_box,\n",
        "                improved_box,\n",
        "                prompt_choice,\n",
        "                current_api_key_state,\n",
        "            ],\n",
        "            outputs=[ask_output],\n",
        "        )\n",
        "\n",
        "        ask_refresh_btn.click(\n",
        "            _ask_refresh,\n",
        "            inputs=[current_user_state],\n",
        "            outputs=[cohort_dropdown],\n",
        "        )\n",
        "\n",
        "# -----------------------------\n",
        "# HISTORY TAB (simple view)\n",
        "# -----------------------------\n",
        "# -----------------------------\n",
        "# HISTORY TAB (simple view)\n",
        "# -----------------------------\n",
        "def build_history_tab():\n",
        "    with gr.Tab(\"History\"):\n",
        "        gr.Markdown(\"### Recent Q&A History\")\n",
        "\n",
        "        # Scope selector so admin can see org-wide history\n",
        "        scope_choice = gr.Radio(\n",
        "            choices=[\"My history only\", \"Org-wide (admin only)\"],\n",
        "            value=\"My history only\",\n",
        "            label=\"View scope\",\n",
        "        )\n",
        "\n",
        "        history_box = gr.Dataframe(\n",
        "            headers=[\"Time (UTC)\", \"Cohort\", \"Question\", \"Model\"],\n",
        "            datatype=[\"str\", \"str\", \"str\", \"str\"],\n",
        "            interactive=False,\n",
        "        )\n",
        "\n",
        "        def _load_history(username: str, role: str, scope: str):\n",
        "            \"\"\"\n",
        "            Load recent history using get_recent_history().\n",
        "\n",
        "            - For normal users: always scoped to their own username.\n",
        "            - For admin: can choose \"My history only\" or \"Org-wide\".\n",
        "            \"\"\"\n",
        "            try:\n",
        "                trace_log(\n",
        "                    f\"HISTORY _load_history called username={username!r}, \"\n",
        "                    f\"role={role!r}, scope={scope!r}\"\n",
        "                )\n",
        "\n",
        "                if not username:\n",
        "                    return []\n",
        "\n",
        "                # Decide how to filter based on role + scope\n",
        "                if role == \"admin\" and scope == \"Org-wide (admin only)\":\n",
        "                    # Admin, org-wide view: don't filter by user_id\n",
        "                    rows = get_recent_history(\n",
        "                        user_id=None,\n",
        "                        cohort_name=None,\n",
        "                        limit=100,\n",
        "                    )\n",
        "                else:\n",
        "                    # Everyone else (or admin in \"My history only\" mode)\n",
        "                    rows = get_recent_history(\n",
        "                        user_id=username,\n",
        "                        cohort_name=None,\n",
        "                        limit=50,\n",
        "                    )\n",
        "\n",
        "                table = []\n",
        "                for r in rows:\n",
        "                    ts = r.get(\"created_at\", \"\") or \"\"\n",
        "                    cohort = r.get(\"cohort_name\", \"\") or \"\"\n",
        "                    # Prefer original_query, fall back to improved_query\n",
        "                    q = r.get(\"original_query\") or r.get(\"improved_query\") or \"\"\n",
        "                    model = r.get(\"chat_model\", \"\") or \"\"\n",
        "\n",
        "                    table.append([ts, cohort, q, model])\n",
        "\n",
        "                return table\n",
        "\n",
        "            except Exception as e:\n",
        "                trace_log(f\"HISTORY _load_history ERROR: {e}\")\n",
        "                return []\n",
        "\n",
        "        # Load on button click to avoid auto-refresh issues\n",
        "        load_btn = gr.Button(\"Refresh History\")\n",
        "        load_btn.click(\n",
        "            _load_history,\n",
        "            inputs=[current_user_state, current_role_state, scope_choice],\n",
        "            outputs=[history_box],\n",
        "        )\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# ADMIN TAB (read-only)\n",
        "# -----------------------------\n",
        "def build_admin_tab():\n",
        "    with gr.Tab(\"Admin\"):\n",
        "        gr.Markdown(\"### Admin View\")\n",
        "\n",
        "        gr.Markdown(\"#### Registered Users\")\n",
        "        users_box = gr.Dataframe(\n",
        "            headers=[\"Username\", \"Role\"],\n",
        "            interactive=False,\n",
        "        )\n",
        "\n",
        "        gr.Markdown(\"#### Cohorts\")\n",
        "        cohorts_box = gr.Dataframe(\n",
        "            headers=[\"Cohort Name\"],\n",
        "            interactive=False,\n",
        "        )\n",
        "\n",
        "        gr.Markdown(\"#### Model Registry\")\n",
        "        models_box = gr.Dataframe(\n",
        "            headers=[\"Provider\", \"Model ID\", \"Type\", \"Enabled\", \"Default\"],\n",
        "            interactive=False,\n",
        "        )\n",
        "\n",
        "        def _load_admin(username, role):\n",
        "            trace_log(f\"ADMIN _load_admin called username={username!r}, role={role!r}\")\n",
        "            if role != \"admin\":\n",
        "                return [], [], []\n",
        "\n",
        "            # Users\n",
        "            try:\n",
        "                conn = get_db_conn()\n",
        "                conn.row_factory = sqlite3.Row\n",
        "                cur = conn.cursor()\n",
        "                cur.execute(\"SELECT username, role FROM users ORDER BY username\")\n",
        "                user_rows = cur.fetchall()\n",
        "                conn.close()\n",
        "                users_table = [[u[\"username\"], u[\"role\"]] for u in user_rows]\n",
        "            except Exception as e:\n",
        "                trace_log(f\"ADMIN load users ERROR: {e}\")\n",
        "                users_table = []\n",
        "\n",
        "            # Cohorts\n",
        "            try:\n",
        "                cohort_names = list_cohorts()\n",
        "                cohorts_table = [[name] for name in cohort_names]\n",
        "            except Exception as e:\n",
        "                trace_log(f\"ADMIN load cohorts ERROR: {e}\")\n",
        "                cohorts_table = []\n",
        "\n",
        "            # Models\n",
        "            try:\n",
        "                models = load_model_registry()\n",
        "                models_table = [\n",
        "                    [\n",
        "                        m[\"provider\"],\n",
        "                        m[\"model_id\"],\n",
        "                        m[\"type\"],\n",
        "                        \"Yes\" if m[\"enabled\"] else \"No\",\n",
        "                        \"Yes\" if m[\"is_default\"] else \"No\",\n",
        "                    ]\n",
        "                    for m in models\n",
        "                ]\n",
        "            except Exception as e:\n",
        "                trace_log(f\"ADMIN load models ERROR: {e}\")\n",
        "                models_table = []\n",
        "\n",
        "            return users_table, cohorts_table, models_table\n",
        "\n",
        "        load_btn = gr.Button(\"Refresh Admin Data\")\n",
        "        load_btn.click(\n",
        "            _load_admin,\n",
        "            inputs=[current_user_state, current_role_state],\n",
        "            outputs=[users_box, cohorts_box, models_box],\n",
        "        )\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# MAIN INTERFACE & STARTUP\n",
        "# -----------------------------\n",
        "def build_interface():\n",
        "    \"\"\"\n",
        "    Build the full Gradio Blocks app with:\n",
        "      - Login screen\n",
        "      - Tabs: Setup, Ask, History, Admin (admin only)\n",
        "      - Logout button to return to login\n",
        "    \"\"\"\n",
        "    with gr.Blocks(title=\"RAG MVP v15\") as demo:\n",
        "        global current_user_state, current_role_state, current_api_key_state\n",
        "        current_user_state = gr.State(\"\")\n",
        "        current_role_state = gr.State(\"\")\n",
        "        current_api_key_state = gr.State(\"\")\n",
        "\n",
        "        # -------- Login View --------\n",
        "        with gr.Column(visible=True) as login_view:\n",
        "            gr.Markdown(\"## RAG MVP v15 — Login\")\n",
        "\n",
        "            username_in = gr.Textbox(label=\"Username\")\n",
        "            password_in = gr.Textbox(label=\"Password\", type=\"password\")\n",
        "            api_key_in = gr.Textbox(\n",
        "                label=\"OpenAI API Key\",\n",
        "                placeholder=\"sk-...\",\n",
        "                type=\"password\",\n",
        "            )\n",
        "            login_btn = gr.Button(\"Login\")\n",
        "            login_status = gr.Markdown()\n",
        "\n",
        "        # -------- Main App View --------\n",
        "        with gr.Column(visible=False) as app_view:\n",
        "            # Top row: title + Logout button\n",
        "            with gr.Row():\n",
        "                header = gr.Markdown(\"## RAG MVP v15\")\n",
        "                logout_btn = gr.Button(\"Logout\")\n",
        "\n",
        "            # Second row: user label\n",
        "            with gr.Row():\n",
        "                gr.Markdown(\"Logged in as:\")\n",
        "                user_label = gr.Markdown()\n",
        "\n",
        "            # Tabs\n",
        "            with gr.Tabs():\n",
        "                with gr.Tab(\"Setup & Cohorts\"):\n",
        "                    build_setup_tab()\n",
        "\n",
        "                build_ask_tab()\n",
        "                build_history_tab()\n",
        "                build_admin_tab()\n",
        "\n",
        "        # -------- Login Logic --------\n",
        "        def _login(username, password, api_key):\n",
        "            \"\"\"\n",
        "            Authenticate user and toggle views.\n",
        "            \"\"\"\n",
        "            try:\n",
        "                user = authenticate_credentials(username, password)\n",
        "            except Exception as e:\n",
        "                trace_log(f\"LOGIN ERROR auth: {e}\")\n",
        "                return (\n",
        "                    f\"❌ Login failed: {e}\",\n",
        "                    gr.update(visible=True),\n",
        "                    gr.update(visible=False),\n",
        "                    \"\",\n",
        "                    \"\",\n",
        "                )\n",
        "\n",
        "            if not user:\n",
        "                trace_log(f\"LOGIN failed for username={username!r}\")\n",
        "                return (\n",
        "                    \"❌ Invalid username or password.\",\n",
        "                    gr.update(visible=True),\n",
        "                    gr.update(visible=False),\n",
        "                    \"\",\n",
        "                    \"\",\n",
        "                )\n",
        "\n",
        "            role = user.get(\"role\", \"user\")\n",
        "            trace_log(f\"LOGIN success username={username!r}, role={role!r}\")\n",
        "\n",
        "            # Show app, hide login, set state\n",
        "            return (\n",
        "                f\"✅ Logged in as **{username}** ({role})\",\n",
        "                gr.update(visible=False),   # hide login view\n",
        "                gr.update(visible=True),    # show app view\n",
        "                username,                   # current_user_state\n",
        "                role,                       # current_role_state\n",
        "            )\n",
        "\n",
        "        # -------- Logout Logic --------\n",
        "        def _logout():\n",
        "            \"\"\"\n",
        "            Clear user-related state and return to login screen.\n",
        "            \"\"\"\n",
        "            trace_log(\"LOGOUT invoked\")\n",
        "\n",
        "            return (\n",
        "                \"\",                         # login_status cleared\n",
        "                gr.update(visible=True),    # show login view\n",
        "                gr.update(visible=False),   # hide app view\n",
        "                \"\",                         # current_user_state cleared\n",
        "                \"\",                         # current_role_state cleared\n",
        "                \"\",                         # current_api_key_state cleared\n",
        "                \"\",                         # user_label cleared\n",
        "            )\n",
        "\n",
        "        # Wire login button\n",
        "        login_btn.click(\n",
        "            _login,\n",
        "            inputs=[username_in, password_in, api_key_in],\n",
        "            outputs=[\n",
        "                login_status,\n",
        "                login_view,\n",
        "                app_view,\n",
        "                current_user_state,\n",
        "                current_role_state,\n",
        "            ],\n",
        "        )\n",
        "\n",
        "        # Keep API key in state separately (not validated here)\n",
        "        def _store_api_key(api_key):\n",
        "            trace_log(\"API key updated in state\")\n",
        "            return api_key\n",
        "\n",
        "        api_key_in.change(\n",
        "            _store_api_key,\n",
        "            inputs=[api_key_in],\n",
        "            outputs=[current_api_key_state],\n",
        "        )\n",
        "\n",
        "        # Show username in header when state changes\n",
        "        def _update_user_label(username, role):\n",
        "            if not username:\n",
        "                return \"\"\n",
        "            return f\"**User:** {username} — **Role:** {role}\"\n",
        "\n",
        "        current_user_state.change(\n",
        "            _update_user_label,\n",
        "            inputs=[current_user_state, current_role_state],\n",
        "            outputs=[user_label],\n",
        "        )\n",
        "\n",
        "        # Wire logout button\n",
        "        logout_btn.click(\n",
        "            _logout,\n",
        "            inputs=[],\n",
        "            outputs=[\n",
        "                login_status,\n",
        "                login_view,\n",
        "                app_view,\n",
        "                current_user_state,\n",
        "                current_role_state,\n",
        "                current_api_key_state,\n",
        "                user_label,\n",
        "            ],\n",
        "        )\n",
        "\n",
        "    return demo\n",
        "\n",
        "\n",
        "# ---- Create & Launch the App ----\n",
        "demo = build_interface()\n",
        "# In Colab: share=False is usually fine; set to True if you want a public link.\n",
        "demo.launch(share=False)\n"
      ],
      "metadata": {
        "id": "nDdkz4szxaoh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMbKHzhPBuZlhTNMR4Bptt9",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}