{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Renlim61/MVP_Product001_2025_Tier120pbc/blob/v18_EPICs/phase1_rag_mvp_v18_EPIC5_2_COMPLETE_FIXED_v11_8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RGtRopF8Io9I",
      "metadata": {
        "id": "RGtRopF8Io9I"
      },
      "source": [
        "## Production Readiness Track\n",
        "##phase1_rag_mvp_v18_EPIC5.2.ipynb\n",
        "## this will be the begining of v18 and its roadmap EPICs\n",
        "## ## product roadmap."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8cd1316",
      "metadata": {
        "id": "e8cd1316"
      },
      "source": [
        "## CELL 0 / STEP 0 – Install & Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "3db881a7",
      "metadata": {
        "id": "3db881a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5634d94-fefd-42ec-efd0-3e7cc8e63a65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Commented out IPython magic to ensure Python compatibility.\n",
        "\n",
        "# Commented out IPython magic to ensure Python compatibility.\n",
        "# CELL 0 / STEP 0 – Install & Imports\n",
        "\n",
        "# ============================================================\n",
        "\n",
        "# Run this once at the top of the notebook (Colab style).\n",
        "\n",
        "#=============================================================\n",
        "%pip install -q faiss-cpu openai gradio PyPDF2 python-docx\n",
        "\n",
        "import os\n",
        "import io\n",
        "import sys\n",
        "import time\n",
        "import json\n",
        "import shutil\n",
        "import pickle\n",
        "import sqlite3\n",
        "from uuid import uuid4\n",
        "from datetime import datetime, timedelta, timezone\n",
        "from typing import List, Dict, Any, Tuple, Optional\n",
        "\n",
        "import numpy as np\n",
        "import faiss\n",
        "import gradio as gr\n",
        "\n",
        "from PyPDF2 import PdfReader\n",
        "from docx import Document as DocxDocument\n",
        "\n",
        "# Global datetime import for the whole notebook\n",
        "from datetime import datetime, timezone, timedelta\n",
        "\n",
        "# Colab detection\n",
        "try:\n",
        "    import google.colab  # type: ignore\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive as colab_drive\n",
        "\n",
        "# OpenAI client (v1 library)\n",
        "from openai import OpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67960dd7",
      "metadata": {
        "id": "67960dd7"
      },
      "source": [
        "## CELL 1 — Execution flags & persistence-safe paths (Colab/Drive)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "2a1e8ef5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2a1e8ef5",
        "outputId": "4699b3ca-e5af-4acc-8654-8b20f9b0fb9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "[v18] Using existing v18 DB: /content/drive/MyDrive/rag_mvp/rag_documents_v18.db\n",
            "PERSISTENCE CONFIG\n",
            "IN_COLAB: True\n",
            "BASE_DIR: /content/drive/MyDrive/rag_mvp\n",
            "DB_PATH : /content/drive/MyDrive/rag_mvp/rag_documents_v18.db\n",
            "INDEX_DIR: /content/drive/MyDrive/rag_mvp/indexes\n",
            "TRACE_LOG_PATH: /content/drive/MyDrive/rag_mvp/debug_trace_v16.log\n"
          ]
        }
      ],
      "source": [
        "# CELL 1 — Execution flags & persistence-safe paths (Colab/Drive)\n",
        "\n",
        "#============================================================\n",
        "# If True, we only run the DB pre-flight checks and stop BEFORE launching Gradio.\n",
        "RUN_PREFLIGHT_ONLY = False\n",
        "\n",
        "import os\n",
        "\n",
        "# ============================================================\n",
        "\n",
        "# PERSISTENCE CONFIG (Colab-safe)\n",
        "\n",
        "# ============================================================\n",
        "\n",
        "# Colab runtime storage under /content is ephemeral. Persist DB + indexes to Drive.\n",
        "USE_DRIVE_PERSISTENCE = True          # Set False only if running locally\n",
        "DRIVE_SUBFOLDER = \"rag_mvp\"           # Folder under MyDrive\n",
        "\n",
        "# If IN_COLAB is not defined for any reason, define it safely.\n",
        "try:\n",
        "    IN_COLAB\n",
        "except NameError:\n",
        "    try:\n",
        "        import google.colab  # type: ignore\n",
        "        IN_COLAB = True\n",
        "    except Exception:\n",
        "        IN_COLAB = False\n",
        "\n",
        "if IN_COLAB and USE_DRIVE_PERSISTENCE:\n",
        "    try:\n",
        "        from google.colab import drive as colab_drive  # type: ignore\n",
        "        colab_drive.mount(\"/content/drive\", force_remount=False)\n",
        "    except Exception as e:\n",
        "        print(\"⚠️ Drive mount issue (continuing):\", e)\n",
        "\n",
        "    BASE_DIR = f\"/content/drive/MyDrive/{DRIVE_SUBFOLDER}\"\n",
        "else:\n",
        "    BASE_DIR = os.path.join(os.getcwd(), DRIVE_SUBFOLDER)\n",
        "\n",
        "os.makedirs(BASE_DIR, exist_ok=True)\n",
        "\n",
        "# Persistent DB paths\n",
        "# - v17 and earlier use: rag_documents.db\n",
        "# - v18 uses: rag_documents_v18.db (side-by-side schema evolution)\n",
        "DB_V17_PATH = os.path.join(BASE_DIR, \"rag_documents.db\")\n",
        "DB_V18_PATH = os.path.join(BASE_DIR, \"rag_documents_v18.db\")\n",
        "\n",
        "# STEP 1 (v18) — Initialize v18 DB once by cloning v17 DB (if present).\n",
        "# This preserves backward compatibility for v17 and prior notebooks.\n",
        "import shutil\n",
        "if not os.path.exists(DB_V18_PATH):\n",
        "    if os.path.exists(DB_V17_PATH):\n",
        "        shutil.copy2(DB_V17_PATH, DB_V18_PATH)\n",
        "        print(f\"[v18] Initialized DB by cloning v17 DB: {DB_V17_PATH} -> {DB_V18_PATH}\")\n",
        "    else:\n",
        "        # No prior DB to clone; v18 will create tables on first use\n",
        "        print(f\"[v18] No v17 DB found at {DB_V17_PATH}. Starting with a fresh v18 DB at {DB_V18_PATH}.\")\n",
        "else:\n",
        "    print(f\"[v18] Using existing v18 DB: {DB_V18_PATH}\")\n",
        "\n",
        "# Active DB path for this notebook/version\n",
        "DB_PATH = DB_V18_PATH# Persistent index/log directories\n",
        "INDEX_DIR = os.path.join(BASE_DIR, \"indexes\")\n",
        "os.makedirs(INDEX_DIR, exist_ok=True)\n",
        "\n",
        "TRACE_LOG_PATH = os.path.join(BASE_DIR, \"debug_trace_v16.log\")\n",
        "\n",
        "print(\"PERSISTENCE CONFIG\")\n",
        "print(\"IN_COLAB:\", IN_COLAB)\n",
        "print(\"BASE_DIR:\", BASE_DIR)\n",
        "print(\"DB_PATH :\", DB_PATH)\n",
        "print(\"INDEX_DIR:\", INDEX_DIR)\n",
        "print(\"TRACE_LOG_PATH:\", TRACE_LOG_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "847b7ca9",
      "metadata": {
        "id": "847b7ca9"
      },
      "source": [
        "## CELL 1.75 — Global Configuration Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "5eed6943",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5eed6943",
        "outputId": "dfb519b7-4528-4123-d185-b7a8142bc61c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CONFIG CONSTANTS LOADED\n",
            "CHUNK_SIZE = 800\n",
            "CHUNK_OVERLAP = 100\n",
            "DEFAULT_EMBED_MODEL = text-embedding-3-small\n"
          ]
        }
      ],
      "source": [
        "# CELL 1.75 — Global Configuration Constants\n",
        "\n",
        "# ============================================================\n",
        "\n",
        "\n",
        "# -------- Chunking --------\n",
        "CHUNK_SIZE = 800          # tokens/characters per chunk (adjust as needed)\n",
        "CHUNK_OVERLAP = 100       # overlap between chunks\n",
        "\n",
        "# -------- Embeddings / Index --------\n",
        "EMBEDDING_DIM = 1536      # OpenAI text-embedding-3-small\n",
        "DEFAULT_EMBED_MODEL = \"text-embedding-3-small\"\n",
        "\n",
        "# -------- Retrieval --------\n",
        "TOP_K = 5\n",
        "# -------- Session Memory (EPIC 3.1) --------\n",
        "SESSION_MEM_MAX_TURNS = 3  # last N Q/A pairs per session (per cohort)\n",
        "\n",
        "# -------- Limits --------\n",
        "MAX_FILES_PER_UPLOAD = 20\n",
        "MAX_FILE_SIZE_MB = 25\n",
        "\n",
        "print(\"CONFIG CONSTANTS LOADED\")\n",
        "print(\"CHUNK_SIZE =\", CHUNK_SIZE)\n",
        "print(\"CHUNK_OVERLAP =\", CHUNK_OVERLAP)\n",
        "print(\"DEFAULT_EMBED_MODEL =\", DEFAULT_EMBED_MODEL)\n",
        "\n",
        "# ============================================================"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22f81759",
      "metadata": {
        "id": "22f81759"
      },
      "source": [
        "## CELL 1.9 — OpenAI Client Factory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "aeb8cd1f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeb8cd1f",
        "outputId": "743dda7c-2eb5-4fb9-d51a-9dd405674b89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ build_openai_client() is defined.\n"
          ]
        }
      ],
      "source": [
        "# CELL 1.9 — OpenAI Client Factory\n",
        "\n",
        "# ============================================================\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "def build_openai_client(api_key: str):\n",
        "    \"\"\"\n",
        "    Returns an OpenAI client using the new SDK interface.\n",
        "    Centralized so ingestion and chat use the same construction.\n",
        "    \"\"\"\n",
        "    if not api_key:\n",
        "        raise ValueError(\"OpenAI API key is required.\")\n",
        "\n",
        "    try:\n",
        "        from openai import OpenAI\n",
        "    except ImportError as e:\n",
        "        raise RuntimeError(\"openai package not installed.\") from e\n",
        "\n",
        "    return OpenAI(api_key=api_key)\n",
        "\n",
        "print(\"✅ build_openai_client() is defined.\")\n",
        "\n",
        "# ============================================================"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9e5573f",
      "metadata": {
        "id": "e9e5573f"
      },
      "source": [
        "## CELL 1.95 — Model Defaults + Resolver"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "78a41ffe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78a41ffe",
        "outputId": "ad3d2978-47df-4afa-ffee-f1217656e815"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ resolve_models() is defined.\n",
            "   CHAT_MODEL_DEFAULT = gpt-4.1-mini\n",
            "   EMBED_MODEL_DEFAULT = text-embedding-3-small\n"
          ]
        }
      ],
      "source": [
        "# CELL 1.95 — Model Defaults + Resolver\n",
        "\n",
        "# ============================================================\n",
        "\n",
        "\n",
        "# Defaults (used across embed + chat if user does not override)\n",
        "EMBED_MODEL_DEFAULT = \"text-embedding-3-small\"\n",
        "CHAT_MODEL_DEFAULT  = \"gpt-4.1-mini\"\n",
        "\n",
        "def resolve_models(chat_model: str | None, embed_model: str | None) -> tuple[str, str]:\n",
        "    \"\"\"\n",
        "    Normalize selected models and apply defaults.\n",
        "    Returns (resolved_chat_model, resolved_embed_model).\n",
        "    \"\"\"\n",
        "    resolved_chat = (chat_model or \"\").strip() or CHAT_MODEL_DEFAULT\n",
        "    resolved_embed = (embed_model or \"\").strip() or EMBED_MODEL_DEFAULT\n",
        "    return resolved_chat, resolved_embed\n",
        "\n",
        "print(\"✅ resolve_models() is defined.\")\n",
        "print(\"   CHAT_MODEL_DEFAULT =\", CHAT_MODEL_DEFAULT)\n",
        "print(\"   EMBED_MODEL_DEFAULT =\", EMBED_MODEL_DEFAULT)\n",
        "\n",
        "# ============================================================"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "910935a7",
      "metadata": {
        "id": "910935a7"
      },
      "source": [
        "## CELL 1.5 / STEP 1.5 – Validate OpenAI Key & Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "a7f16475",
      "metadata": {
        "id": "a7f16475"
      },
      "outputs": [],
      "source": [
        "# CELL 1.5 / STEP 1.5 – Validate OpenAI Key & Models\n",
        "\n",
        "def validate_openai_key_and_models(api_key: str, chat_model: str, embed_model: str) -> str:\n",
        "    \"\"\"\n",
        "    Lightweight validation:\n",
        "    - Instantiate client\n",
        "    - Do a tiny chat completion\n",
        "    - Do a small embedding call\n",
        "    Returns a human-readable status string.\n",
        "    \"\"\"\n",
        "    if not api_key:\n",
        "        return \"❌ Please provide an OpenAI API key.\"\n",
        "\n",
        "    try:\n",
        "        client = build_openai_client(api_key)\n",
        "        resolved_chat, resolved_embed = resolve_models(chat_model, embed_model)\n",
        "\n",
        "        # Tiny chat test\n",
        "        _ = client.chat.completions.create(\n",
        "            model=resolved_chat,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"Model availability test.\"},\n",
        "                {\"role\": \"user\", \"content\": \"Respond with 'OK' only.\"},\n",
        "            ],\n",
        "            max_tokens=2,\n",
        "            temperature=0.0,\n",
        "        )\n",
        "\n",
        "        # Tiny embedding test\n",
        "        _ = client.embeddings.create(\n",
        "            model=resolved_embed,\n",
        "            input=[\"test\"],\n",
        "        )\n",
        "\n",
        "        return f\"✅ OpenAI key valid. Chat model: `{resolved_chat}`, Embed model: `{resolved_embed}`\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"❌ Error validating key/models: {e}\"\n",
        "\n",
        "#============================================================"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eed83aaf",
      "metadata": {
        "id": "eed83aaf"
      },
      "source": [
        "## CELL 1.25 — Soft DB sanity gate (bootstrap + report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "01462054",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01462054",
        "outputId": "db57d60b-0a86-49cb-bef7-e0c41e4aa63a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "DB SANITY GATE — PRE-FLIGHT CHECK\n",
            "================================================================================\n",
            "BASE_DIR: /content/drive/MyDrive/rag_mvp\n",
            "DB_PATH : /content/drive/MyDrive/rag_mvp/rag_documents_v18.db\n",
            "Exists  : True\n",
            "Size    : 229376\n",
            "Modified: Sat Jan  3 00:09:50 2026\n",
            "Tables  : ['app_meta', 'audit_log', 'chat_history', 'cohort_docs', 'cohort_meta', 'cohorts', 'demo_prompts', 'documents', 'eval_questions', 'eval_runs', 'eval_sets', 'model_registry', 'rag_trace_events', 'session_memory_store', 'sqlite_sequence', 'users']\n",
            "✅ DB SANITY GATE PASSED\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# CELL 1.25 — Soft DB sanity gate (bootstrap + report)\n",
        "\n",
        "#============================================================\n",
        "import sqlite3\n",
        "import time\n",
        "\n",
        "REQUIRED_TABLES = [\n",
        "    \"documents\",\n",
        "    \"cohort_docs\",\n",
        "    \"cohorts\",\n",
        "    \"cohort_meta\",\n",
        "    \"users\",\n",
        "    \"chat_history\",\n",
        "    \"audit_log\",\n",
        "    \"model_registry\",\n",
        "]\n",
        "\n",
        "def soft_db_sanity_gate(db_path: str, base_dir: str, required_tables: list[str]) -> None:\n",
        "    print(\"=\"*80)\n",
        "    print(\"DB SANITY GATE — PRE-FLIGHT CHECK\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"BASE_DIR:\", base_dir)\n",
        "    print(\"DB_PATH :\", db_path)\n",
        "\n",
        "    os.makedirs(os.path.dirname(db_path), exist_ok=True)\n",
        "\n",
        "    # Create DB file if missing (do NOT fail on first run)\n",
        "    first_create = not os.path.exists(db_path)\n",
        "    if first_create:\n",
        "        # Touch/create by connecting\n",
        "        conn = sqlite3.connect(db_path)\n",
        "        conn.commit()\n",
        "        conn.close()\n",
        "\n",
        "    # Bootstrap minimal schema so the app can start from a clean environment\n",
        "    conn = sqlite3.connect(db_path)\n",
        "    cur = conn.cursor()\n",
        "\n",
        "    # Minimal table definitions (aligned with MVP expectations)\n",
        "    cur.execute(\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS documents (\n",
        "            id              TEXT PRIMARY KEY,\n",
        "            doc_name        TEXT NOT NULL,\n",
        "            cohort_name     TEXT NOT NULL,\n",
        "            index_id        TEXT NOT NULL,\n",
        "            n_chunks        INTEGER NOT NULL,\n",
        "            embed_model     TEXT NOT NULL,\n",
        "            created_at      TEXT NOT NULL\n",
        "        )\n",
        "    \"\"\")\n",
        "    cur.execute(\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS cohort_docs (\n",
        "            cohort_name TEXT NOT NULL,\n",
        "            doc_name    TEXT NOT NULL\n",
        "        )\n",
        "    \"\"\")\n",
        "    cur.execute(\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS cohorts (\n",
        "            id              INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "            name            TEXT NOT NULL UNIQUE,\n",
        "            description     TEXT,\n",
        "            owner_user_id   TEXT,\n",
        "            created_at      TEXT NOT NULL,\n",
        "            updated_at      TEXT NOT NULL\n",
        "        )\n",
        "    \"\"\")\n",
        "    cur.execute(\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS cohort_meta (\n",
        "            cohort_name    TEXT PRIMARY KEY,\n",
        "            owner_user_id  TEXT,\n",
        "            is_shared      INTEGER DEFAULT 0,\n",
        "            allow_clone    INTEGER DEFAULT 0,\n",
        "            created_ts     TEXT\n",
        "        )\n",
        "    \"\"\")\n",
        "    cur.execute(\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS users (\n",
        "            user_id      TEXT PRIMARY KEY,\n",
        "            display_name TEXT,\n",
        "            role         TEXT,\n",
        "            created_at   TEXT NOT NULL\n",
        "        )\n",
        "    \"\"\")\n",
        "    cur.execute(\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS chat_history (\n",
        "            id             INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "            user_id        TEXT,\n",
        "            role           TEXT,\n",
        "            cohort_name    TEXT,\n",
        "            original_query TEXT,\n",
        "            improved_query TEXT,\n",
        "            which_prompt   TEXT,\n",
        "            answer         TEXT,\n",
        "            chat_model     TEXT,\n",
        "            created_at     TEXT NOT NULL\n",
        "        )\n",
        "    \"\"\")\n",
        "    cur.execute(\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS audit_log (\n",
        "            id        INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "            ts        TEXT NOT NULL,\n",
        "            username  TEXT,\n",
        "            role      TEXT,\n",
        "            action    TEXT NOT NULL,\n",
        "            details   TEXT\n",
        "        )\n",
        "    \"\"\")\n",
        "    cur.execute(\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS model_registry (\n",
        "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "            provider TEXT,\n",
        "            model_id TEXT,\n",
        "            display_name TEXT,\n",
        "            model_type TEXT,\n",
        "            enabled INTEGER DEFAULT 1,\n",
        "            is_default INTEGER DEFAULT 0,\n",
        "            cost_score INTEGER DEFAULT 2,\n",
        "            latency_score INTEGER DEFAULT 2,\n",
        "            max_context_tokens INTEGER,\n",
        "            api_base TEXT,\n",
        "            notes TEXT\n",
        "        )\n",
        "    \"\"\")\n",
        "\n",
        "    conn.commit()\n",
        "\n",
        "    # Inspect tables\n",
        "    cur.execute(\"SELECT name FROM sqlite_master WHERE type='table' ORDER BY name\")\n",
        "    tables = [r[0] for r in cur.fetchall()]\n",
        "    missing = [t for t in required_tables if t not in set(tables)]\n",
        "\n",
        "    # File stats\n",
        "    exists = os.path.exists(db_path)\n",
        "    size = os.path.getsize(db_path) if exists else None\n",
        "    mtime = time.ctime(os.path.getmtime(db_path)) if exists else None\n",
        "\n",
        "    print(\"Exists  :\", exists)\n",
        "    print(\"Size    :\", size)\n",
        "    print(\"Modified:\", mtime)\n",
        "    print(\"Tables  :\", tables)\n",
        "\n",
        "    if missing:\n",
        "        print(\"❌ DB SANITY GATE FAILED — still missing required tables:\", missing)\n",
        "        raise RuntimeError(f\"DB schema incomplete after bootstrap. Missing tables: {missing}\")\n",
        "\n",
        "    print(\"✅ DB SANITY GATE PASSED\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    conn.close()\n",
        "\n",
        "# Run the gate\n",
        "soft_db_sanity_gate(DB_PATH, BASE_DIR, REQUIRED_TABLES)\n",
        "\n",
        "# Optional stop (without raising a traceback)\n",
        "STOP_AFTER_PREFLIGHT = bool(RUN_PREFLIGHT_ONLY)\n",
        "if STOP_AFTER_PREFLIGHT:\n",
        "    print(\"Stopping after pre-flight by configuration (RUN_PREFLIGHT_ONLY=True).\")\n",
        "\n",
        "# ============================================================"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b1a1131",
      "metadata": {
        "id": "0b1a1131"
      },
      "source": [
        "## CELL 2 / STEP 2 – Document Loading & Chunking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "fdcb67f5",
      "metadata": {
        "id": "fdcb67f5"
      },
      "outputs": [],
      "source": [
        "# CELL 2 / STEP 2 – Document Loading & Chunking\n",
        "\n",
        "def load_pdf(file_bytes: bytes) -> str:\n",
        "    reader = PdfReader(io.BytesIO(file_bytes))\n",
        "    texts = []\n",
        "    for page in reader.pages:\n",
        "        try:\n",
        "            txt = page.extract_text() or \"\"\n",
        "        except Exception:\n",
        "            txt = \"\"\n",
        "        texts.append(txt)\n",
        "    return \"\\n\".join(texts)\n",
        "\n",
        "def load_docx(file_bytes: bytes) -> str:\n",
        "    f = io.BytesIO(file_bytes)\n",
        "    doc = DocxDocument(f)\n",
        "    return \"\\n\".join(p.text for p in doc.paragraphs)\n",
        "\n",
        "def load_txt(file_bytes: bytes, encoding: str = \"utf-8\") -> str:\n",
        "    return file_bytes.decode(encoding, errors=\"ignore\")\n",
        "\n",
        "def load_file_to_text(file_obj) -> Tuple[str, str]:\n",
        "    \"\"\"\n",
        "    Accepts either:\n",
        "    - a string filepath (when gr.File(type=\"filepath\") is used), or\n",
        "    - a file-like object with a .name attribute (older behavior).\n",
        "\n",
        "    Returns:\n",
        "      (text_content, original_filename)\n",
        "    \"\"\"\n",
        "    # Case 1: gr.File(type=\"filepath\") -> we get a string path\n",
        "    if isinstance(file_obj, str):\n",
        "        path = file_obj\n",
        "        name = os.path.basename(path)\n",
        "    else:\n",
        "        # Case 2: some object with a .name attribute\n",
        "        path = getattr(file_obj, \"name\", None)\n",
        "        if path is None:\n",
        "            raise ValueError(\"Unsupported file object from uploader.\")\n",
        "        name = os.path.basename(path)\n",
        "\n",
        "    with open(path, \"rb\") as f:\n",
        "        data = f.read()\n",
        "\n",
        "    lower = name.lower()\n",
        "    if lower.endswith(\".pdf\"):\n",
        "        text = load_pdf(data)\n",
        "    elif lower.endswith(\".docx\"):\n",
        "        text = load_docx(data)\n",
        "    elif lower.endswith(\".txt\"):\n",
        "        text = load_txt(data)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported file type for {name}\")\n",
        "\n",
        "    return text, name\n",
        "\n",
        "\n",
        "def chunk_text(text: str, chunk_size: int = CHUNK_SIZE, overlap: int = CHUNK_OVERLAP) -> List[str]:\n",
        "    \"\"\"\n",
        "    Simple sliding-window chunking.\n",
        "    \"\"\"\n",
        "    text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
        "    tokens = text.split()\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(tokens):\n",
        "        end = start + chunk_size\n",
        "        chunk_tokens = tokens[start:end]\n",
        "        chunk = \" \".join(chunk_tokens)\n",
        "        chunks.append(chunk)\n",
        "        start += chunk_size - overlap\n",
        "    return chunks\n",
        "\n",
        "# ============================================================"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f33088b",
      "metadata": {
        "id": "2f33088b"
      },
      "source": [
        "## CELL 3 / STEP 3 – Embedding & FAISS Index Helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "216e3c7c",
      "metadata": {
        "id": "216e3c7c"
      },
      "outputs": [],
      "source": [
        "# CELL 3 / STEP 3 – Embedding & FAISS Index Helpers\n",
        "\n",
        "def embed_texts(\n",
        "    api_key: str,\n",
        "    embed_model: str,\n",
        "    texts: List[str],\n",
        "    batch_size: int = 32,\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Embed a list of texts using OpenAI embeddings.\n",
        "    Returns an ndarray of shape (N, D).\n",
        "    \"\"\"\n",
        "    client = build_openai_client(api_key)\n",
        "    resolved_chat, resolved_embed = resolve_models(\"\", embed_model)\n",
        "\n",
        "    vectors: List[List[float]] = []\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i : i + batch_size]\n",
        "        resp = client.embeddings.create(model=resolved_embed, input=batch)\n",
        "        for d in resp.data:\n",
        "            vectors.append(d.embedding)\n",
        "\n",
        "    arr = np.array(vectors, dtype=\"float32\")\n",
        "    return arr\n",
        "\n",
        "def build_faiss_index(vectors: np.ndarray) -> faiss.IndexFlatIP:\n",
        "    \"\"\"\n",
        "    Build a simple inner-product FAISS index from vectors.\n",
        "    \"\"\"\n",
        "    norm = np.linalg.norm(vectors, axis=1, keepdims=True) + 1e-10\n",
        "    normed = vectors / norm\n",
        "    dim = normed.shape[1]\n",
        "    index = faiss.IndexFlatIP(dim)\n",
        "    index.add(normed)\n",
        "    return index\n",
        "\n",
        "def save_index(index: faiss.IndexFlatIP, index_id: str):\n",
        "    path = os.path.join(INDEX_DIR, f\"{index_id}.faiss\")\n",
        "    faiss.write_index(index, path)\n",
        "\n",
        "def load_index(index_id: str) -> faiss.IndexFlatIP:\n",
        "    path = os.path.join(INDEX_DIR, f\"{index_id}.faiss\")\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"Index file not found: {path}\")\n",
        "    return faiss.read_index(path)\n",
        "\n",
        "def save_metadata(index_id: str, meta: Dict[str, Any]):\n",
        "    path = os.path.join(INDEX_DIR, f\"{index_id}.pkl\")\n",
        "    with open(path, \"wb\") as f:\n",
        "        pickle.dump(meta, f)\n",
        "\n",
        "def load_metadata(index_id: str) -> Dict[str, Any]:\n",
        "    path = os.path.join(INDEX_DIR, f\"{index_id}.pkl\")\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"Metadata file not found: {path}\")\n",
        "    with open(path, \"rb\") as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "# ============================================================"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "636ebf1f",
      "metadata": {
        "id": "636ebf1f"
      },
      "source": [
        "## CELL 4 / STEP 4 – SQLite Persistence for Documents & Cohorts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "0d4e1477",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0d4e1477",
        "outputId": "b6c10f83-96f9-4341-c7f4-be9d0863d0de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STEP 4 starting with DB_PATH: /content/drive/MyDrive/rag_mvp/rag_documents_v18.db\n"
          ]
        }
      ],
      "source": [
        "# CELL 4 / STEP 4 – SQLite Persistence for Documents & Cohorts\n",
        "import os\n",
        "import sqlite3\n",
        "from uuid import uuid4\n",
        "\n",
        "assert \"BASE_DIR\" in globals(), \"BASE_DIR is not defined. Run CELL 1 first.\"\n",
        "assert \"DB_PATH\"  in globals(), \"DB_PATH is not defined. Run CELL 1 first.\"\n",
        "print(\"STEP 4 starting with DB_PATH:\", DB_PATH)\n",
        "\n",
        "def get_db_conn():\n",
        "    # Ensure BASE_DIR exists (defined in CELL 1)\n",
        "    os.makedirs(BASE_DIR, exist_ok=True)\n",
        "    # Ensure DB folder exists\n",
        "    os.makedirs(os.path.dirname(DB_PATH), exist_ok=True)\n",
        "    return sqlite3.connect(DB_PATH)\n",
        "\n",
        "\n",
        "\n",
        "def ensure_docs_table():\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "    cur.execute(\n",
        "        \"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS documents (\n",
        "            id              TEXT PRIMARY KEY,\n",
        "            doc_name        TEXT NOT NULL,\n",
        "            cohort_name     TEXT NOT NULL,\n",
        "            index_id        TEXT NOT NULL,\n",
        "            n_chunks        INTEGER NOT NULL,\n",
        "            embed_model     TEXT NOT NULL,\n",
        "            created_at      TEXT NOT NULL\n",
        "        )\n",
        "        \"\"\"\n",
        "    )\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "\n",
        "def ensure_cohort_table():\n",
        "    \"\"\"\n",
        "    Ensure both:\n",
        "      - cohorts: cohort metadata (required by v16 tests)\n",
        "      - cohort_docs: mapping of cohort_name -> doc_name (used by the app)\n",
        "    \"\"\"\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "\n",
        "    # ---- New table required by tests ----\n",
        "    cur.execute(\n",
        "        \"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS cohorts (\n",
        "            id              INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "            name            TEXT NOT NULL UNIQUE,\n",
        "            description     TEXT,\n",
        "            owner_user_id   TEXT,\n",
        "            created_at      TEXT NOT NULL,\n",
        "            updated_at      TEXT NOT NULL\n",
        "        )\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    # ---- Existing mapping table (unchanged behavior) ----\n",
        "    cur.execute(\n",
        "        \"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS cohort_docs (\n",
        "            cohort_name TEXT NOT NULL,\n",
        "            doc_name    TEXT NOT NULL\n",
        "        )\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "\n",
        "def list_cohorts() -> List[str]:\n",
        "    # We keep existing behavior: list distinct names from cohort_docs\n",
        "    ensure_cohort_table()\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "    cur.execute(\"SELECT DISTINCT cohort_name FROM cohort_docs ORDER BY cohort_name ASC\")\n",
        "    rows = cur.fetchall()\n",
        "    conn.close()\n",
        "    print(\"Cohorts:\", list_cohorts()[:20], \"count=\", len(list_cohorts()))\n",
        "\n",
        "    return [r[0] for r in rows]\n",
        "\n",
        "\n",
        "def list_docs_in_cohort(cohort_name: str) -> List[str]:\n",
        "    ensure_cohort_table()\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "    cur.execute(\n",
        "        \"SELECT doc_name FROM cohort_docs WHERE cohort_name = ? ORDER BY doc_name ASC\",\n",
        "        (cohort_name,),\n",
        "    )\n",
        "    rows = cur.fetchall()\n",
        "    conn.close()\n",
        "    return [r[0] for r in rows]\n",
        "\n",
        "\n",
        "def add_docs_to_cohort(cohort_name: str, doc_names: List[str]):\n",
        "    ensure_cohort_table()\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "    for dn in doc_names:\n",
        "        cur.execute(\n",
        "            \"INSERT INTO cohort_docs (cohort_name, doc_name) VALUES (?, ?)\",\n",
        "            (cohort_name, dn),\n",
        "        )\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "\n",
        "def rename_cohort(old_name: str, new_name: str):\n",
        "    ensure_cohort_table()\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "    cur.execute(\n",
        "        \"UPDATE cohort_docs SET cohort_name = ? WHERE cohort_name = ?\",\n",
        "        (new_name, old_name),\n",
        "    )\n",
        "    cur.execute(\n",
        "        \"UPDATE documents SET cohort_name = ? WHERE cohort_name = ?\",\n",
        "        (new_name, old_name),\n",
        "    )\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "\n",
        "def delete_cohort(cohort_name: str, reassign_to: Optional[str] = None) -> str:\n",
        "    \"\"\"\n",
        "    Delete a cohort. If reassign_to is provided, documents move there.\n",
        "    Otherwise, documents are deleted (and their indexes removed).\n",
        "    NOTE: This is intentionally explicit to avoid orphaned docs.\n",
        "    \"\"\"\n",
        "    ensure_docs_table()\n",
        "    ensure_cohort_table()\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "\n",
        "    cur.execute(\n",
        "        \"SELECT id, index_id FROM documents WHERE cohort_name = ?\",\n",
        "        (cohort_name,),\n",
        "    )\n",
        "    docs = cur.fetchall()\n",
        "\n",
        "    if reassign_to:\n",
        "        # Just move docs\n",
        "        cur.execute(\n",
        "            \"UPDATE documents SET cohort_name = ? WHERE cohort_name = ?\",\n",
        "            (reassign_to, cohort_name),\n",
        "        )\n",
        "        cur.execute(\n",
        "            \"UPDATE cohort_docs SET cohort_name = ? WHERE cohort_name = ?\",\n",
        "            (reassign_to, cohort_name),\n",
        "        )\n",
        "        msg = f\"✅ Cohort '{cohort_name}' renamed/reassigned to '{reassign_to}'. No indexes deleted.\"\n",
        "    else:\n",
        "        # Delete docs and indexes\n",
        "        for doc_id, index_id in docs:\n",
        "            # Remove index & metadata\n",
        "            faiss_path = os.path.join(INDEX_DIR, f\"{index_id}.faiss\")\n",
        "            pkl_path = os.path.join(INDEX_DIR, f\"{index_id}.pkl\")\n",
        "            for p in [faiss_path, pkl_path]:\n",
        "                if os.path.exists(p):\n",
        "                    os.remove(p)\n",
        "            cur.execute(\"DELETE FROM documents WHERE id = ?\", (doc_id,))\n",
        "\n",
        "        cur.execute(\"DELETE FROM cohort_docs WHERE cohort_name = ?\", (cohort_name,))\n",
        "        msg = f\"✅ Cohort '{cohort_name}' and its documents/indexes were deleted.\"\n",
        "\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "    return msg\n",
        "\n",
        "\n",
        "def register_document(\n",
        "    doc_name: str,\n",
        "    cohort_name: str,\n",
        "    index_id: str,\n",
        "    n_chunks: int,\n",
        "    embed_model: str,\n",
        "):\n",
        "    ensure_docs_table()\n",
        "    ensure_cohort_table()\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "    doc_id = str(uuid4())\n",
        "\n",
        "    # ✅ Use dt.datetime (module alias) so the bottom `import datetime`\n",
        "    #    in the self-test cell cannot break this.\n",
        "    created_at = dt.datetime.now(dt.timezone.utc).isoformat()\n",
        "\n",
        "    cur.execute(\n",
        "        \"\"\"\n",
        "        INSERT INTO documents (id, doc_name, cohort_name, index_id, n_chunks,\n",
        "                               embed_model, created_at)\n",
        "        VALUES (?, ?, ?, ?, ?, ?, ?)\n",
        "        \"\"\",\n",
        "        (doc_id, doc_name, cohort_name, index_id, n_chunks, embed_model, created_at),\n",
        "    )\n",
        "    cur.execute(\n",
        "        \"INSERT INTO cohort_docs (cohort_name, doc_name) VALUES (?, ?)\",\n",
        "        (cohort_name, doc_name),\n",
        "    )\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "    return doc_id\n",
        "\n",
        "\n",
        "def get_doc_index_id(doc_name: str, cohort_name: str) -> Optional[str]:\n",
        "    ensure_docs_table()\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "    cur.execute(\n",
        "        \"\"\"\n",
        "        SELECT index_id\n",
        "        FROM documents\n",
        "        WHERE doc_name = ? AND cohort_name = ?\n",
        "        \"\"\",\n",
        "        (doc_name, cohort_name),\n",
        "    )\n",
        "    row = cur.fetchone()\n",
        "    conn.close()\n",
        "    if row:\n",
        "        return row[0]\n",
        "    return None\n",
        "\n",
        "\n",
        "\n",
        "def list_index_ids_for_doc(doc_name: str, cohort_name: str) -> List[str]:\n",
        "    \"\"\"Return all index_id values for a given (cohort_name, doc_name).\"\"\"\n",
        "    ensure_docs_table()\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "    cur.execute(\n",
        "        \"\"\"\n",
        "        SELECT index_id\n",
        "        FROM documents\n",
        "        WHERE doc_name = ? AND cohort_name = ?\n",
        "        \"\"\",\n",
        "        (doc_name, cohort_name),\n",
        "    )\n",
        "    rows = cur.fetchall() or []\n",
        "    conn.close()\n",
        "    return [r[0] for r in rows if r and r[0]]\n",
        "\n",
        "\n",
        "def delete_index_artifacts(index_id: str) -> None:\n",
        "    \"\"\"Best-effort delete of on-disk FAISS + metadata artifacts for an index_id.\"\"\"\n",
        "    try:\n",
        "        faiss_path = os.path.join(INDEX_DIR, f\"{index_id}.faiss\")\n",
        "        meta_path = os.path.join(INDEX_DIR, f\"{index_id}.pkl\")\n",
        "        if os.path.exists(faiss_path):\n",
        "            os.remove(faiss_path)\n",
        "        if os.path.exists(meta_path):\n",
        "            os.remove(meta_path)\n",
        "    except Exception as e:\n",
        "        trace_log(f\"delete_index_artifacts WARNING index_id={index_id}: {e}\")\n",
        "\n",
        "\n",
        "def overwrite_document_in_cohort(\n",
        "    cohort_name: str,\n",
        "    doc_name: str,\n",
        ") -> int:\n",
        "    \"\"\"Remove existing records + artifacts for (cohort_name, doc_name).\n",
        "\n",
        "    Returns the number of existing document rows removed.\n",
        "    \"\"\"\n",
        "    ensure_docs_table()\n",
        "    ensure_cohort_table()\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "\n",
        "    # Gather existing index ids to clean up artifacts\n",
        "    index_ids = list_index_ids_for_doc(doc_name=doc_name, cohort_name=cohort_name)\n",
        "\n",
        "    # Delete DB rows (documents + cohort_docs mapping)\n",
        "    cur.execute(\n",
        "        \"\"\"DELETE FROM documents WHERE doc_name = ? AND cohort_name = ?\"\"\",\n",
        "        (doc_name, cohort_name),\n",
        "    )\n",
        "    deleted_docs = cur.rowcount or 0\n",
        "\n",
        "    cur.execute(\n",
        "        \"\"\"DELETE FROM cohort_docs WHERE cohort_name = ? AND doc_name = ?\"\"\",\n",
        "        (cohort_name, doc_name),\n",
        "    )\n",
        "\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "    # Delete index artifacts (best-effort)\n",
        "    for ix in index_ids:\n",
        "        delete_index_artifacts(ix)\n",
        "\n",
        "    return deleted_docs\n",
        "\n",
        "\n",
        "def list_all_documents() -> List[Tuple[str, str, str]]:\n",
        "    \"\"\"\n",
        "    Return list of (doc_name, cohort_name, created_at).\n",
        "    \"\"\"\n",
        "    ensure_docs_table()\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "    cur.execute(\n",
        "        \"SELECT doc_name, cohort_name, created_at FROM documents ORDER BY created_at DESC\"\n",
        "    )\n",
        "    rows = cur.fetchall()\n",
        "    conn.close()\n",
        "    return rows\n",
        "# ------------------------------------------------------------\n",
        "# SCHEMA BOOTSTRAP (ensures required tables exist for tests)\n",
        "# ------------------------------------------------------------\n",
        "ensure_docs_table()\n",
        "ensure_cohort_table()\n",
        "\n",
        "# ============================================================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "61b587c0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61b587c0",
        "outputId": "0e3b0e0b-0fa5-47c4-975b-e96c3b88c8fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ STEP 4.5 complete: EPIC 3.6–4.1 tables ensured.\n"
          ]
        }
      ],
      "source": [
        "# CELL 4.5 / STEP 4.5 – EPIC 3.6–4.1 DB Extensions (Session Memory, Traces, Eval)\n",
        "\n",
        "import json\n",
        "import time\n",
        "import sqlite3\n",
        "\n",
        "def ensure_epic_tables():\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "\n",
        "    # ============================================================\n",
        "    # EPIC 3.6 – Optional persisted session memory (disabled by default)\n",
        "    # ============================================================\n",
        "    cur.execute(\n",
        "        \"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS session_memory_store (\n",
        "            user_id     TEXT NOT NULL,\n",
        "            scope       TEXT NOT NULL DEFAULT 'session',\n",
        "            memory_json TEXT NOT NULL,\n",
        "            updated_at  TEXT NOT NULL,\n",
        "            expires_at  TEXT,\n",
        "            PRIMARY KEY (user_id, scope)\n",
        "        )\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    # ============================================================\n",
        "    # EPIC 3.7 – Trace events (lightweight observability)\n",
        "    # ============================================================\n",
        "    cur.execute(\n",
        "        \"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS rag_trace_events (\n",
        "            trace_id        TEXT PRIMARY KEY,\n",
        "            created_at      TEXT NOT NULL,\n",
        "            user_id         TEXT,\n",
        "            cohort_name     TEXT,\n",
        "            model_id        TEXT,\n",
        "            provenance_mode TEXT,\n",
        "            memory_scope    TEXT,\n",
        "            memory_used     INTEGER,\n",
        "            rag_used        INTEGER,\n",
        "            retrieved_k     INTEGER,\n",
        "            retrieval_query TEXT,\n",
        "            note            TEXT\n",
        "        )\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    # ============================================================\n",
        "    # EPIC 4.1 – Evaluation harness tables\n",
        "    # ============================================================\n",
        "    cur.execute(\n",
        "        \"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS eval_sets (\n",
        "            eval_set_id TEXT PRIMARY KEY,\n",
        "            name        TEXT NOT NULL,\n",
        "            description TEXT,\n",
        "            created_at  TEXT NOT NULL\n",
        "        )\n",
        "        \"\"\"\n",
        "    )\n",
        "    cur.execute(\n",
        "        \"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS eval_questions (\n",
        "            eval_q_id        TEXT PRIMARY KEY,\n",
        "            eval_set_id      TEXT NOT NULL,\n",
        "            cohort_name      TEXT,\n",
        "            question         TEXT NOT NULL,\n",
        "            provenance_mode  TEXT NOT NULL DEFAULT 'Explain Sources',\n",
        "            expect_evidence  INTEGER NOT NULL DEFAULT 1,\n",
        "            expect_refusal   INTEGER NOT NULL DEFAULT 0,\n",
        "            created_at       TEXT NOT NULL,\n",
        "            FOREIGN KEY(eval_set_id) REFERENCES eval_sets(eval_set_id)\n",
        "        )\n",
        "        \"\"\"\n",
        "    )\n",
        "    cur.execute(\n",
        "        \"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS eval_runs (\n",
        "            eval_run_id   TEXT PRIMARY KEY,\n",
        "            eval_set_id   TEXT NOT NULL,\n",
        "            model_id      TEXT,\n",
        "            created_at    TEXT NOT NULL,\n",
        "            summary_json  TEXT NOT NULL,\n",
        "            FOREIGN KEY(eval_set_id) REFERENCES eval_sets(eval_set_id)\n",
        "        )\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "ensure_epic_tables()\n",
        "print(\"✅ STEP 4.5 complete: EPIC 3.6–4.1 tables ensured.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c18c3dd",
      "metadata": {
        "id": "6c18c3dd"
      },
      "source": [
        "## CELL 4.5 / STEP 4.5 – Users & Chat History (7-Day Retention)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "55c50860",
      "metadata": {
        "id": "55c50860"
      },
      "outputs": [],
      "source": [
        "# CELL 4.5 / STEP 4.5 – Users & Chat History (7-Day Retention)\n",
        "\n",
        "from dataclasses import dataclass\n",
        "import datetime as dt\n",
        "\n",
        "# ============================================================\n",
        "# USER IDENTITY MODEL\n",
        "\n",
        "@dataclass\n",
        "class SessionUser:\n",
        "    username: str | None = None\n",
        "    role: str = \"anonymous\"   # \"anonymous\", \"user\", \"admin\"\n",
        "\n",
        "    @property\n",
        "    def is_authenticated(self) -> bool:\n",
        "        return bool(self.username and str(self.username).strip()) and self.role in (\"user\", \"admin\")\n",
        "\n",
        "    @property\n",
        "    def is_admin(self) -> bool:\n",
        "        return self.role == \"admin\"\n",
        "\n",
        "\n",
        "def require_login(session_user, action: str = \"access this feature\") -> SessionUser:\n",
        "    \"\"\"Enforce authenticated access (no access for anonymous users).\n",
        "\n",
        "    Accepts SessionUser, dict, or None and returns a normalized SessionUser.\n",
        "    Raises PermissionError if not authenticated.\n",
        "    \"\"\"\n",
        "    if isinstance(session_user, SessionUser):\n",
        "        su = session_user\n",
        "    elif isinstance(session_user, dict):\n",
        "        su = SessionUser(\n",
        "            username=((session_user.get(\"username\") or \"\").strip() or None),\n",
        "            role=(session_user.get(\"role\") or \"anonymous\"),\n",
        "        )\n",
        "    else:\n",
        "        su = SessionUser(username=None, role=\"anonymous\")\n",
        "\n",
        "    if not su.is_authenticated:\n",
        "        raise PermissionError(f\"Login required to {action}.\")\n",
        "    return su\n",
        "\n",
        "# MVP in-memory auth store (will be replaced later by ICAM/SSO)\n",
        "USERS = {\n",
        "    \"admin\": {\"password\": \"admin123\", \"role\": \"admin\"},\n",
        "    \"demo1\":  {\"password\": \"demo1123\",  \"role\": \"user\"},\n",
        "    \"demo2\": {\"password\": \"demo2123\", \"role\": \"user\"},\n",
        "}\n",
        "\n",
        "# ============================================================\n",
        "# EPIC 3.1 — Session-scoped memory helpers (in-memory only)\n",
        "\n",
        "# ============================================================\n",
        "\n",
        "def init_session_memory_state() -> dict:\n",
        "    \"\"\"\n",
        "    Session memory scope is configurable: per cohort (default) or per session.  # EPIC 3.3\n",
        "    Shape:\n",
        "      {\"cohort_name\": <str|None>, \"turns\": [{\"q\": str, \"a\": str}, ...]}\n",
        "    \"\"\"\n",
        "    return {\"enabled\": True, \"scope\": \"cohort\", \"cohort_name\": None, \"turns\": [], \"max_turns\": SESSION_MEM_MAX_TURNS}  # EPIC 3.3\n",
        "\n",
        "\n",
        "def format_session_memory_prompt(turns: list[dict]) -> str:\n",
        "    \"\"\"\n",
        "    Convert last N Q/A pairs into a compact system prompt appendix.\n",
        "    Keep it short—this is \"lightweight memory\".\n",
        "    \"\"\"\n",
        "    if not turns:\n",
        "        return \"\"\n",
        "\n",
        "    lines = []\n",
        "    lines.append(\"You have access to the prior Q/A from this session (same cohort).\")\n",
        "    lines.append(\"Use it only to maintain continuity if relevant; do not invent facts.\\n\")\n",
        "\n",
        "    last_q = (turns[-1].get(\"q\") or \"\").strip()\n",
        "    if last_q:\n",
        "        lines.append(f\"Last user question (most recent): {last_q}\")\n",
        "        lines.append(\"\")\n",
        "\n",
        "\n",
        "    for i, t in enumerate(turns, start=1):\n",
        "        q = (t.get(\"q\") or \"\").strip()\n",
        "        a = (t.get(\"a\") or \"\").strip()\n",
        "        if not q and not a:\n",
        "            continue\n",
        "        lines.append(f\"Session Q{i}: {q}\")\n",
        "        lines.append(f\"Session A{i}: {a}\")\n",
        "        lines.append(\"\")\n",
        "    return \"\\n\".join(lines).strip()\n",
        "\n",
        "\n",
        "def update_session_memory_state(mem: dict, cohort_name: str, q: str, a: str, max_turns: int) -> dict:\n",
        "    \"\"\"\n",
        "    - If cohort changes, clear memory\n",
        "    - Append the new Q/A\n",
        "    - Trim to last max_turns\n",
        "    \"\"\"\n",
        "    if not isinstance(mem, dict):\n",
        "        mem = init_session_memory_state()\n",
        "\n",
        "    # EPIC 3.2 — Admin toggle: if disabled, keep memory empty and do not append\n",
        "    if not mem.get(\"enabled\", True):\n",
        "        mem[\"cohort_name\"] = cohort_name\n",
        "        mem[\"turns\"] = []\n",
        "        return mem\n",
        "\n",
        "    cur_cohort = mem.get(\"cohort_name\")\n",
        "    scope = (mem.get(\"scope\") or \"cohort\").lower()  # EPIC 3.3\n",
        "    if scope == \"cohort\" and cur_cohort != cohort_name:\n",
        "        mem = init_session_memory_state()\n",
        "        mem[\"cohort_name\"] = cohort_name\n",
        "    else:\n",
        "        mem[\"cohort_name\"] = cohort_name\n",
        "\n",
        "    turns = mem.get(\"turns\") or []\n",
        "    turns.append({\"q\": (q or \"\").strip(), \"a\": (a or \"\").strip()})\n",
        "\n",
        "    # Trim\n",
        "    if max_turns and len(turns) > max_turns:\n",
        "        turns = turns[-max_turns:]\n",
        "\n",
        "    mem[\"turns\"] = turns\n",
        "    return mem\n",
        "\n",
        "# ============================================================\n",
        "# USERS TABLE\n",
        "\n",
        "def ensure_user_table():\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "    cur.execute(\n",
        "        \"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS users (\n",
        "            user_id      TEXT PRIMARY KEY,\n",
        "            display_name TEXT,\n",
        "            role         TEXT,\n",
        "            created_at   TEXT NOT NULL\n",
        "        )\n",
        "        \"\"\"\n",
        "    )\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "\n",
        "def upsert_user(user_id: str, role: str, display_name: Optional[str] = None):\n",
        "    \"\"\"\n",
        "    Basic ICAM-ready user record.\n",
        "    Inserts new or updates existing users.\n",
        "    \"\"\"\n",
        "    if not user_id:\n",
        "        return\n",
        "\n",
        "    ensure_user_table()\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "    now = dt.datetime.now(dt.timezone.utc).isoformat()\n",
        "\n",
        "    cur.execute(\n",
        "        \"\"\"\n",
        "        INSERT INTO users (user_id, display_name, role, created_at)\n",
        "        VALUES (?, ?, ?, ?)\n",
        "        ON CONFLICT(user_id) DO UPDATE SET\n",
        "            display_name = COALESCE(?, users.display_name),\n",
        "            role = COALESCE(?, users.role)\n",
        "        \"\"\",\n",
        "        (user_id, display_name, role, now, display_name, role),\n",
        "    )\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "# ============================================================\n",
        "# CHAT HISTORY TABLE\n",
        "\n",
        "def ensure_chat_history_table():\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "    cur.execute(\n",
        "        \"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS chat_history (\n",
        "            id             INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "            user_id        TEXT,\n",
        "            role           TEXT,\n",
        "            cohort_name    TEXT,\n",
        "            original_query TEXT,\n",
        "            improved_query TEXT,\n",
        "            which_prompt   TEXT,\n",
        "            answer         TEXT,\n",
        "            chat_model     TEXT,\n",
        "            created_at     TEXT NOT NULL\n",
        "        )\n",
        "        \"\"\"\n",
        "    )\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "# ============================================================\n",
        "# 7-DAY RETENTION\n",
        "\n",
        "def prune_chat_history(days: int = 7):\n",
        "    ensure_chat_history_table()\n",
        "    cutoff = dt.datetime.now(dt.timezone.utc) - dt.timedelta(days=days)\n",
        "    cutoff_iso = cutoff.isoformat()\n",
        "\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "    cur.execute(\"DELETE FROM chat_history WHERE created_at < ?\", (cutoff_iso,))\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "# ============================================================\n",
        "#v14 BEHAVIOR: Save Chat Interaction (DETAILED LOGGING)\n",
        "\n",
        "def save_chat_interaction(\n",
        "    user_id: str,\n",
        "    role: str,\n",
        "    cohort_name: str,\n",
        "    original_query: str,\n",
        "    improved_query: str,\n",
        "    which_prompt: str,\n",
        "    answer: str,\n",
        "    chat_model: str,\n",
        "):\n",
        "    \"\"\"\n",
        "    This is your existing v16 logging mechanism.\n",
        "    Now fully preserved and compatible with v16.\n",
        "    \"\"\"\n",
        "    ensure_chat_history_table()\n",
        "    prune_chat_history(days=7)\n",
        "\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "    now = dt.datetime.now(dt.timezone.utc).isoformat()\n",
        "\n",
        "    cur.execute(\n",
        "        \"\"\"\n",
        "        INSERT INTO chat_history (\n",
        "            user_id, role, cohort_name, original_query, improved_query,\n",
        "            which_prompt, answer, chat_model, created_at\n",
        "        )\n",
        "        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
        "        \"\"\",\n",
        "        (\n",
        "            user_id or None,\n",
        "            role or None,\n",
        "            cohort_name or None,\n",
        "            original_query,\n",
        "            improved_query,\n",
        "            which_prompt,\n",
        "            answer,\n",
        "            chat_model,\n",
        "            now,\n",
        "        ),\n",
        "    )\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "# ============================================================\n",
        "# v16 REQUIRED FUNCTIONS – FLEXIBLE SIGNATURES\n",
        "\n",
        "def _extract_user_id(user: Any) -> str:\n",
        "    \"\"\"\n",
        "    Helper to derive a stable user_id from various representations.\n",
        "    \"\"\"\n",
        "    if user is None:\n",
        "        return \"anonymous\"\n",
        "\n",
        "    # dict-like\n",
        "    if isinstance(user, dict):\n",
        "        for key in (\"username\", \"user_id\", \"name\"):\n",
        "            v = user.get(key)\n",
        "            if v:\n",
        "                return str(v)\n",
        "\n",
        "    # attribute-based (SessionUser or other objects)\n",
        "    for attr in (\"username\", \"user_id\", \"name\"):\n",
        "        try:\n",
        "            v = getattr(user, attr, None)\n",
        "            if v:\n",
        "                return str(v)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # mapping-like (sqlite3.Row, etc.)\n",
        "    try:\n",
        "        for key in (\"username\", \"user_id\", \"name\"):\n",
        "            if key in user:\n",
        "                v = user[key]\n",
        "                if v:\n",
        "                    return str(v)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    return \"anonymous\"\n",
        "\n",
        "\n",
        "def save_chat_history(\n",
        "    messages=None,\n",
        "    *args,\n",
        "    user: Any = None,\n",
        "    user_id: Optional[str] = None,\n",
        "    cohort_name: str = \"default\",\n",
        "    cohort: Optional[str] = None,\n",
        "    **kwargs,\n",
        "):\n",
        "    \"\"\"\n",
        "    v16 Test Suite Requirement:\n",
        "    Supports BOTH:\n",
        "      1) save_chat_history(messages=[{role, content}, ...], user=..., cohort=...)\n",
        "      2) save_chat_history(user=..., cohort=..., question=\"Q\", answer=\"A\", model_used=\"...\")\n",
        "\n",
        "    In (2) we write a single row using the Q/A fields.\n",
        "    \"\"\"\n",
        "    ensure_chat_history_table()\n",
        "    prune_chat_history(days=7)\n",
        "\n",
        "    # Normalize cohort alias\n",
        "    if cohort is not None:\n",
        "        cohort_name = cohort\n",
        "\n",
        "    # Derive user_id if needed\n",
        "    if user_id is None:\n",
        "        user_id = _extract_user_id(user)\n",
        "\n",
        "    # ---- Path 1: test-style call with question/answer/model_used ----\n",
        "    question = kwargs.get(\"question\")\n",
        "    answer = kwargs.get(\"answer\")\n",
        "    model_used = kwargs.get(\"model_used\")\n",
        "\n",
        "    if messages is None and (question is not None or answer is not None):\n",
        "        now = dt.datetime.now(dt.timezone.utc).isoformat()\n",
        "        conn = get_db_conn()\n",
        "        cur = conn.cursor()\n",
        "\n",
        "        cur.execute(\n",
        "            \"\"\"\n",
        "            INSERT INTO chat_history (\n",
        "                user_id, role, cohort_name, original_query,\n",
        "                improved_query, which_prompt, answer,\n",
        "                chat_model, created_at\n",
        "            )\n",
        "            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
        "            \"\"\",\n",
        "            (\n",
        "                user_id,          # user_id\n",
        "                \"user\",           # role\n",
        "                cohort_name,      # cohort_name\n",
        "                question or \"\",   # original_query\n",
        "                None,             # improved_query\n",
        "                None,             # which_prompt\n",
        "                answer or \"\",     # answer\n",
        "                model_used,       # chat_model\n",
        "                now,              # created_at\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        conn.commit()\n",
        "        conn.close()\n",
        "        return  # We're done for this style of call\n",
        "\n",
        "    # ---- Path 2: normal messages-based usage ----\n",
        "    if messages is None:\n",
        "        messages = kwargs.get(\"messages\", None)\n",
        "\n",
        "    # If still no messages, treat as no-op\n",
        "    if messages is None:\n",
        "        return\n",
        "\n",
        "    now = dt.datetime.now(dt.timezone.utc).isoformat()\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "\n",
        "    for m in messages:\n",
        "        role = m.get(\"role\", \"user\")\n",
        "        content = m.get(\"content\", \"\")\n",
        "\n",
        "        cur.execute(\n",
        "            \"\"\"\n",
        "            INSERT INTO chat_history (\n",
        "                user_id, role, cohort_name, original_query,\n",
        "                improved_query, which_prompt, answer,\n",
        "                chat_model, created_at\n",
        "            )\n",
        "            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
        "            \"\"\",\n",
        "            (\n",
        "                user_id,\n",
        "                role,\n",
        "                cohort_name,\n",
        "                content,   # original_query\n",
        "                None,\n",
        "                None,\n",
        "                None,\n",
        "                None,\n",
        "                now,\n",
        "            ),\n",
        "        )\n",
        "\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "    # Update session memory (EPIC 3.1)\n",
        "    session_mem = update_session_memory_state(\n",
        "        mem=session_mem,\n",
        "        cohort_name=cohort_name,\n",
        "        q=final_question,\n",
        "        a=raw_answer or \"\",\n",
        "        max_turns=int((session_mem or {}).get('max_turns') or SESSION_MEM_MAX_TURNS),  # EPIC 3.3\n",
        "\n",
        "    )\n",
        "\n",
        "\n",
        "def load_chat_history(\n",
        "    *args,\n",
        "    **kwargs,\n",
        "):\n",
        "    \"\"\"\n",
        "    v16 Test Suite Requirement:\n",
        "    Returns list[dict] with keys: role, content, created_at.\n",
        "\n",
        "    For simplicity and maximum compatibility with the tests,\n",
        "    we ignore user/cohort filters here and just return the\n",
        "    oldest messages up to `limit`.\n",
        "    \"\"\"\n",
        "    ensure_chat_history_table()\n",
        "\n",
        "    limit = int(kwargs.get(\"limit\", 50))\n",
        "\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "\n",
        "    cur.execute(\n",
        "        \"\"\"\n",
        "        SELECT role, original_query, created_at\n",
        "        FROM chat_history\n",
        "        ORDER BY created_at ASC\n",
        "        LIMIT ?\n",
        "        \"\"\",\n",
        "        (limit,),\n",
        "    )\n",
        "\n",
        "    rows = cur.fetchall()\n",
        "    conn.close()\n",
        "\n",
        "    return [\n",
        "        {\"role\": r[0], \"content\": r[1], \"created_at\": r[2]}\n",
        "        for r in rows\n",
        "    ]\n",
        "\n",
        "# ============================================================\n",
        "# RECENT HISTORY (Admin screens / Debug UI)\n",
        "\n",
        "def get_recent_history(\n",
        "    user_id: Optional[str] = None,\n",
        "    cohort_name: Optional[str] = None,\n",
        "    limit: int = 50,\n",
        ") -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Returns full detailed history rows for admin/debug views.\n",
        "    \"\"\"\n",
        "    ensure_chat_history_table()\n",
        "\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "\n",
        "    query = \"\"\"\n",
        "        SELECT user_id, role, cohort_name, original_query, improved_query,\n",
        "               which_prompt, answer, chat_model, created_at\n",
        "        FROM chat_history\n",
        "    \"\"\"\n",
        "    params = []\n",
        "    conditions = []\n",
        "\n",
        "    if user_id:\n",
        "        conditions.append(\"user_id = ?\")\n",
        "        params.append(user_id)\n",
        "    if cohort_name:\n",
        "        conditions.append(\"cohort_name = ?\")\n",
        "        params.append(cohort_name)\n",
        "\n",
        "    if conditions:\n",
        "        query += \" WHERE \" + \" AND \".join(conditions)\n",
        "\n",
        "    query += \" ORDER BY created_at DESC LIMIT ?\"\n",
        "    params.append(limit)\n",
        "\n",
        "    cur.execute(query, params)\n",
        "    rows = cur.fetchall()\n",
        "    conn.close()\n",
        "\n",
        "    return [\n",
        "        {\n",
        "            \"user_id\": r[0],\n",
        "            \"role\": r[1],\n",
        "            \"cohort_name\": r[2],\n",
        "            \"original_query\": r[3],\n",
        "            \"improved_query\": r[4],\n",
        "            \"which_prompt\": r[5],\n",
        "            \"answer\": r[6],\n",
        "            \"chat_model\": r[7],\n",
        "            \"created_at\": r[8],\n",
        "        }\n",
        "        for r in rows\n",
        "    ]\n",
        "\n",
        "# ============================================================\n",
        "# ADMIN: LIST USERS\n",
        "\n",
        "def list_users() -> List[Tuple[str, str, str]]:\n",
        "    ensure_user_table()\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "    cur.execute(\"SELECT user_id, display_name, role FROM users ORDER BY created_at DESC\")\n",
        "    rows = cur.fetchall()\n",
        "    conn.close()\n",
        "    return rows\n",
        "\n",
        "# ============================================================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "fYdG8C4ECtQh",
      "metadata": {
        "id": "fYdG8C4ECtQh"
      },
      "outputs": [],
      "source": [
        "# EPIC 5.2 / Admin helper (global)\n",
        "def _ensure_admin(session_user):\n",
        "    \"\"\"Return (SessionUser|None, message). Accepts session_state dict or SessionUser.\"\"\"\n",
        "    if not session_user:\n",
        "        return None, \"❌ Not logged in.\"\n",
        "    if isinstance(session_user, dict):\n",
        "        username = (session_user.get(\"username\") or \"\").strip() or None\n",
        "        role = (session_user.get(\"role\") or \"user\").strip() or \"user\"\n",
        "    else:\n",
        "        username = (getattr(session_user, \"username\", None) or \"\").strip() or None\n",
        "        role = (getattr(session_user, \"role\", \"user\") or \"user\")\n",
        "    su = SessionUser(username=username, role=role)\n",
        "    if not su.is_admin:\n",
        "        return None, \"❌ Admin privileges required.\"\n",
        "    return su, \"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "56ee75bf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56ee75bf",
        "outputId": "b6dd66a3-e63c-4635-9c10-da921667bff7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ STEP 4.6 loaded: EPIC 3.6 memory governance helpers ready (persistence OFF by default).\n"
          ]
        }
      ],
      "source": [
        "# CELL 4.6 / STEP 4.6 – EPIC 3.6 Session Memory Governance + Export/Import + Optional Persistence\n",
        "\n",
        "import json\n",
        "import re\n",
        "from datetime import datetime as _dt, timedelta as _td\n",
        "\n",
        "# ============================================================\n",
        "# EPIC 3.6 – Controls\n",
        "# ============================================================\n",
        "EPIC36_PERSIST_MEMORY_ENABLED = False   # OFF by default; enable when ready\n",
        "EPIC36_PERSIST_TTL_HOURS = 24\n",
        "\n",
        "# Allow-list categories for memory entries (simple governance)\n",
        "EPIC36_ALLOWED_CATEGORIES = {\"identity\", \"preference\", \"task\", \"other\"}\n",
        "\n",
        "# Hard blocklist patterns (never store)\n",
        "_EPIC36_SECRET_PATTERNS = [\n",
        "    r\"sk-[A-Za-z0-9]{10,}\",                 # OpenAI style keys\n",
        "    r\"-----BEGIN [A-Z ]+PRIVATE KEY-----\",  # private keys\n",
        "    r\"password\\s*[:=]\",                    # password:\n",
        "    r\"api\\s*key\\s*[:=]\",                  # api key:\n",
        "]\n",
        "\n",
        "def epic36_is_secret(text: str) -> bool:\n",
        "    t = (text or \"\").strip()\n",
        "    if not t:\n",
        "        return False\n",
        "    for p in _EPIC36_SECRET_PATTERNS:\n",
        "        if re.search(p, t, flags=re.IGNORECASE):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def epic36_sanitize_memory_items(items: list[dict]) -> list[dict]:\n",
        "    \"\"\"Remove secrets and enforce categories.\"\"\"\n",
        "    out = []\n",
        "    for it in items or []:\n",
        "        if not isinstance(it, dict):\n",
        "            continue\n",
        "        key = str(it.get(\"key\", \"\")).strip()\n",
        "        val = str(it.get(\"value\", \"\")).strip()\n",
        "        cat = str(it.get(\"category\", \"other\") or \"other\").strip().lower()\n",
        "        if cat not in EPIC36_ALLOWED_CATEGORIES:\n",
        "            cat = \"other\"\n",
        "        if epic36_is_secret(key) or epic36_is_secret(val):\n",
        "            continue\n",
        "        if key and val:\n",
        "            out.append({\"category\": cat, \"key\": key, \"value\": val})\n",
        "    return out\n",
        "\n",
        "def epic36_export_session_memory_json(session_mem: dict) -> str:\n",
        "    \"\"\"Export current in-memory session memory to a JSON string.\"\"\"\n",
        "    if not session_mem:\n",
        "        return json.dumps({\"enabled\": False, \"scope\": \"session\", \"items\": []}, indent=2)\n",
        "    safe = dict(session_mem)\n",
        "    safe[\"items\"] = epic36_sanitize_memory_items(safe.get(\"items\", []))\n",
        "    return json.dumps(safe, indent=2)\n",
        "\n",
        "def epic36_import_session_memory_json(json_text: str, session_mem: dict | None = None) -> dict:\n",
        "    \"\"\"Import JSON into the current session memory state (sanitized).\"\"\"\n",
        "    base = session_mem or init_session_memory_state()\n",
        "    try:\n",
        "        data = json.loads(json_text or \"{}\")\n",
        "        enabled = bool(data.get(\"enabled\", base.get(\"enabled\", False)))\n",
        "        scope = data.get(\"scope\", base.get(\"scope\", \"session\"))\n",
        "        if scope not in (\"session\", \"cohort\"):\n",
        "            scope = \"session\"\n",
        "        items = epic36_sanitize_memory_items(data.get(\"items\", []))\n",
        "        base.update({\"enabled\": enabled, \"scope\": scope, \"items\": items})\n",
        "        return base\n",
        "    except Exception:\n",
        "        return base\n",
        "\n",
        "def epic36_persist_session_memory(user_id: str, session_mem: dict, scope: str = \"session\") -> None:\n",
        "    \"\"\"Persist memory to SQLite if enabled.\"\"\"\n",
        "    if not EPIC36_PERSIST_MEMORY_ENABLED:\n",
        "        return\n",
        "    if not user_id:\n",
        "        return\n",
        "    mem = dict(session_mem or {})\n",
        "    mem[\"items\"] = epic36_sanitize_memory_items(mem.get(\"items\", []))\n",
        "    updated_at = _dt.utcnow().isoformat(timespec=\"seconds\") + \"Z\"\n",
        "    expires_at = (_dt.utcnow() + _td(hours=EPIC36_PERSIST_TTL_HOURS)).isoformat(timespec=\"seconds\") + \"Z\"\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "    cur.execute(\n",
        "        \"\"\"INSERT INTO session_memory_store (user_id, scope, memory_json, updated_at, expires_at)\n",
        "             VALUES (?, ?, ?, ?, ?)\n",
        "             ON CONFLICT(user_id, scope) DO UPDATE SET\n",
        "                memory_json=excluded.memory_json,\n",
        "                updated_at=excluded.updated_at,\n",
        "                expires_at=excluded.expires_at\"\"\",\n",
        "        (user_id, scope, json.dumps(mem), updated_at, expires_at),\n",
        "    )\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "def epic36_load_persisted_session_memory(user_id: str, scope: str = \"session\") -> dict | None:\n",
        "    \"\"\"Load persisted memory if enabled and not expired.\"\"\"\n",
        "    if not EPIC36_PERSIST_MEMORY_ENABLED:\n",
        "        return None\n",
        "    if not user_id:\n",
        "        return None\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "    cur.execute(\n",
        "        \"SELECT memory_json, expires_at FROM session_memory_store WHERE user_id=? AND scope=?\",\n",
        "        (user_id, scope),\n",
        "    )\n",
        "    row = cur.fetchone()\n",
        "    conn.close()\n",
        "    if not row:\n",
        "        return None\n",
        "    mem_json, expires_at = row\n",
        "    if expires_at:\n",
        "        try:\n",
        "            exp = _dt.fromisoformat(expires_at.replace(\"Z\",\"\"))\n",
        "            if _dt.utcnow() > exp:\n",
        "                return None\n",
        "        except Exception:\n",
        "            pass\n",
        "    try:\n",
        "        data = json.loads(mem_json)\n",
        "        if isinstance(data, dict):\n",
        "            data[\"items\"] = epic36_sanitize_memory_items(data.get(\"items\", []))\n",
        "            return data\n",
        "    except Exception:\n",
        "        return None\n",
        "    return None\n",
        "\n",
        "print(\"✅ STEP 4.6 loaded: EPIC 3.6 memory governance helpers ready (persistence OFF by default).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf61cbc7",
      "metadata": {
        "id": "cf61cbc7"
      },
      "source": [
        "## CELL 4.6 / STEP 4.6 – Audit Log (v16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "d867138c",
      "metadata": {
        "id": "d867138c"
      },
      "outputs": [],
      "source": [
        "# CELL 4.6 / STEP 4.6 – Audit Log (v16)\n",
        "\n",
        "def ensure_audit_table():\n",
        "    \"\"\"\n",
        "    Create an audit_log table if it doesn't already exist.\n",
        "    \"\"\"\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "    cur.execute(\n",
        "        \"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS audit_log (\n",
        "            id        INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "            ts        TEXT NOT NULL,\n",
        "            username  TEXT,\n",
        "            role      TEXT,\n",
        "            action    TEXT NOT NULL,\n",
        "            details   TEXT\n",
        "        )\n",
        "        \"\"\"\n",
        "    )\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "\n",
        "def log_audit(username: str, role: str, action: str, details: str = \"\"):\n",
        "    \"\"\"\n",
        "    Insert a row into the audit_log table.\n",
        "    - ts: UTC ISO timestamp\n",
        "    - username / role: may be None/empty for anonymous\n",
        "    - action: short code, e.g. 'login', 'ask', 'admin_refresh', 'delete_cohort'\n",
        "    - details: freeform string with context\n",
        "    \"\"\"\n",
        "    ensure_audit_table()\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "\n",
        "    # Use the global datetime alias `dt` so we don't conflict with any\n",
        "    # later \"import datetime\" inside the self-test cell.\n",
        "    now = dt.datetime.now(dt.timezone.utc).isoformat()\n",
        "\n",
        "    cur.execute(\n",
        "        \"\"\"\n",
        "        INSERT INTO audit_log (ts, username, role, action, details)\n",
        "        VALUES (?, ?, ?, ?, ?)\n",
        "        \"\"\",\n",
        "        (now, username or \"\", role or \"\", action, details or \"\"),\n",
        "    )\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "\n",
        "def trace_log(message: str):\n",
        "    \"\"\"\n",
        "    Append a timestamped debug line to debug_trace_v16.log in BASE_DIR.\n",
        "\n",
        "    Used to capture errors/warnings that happen inside Gradio callbacks\n",
        "    where the UI might just show a generic 'Error'.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        ts = dt.datetime.now(dt.timezone.utc).isoformat()\n",
        "    except Exception:\n",
        "        ts = \"UNKNOWN_TIME\"\n",
        "\n",
        "    line = f\"{ts} {message}\\n\"\n",
        "    try:\n",
        "        with open(TRACE_LOG_PATH, \"a\", encoding=\"utf-8\") as f:\n",
        "            f.write(line)\n",
        "    except Exception as e:\n",
        "        # Last resort: don't let logging itself crash anything\n",
        "        print(\"TRACE_LOG_ERROR\", e, line)\n",
        "\n",
        "\n",
        "\n",
        "import traceback as _tb\n",
        "\n",
        "def safe_ui_call(fn, fallback=None):\n",
        "    \"\"\"Wrap a Gradio callback so exceptions don't surface as generic 'Error' UI outputs.\n",
        "    Writes full traceback to trace_log() and returns a safe fallback value.\n",
        "    \"\"\"\n",
        "    if fallback is None:\n",
        "        fallback = \"❌ An internal error occurred. See debug trace log.\"\n",
        "    def _wrapped(*args, **kwargs):\n",
        "        try:\n",
        "            return fn(*args, **kwargs)\n",
        "        except Exception as e:\n",
        "            try:\n",
        "                trace_log(f\"[UI ERROR] {getattr(fn, '__name__', 'callback')}: {e}\\n{_tb.format_exc()}\")\n",
        "            except Exception:\n",
        "                pass\n",
        "            return fallback\n",
        "    return _wrapped\n",
        "\n",
        "# ============================================================\n",
        "# STEP 4.7 – Cohort Ownership & Sharing (v16-safe)\n",
        "\n",
        "from typing import Optional, List, Dict, Any\n",
        "\n",
        "def ensure_cohort_meta_table():\n",
        "    \"\"\"\n",
        "    Metadata for cohorts (v16_1 Phase 1):\n",
        "      - owner_user_id: who created/owns the cohort (REQUIRED)\n",
        "      - is_shared: 0 = private to owner/admin, 1 = visible to all authenticated users\n",
        "      - allow_clone: 0 = non-owners cannot clone, 1 = cloning enabled for authenticated users (when shared)\n",
        "      - created_ts: UTC timestamp\n",
        "\n",
        "    Notes:\n",
        "      - Performs a lightweight schema migration if cohort_meta already exists.\n",
        "    \"\"\"\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "\n",
        "    # Create base table if missing\n",
        "    cur.execute(\n",
        "        \"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS cohort_meta (\n",
        "            cohort_name    TEXT PRIMARY KEY,\n",
        "            owner_user_id  TEXT,\n",
        "            is_shared      INTEGER DEFAULT 0,\n",
        "            created_ts     TEXT\n",
        "        )\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    # ---- Schema migration (add allow_clone if missing) ----\n",
        "    try:\n",
        "        cur.execute(\"PRAGMA table_info(cohort_meta)\")\n",
        "        cols = {r[1] for r in cur.fetchall()}  # r[1] = column name\n",
        "        if \"allow_clone\" not in cols:\n",
        "            cur.execute(\"ALTER TABLE cohort_meta ADD COLUMN allow_clone INTEGER DEFAULT 0\")\n",
        "        if \"description\" not in cols:\n",
        "            cur.execute(\"ALTER TABLE cohort_meta ADD COLUMN description TEXT\")\n",
        "        if \"intended_audience\" not in cols:\n",
        "            cur.execute(\"ALTER TABLE cohort_meta ADD COLUMN intended_audience TEXT\")\n",
        "        if \"updated_ts\" not in cols:\n",
        "            cur.execute(\"ALTER TABLE cohort_meta ADD COLUMN updated_ts TEXT\")\n",
        "    except Exception as e:\n",
        "        # Do not hard-fail cohort operations due to migration issues\n",
        "        trace_log(f\"ensure_cohort_meta_table migration warning: {e}\")\n",
        "\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "def _extract_username_from_user(user: SessionUser | dict | None) -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Helper: accept SessionUser, dict, or None and pull out a username string.\n",
        "    \"\"\"\n",
        "    if user is None:\n",
        "        return None\n",
        "\n",
        "    # If SessionUser dataclass\n",
        "    if isinstance(user, SessionUser):\n",
        "        return user.username\n",
        "\n",
        "    # If dict-like\n",
        "    if isinstance(user, dict):\n",
        "        # Try common keys in order\n",
        "        for k in (\"username\", \"user_id\", \"name\"):\n",
        "            try:\n",
        "                v = user.get(k)\n",
        "                if v:\n",
        "                    return str(v)\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "def set_cohort_owner(cohort_name: str, user: SessionUser | dict | None):\n",
        "    \"\"\"\n",
        "    Register the owner of a cohort, but do NOT change it once set.\n",
        "\n",
        "    HARD REQUIREMENT (v16_1 Phase 1 security):\n",
        "      - Cohorts must have an authenticated owner.\n",
        "      - We do NOT silently fall back to \"anonymous\".\n",
        "\n",
        "    Behavior:\n",
        "      - On first creation: insert owner_user_id and is_shared=0 (private).\n",
        "      - On subsequent calls for the same cohort_name: owner remains unchanged.\n",
        "    \"\"\"\n",
        "    if not cohort_name:\n",
        "        return\n",
        "\n",
        "    from datetime import datetime, timezone\n",
        "\n",
        "    owner = _extract_username_from_user(user)\n",
        "    if not owner or not str(owner).strip() or str(owner).strip().lower() == \"anonymous\":\n",
        "        raise ValueError(\"Cohort owner is required. Please log in before creating/updating a cohort.\")\n",
        "\n",
        "\n",
        "    ensure_cohort_meta_table()\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "\n",
        "    now = datetime.now(timezone.utc).isoformat()\n",
        "\n",
        "    # Insert cohort_meta row if missing; if it already exists with an invalid owner, repair the owner once.\n",
        "    cur.execute(\n",
        "        \"\"\"\n",
        "        INSERT INTO cohort_meta (cohort_name, owner_user_id, is_shared, created_ts)\n",
        "        VALUES (?, ?, 0, ?)\n",
        "        ON CONFLICT(cohort_name) DO UPDATE SET\n",
        "            owner_user_id = excluded.owner_user_id\n",
        "        WHERE cohort_meta.owner_user_id IS NULL\n",
        "           OR TRIM(cohort_meta.owner_user_id) = ''\n",
        "           OR LOWER(TRIM(cohort_meta.owner_user_id)) = 'anonymous'\n",
        "        \"\"\",\n",
        "        (cohort_name, owner, now),\n",
        "    )\n",
        "\n",
        "\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "\n",
        "def set_cohort_sharing(cohort_name: str, is_shared: bool | int):\n",
        "    \"\"\"\n",
        "    Update ONLY the is_shared flag for a cohort in cohort_meta.\n",
        "\n",
        "    is_shared:\n",
        "        False/0 -> private (owner + admins)\n",
        "        True/1  -> shared (all users can see; non-owners are read-only)\n",
        "    \"\"\"\n",
        "    if not cohort_name:\n",
        "        return\n",
        "\n",
        "    ensure_cohort_meta_table()\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "    shared_flag = 1 if is_shared else 0\n",
        "\n",
        "    cur.execute(\n",
        "        \"\"\"\n",
        "        UPDATE cohort_meta\n",
        "        SET is_shared = ?\n",
        "        WHERE cohort_name = ?\n",
        "        \"\"\",\n",
        "        (shared_flag, cohort_name),\n",
        "    )\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "def set_cohort_clone_allowed(cohort_name: str, allow_clone: bool | int):\n",
        "    \"\"\"\n",
        "    Update ONLY the allow_clone flag for a cohort in cohort_meta.\n",
        "\n",
        "    allow_clone:\n",
        "        False/0 -> non-owners cannot clone (even if shared)\n",
        "        True/1  -> if shared, any authenticated user may clone\n",
        "    \"\"\"\n",
        "    if not cohort_name:\n",
        "        return\n",
        "\n",
        "    ensure_cohort_meta_table()\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "    flag = 1 if allow_clone else 0\n",
        "\n",
        "    cur.execute(\n",
        "        \"\"\"\n",
        "        UPDATE cohort_meta\n",
        "        SET allow_clone = ?\n",
        "        WHERE cohort_name = ?\n",
        "        \"\"\",\n",
        "        (flag, cohort_name),\n",
        "    )\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "def set_cohort_allow_clone(cohort_name: str, allow_clone: bool) -> None:\n",
        "    \"\"\"\n",
        "    Backward-compatible alias for older call sites.\n",
        "    \"\"\"\n",
        "    return set_cohort_clone_allowed(cohort_name, allow_clone)\n",
        "\n",
        "\n",
        "#def set_cohort_clone_allowed(cohort_name: str, allow_clone: bool | int):\n",
        "#    \"\"\"Backward-compatible alias for earlier UI code.\"\"\"\n",
        "#    return set_cohort_clone_allowed(cohort_name, allow_clone)\n",
        "\n",
        "def _get_cohort_meta(cohort_name: str) -> tuple[str | None, int, int]:\n",
        "    \"\"\"\n",
        "    Returns (owner_user_id, is_shared, allow_clone).\n",
        "    If cohort not found in meta, returns (None, 0, 0).\n",
        "    \"\"\"\n",
        "    ensure_cohort_meta_table()\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "    cur.execute(\n",
        "        \"SELECT owner_user_id, COALESCE(is_shared,0), COALESCE(allow_clone,0) FROM cohort_meta WHERE cohort_name = ?\",\n",
        "        (cohort_name,),\n",
        "    )\n",
        "    row = cur.fetchone()\n",
        "    conn.close()\n",
        "    if not row:\n",
        "        return None, 0, 0\n",
        "    return (row[0], int(row[1] or 0), int(row[2] or 0))\n",
        "\n",
        "\n",
        "def can_clone_cohort(session_user: Any, source_cohort: str) -> tuple[bool, str | None]:\n",
        "    \"\"\"\n",
        "    Returns (allowed, message_if_denied).\n",
        "\n",
        "    Rules (Option A):\n",
        "      - Admin: may always clone.\n",
        "      - Owner: may always clone.\n",
        "      - Other authenticated users:\n",
        "          * allowed ONLY if source cohort is_shared=1 AND allow_clone=1\n",
        "      - Anonymous: not allowed.\n",
        "    \"\"\"\n",
        "    if not source_cohort:\n",
        "        return False, \"❌ Please select a cohort to clone.\"\n",
        "\n",
        "    username = _extract_user_id(session_user)\n",
        "    if not username or username == \"anonymous\":\n",
        "        return False, \"❌ You must be logged in to clone a cohort.\"\n",
        "\n",
        "    # Determine admin role\n",
        "    role = None\n",
        "    if isinstance(session_user, dict):\n",
        "        role = session_user.get(\"role\")\n",
        "    elif isinstance(session_user, SessionUser):\n",
        "        role = \"admin\" if session_user.is_admin else \"user\"\n",
        "    else:\n",
        "        role = USERS.get(username, {}).get(\"role\", \"user\")\n",
        "\n",
        "    if role == \"admin\":\n",
        "        return True, None\n",
        "\n",
        "    owner, is_shared, allow_clone = _get_cohort_meta(source_cohort)\n",
        "\n",
        "    # Owner may always clone\n",
        "    if owner and owner == username:\n",
        "        return True, None\n",
        "\n",
        "    # Non-owner: only if shared + cloning allowed\n",
        "    if is_shared == 1 and allow_clone == 1:\n",
        "        return True, None\n",
        "\n",
        "    return False, \"❌ Cloning is not permitted for this cohort (owner has disabled cloning).\"\n",
        "\n",
        "\n",
        "def clone_cohort(source_cohort: str, target_cohort: str, new_owner: str) -> str:\n",
        "    \"\"\"\n",
        "    Clone a cohort by duplicating its document rows and index artifacts.\n",
        "\n",
        "    Implementation notes:\n",
        "      - We copy each source FAISS index (.faiss) and metadata (.pkl) to a new index_id.\n",
        "      - We create new rows in documents and cohort_docs for the target cohort.\n",
        "      - The cloned cohort is created as PRIVATE by default (is_shared=0, allow_clone=0).\n",
        "    \"\"\"\n",
        "    if not source_cohort or not target_cohort:\n",
        "        return \"❌ Source and target cohort names are required.\"\n",
        "\n",
        "    if not new_owner or not str(new_owner).strip() or str(new_owner).strip().lower() == \"anonymous\":\n",
        "        return \"❌ You must be logged in to clone a cohort (owner is required).\"\n",
        "\n",
        "\n",
        "    target_cohort = target_cohort.strip()\n",
        "    if cohort_exists(target_cohort):\n",
        "        return f\"❌ Target cohort '{target_cohort}' already exists. Choose a different name.\"\n",
        "\n",
        "    ensure_docs_table()\n",
        "    ensure_cohort_table()\n",
        "    ensure_cohort_meta_table()\n",
        "\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "\n",
        "    cur.execute(\n",
        "        \"SELECT doc_name, index_id, n_chunks, embed_model FROM documents WHERE cohort_name = ?\",\n",
        "        (source_cohort,),\n",
        "    )\n",
        "    rows = cur.fetchall()\n",
        "\n",
        "    if not rows:\n",
        "        conn.close()\n",
        "        return f\"❌ Source cohort '{source_cohort}' has no documents to clone.\"\n",
        "\n",
        "    created_at = dt.datetime.now(dt.timezone.utc).isoformat()\n",
        "\n",
        "    # Create a meta row for the new cohort (private by default)\n",
        "    cur.execute(\n",
        "        \"\"\"\n",
        "        INSERT INTO cohort_meta (cohort_name, owner_user_id, is_shared, allow_clone, created_ts)\n",
        "        VALUES (?, ?, 0, 0, ?)\n",
        "        ON CONFLICT(cohort_name) DO NOTHING\n",
        "        \"\"\",\n",
        "        (target_cohort, (new_owner or \"\").strip(), created_at),\n",
        "    )\n",
        "\n",
        "    cloned_count = 0\n",
        "    for (doc_name, src_index_id, n_chunks, embed_model) in rows:\n",
        "        new_index_id = str(uuid4())\n",
        "\n",
        "        # Copy FAISS + PKL artifacts to new ids\n",
        "        src_faiss = os.path.join(INDEX_DIR, f\"{src_index_id}.faiss\")\n",
        "        src_pkl = os.path.join(INDEX_DIR, f\"{src_index_id}.pkl\")\n",
        "        dst_faiss = os.path.join(INDEX_DIR, f\"{new_index_id}.faiss\")\n",
        "        dst_pkl = os.path.join(INDEX_DIR, f\"{new_index_id}.pkl\")\n",
        "\n",
        "        if not os.path.exists(src_faiss) or not os.path.exists(src_pkl):\n",
        "            trace_log(f\"clone_cohort warning: missing artifacts for index_id={src_index_id}\")\n",
        "            continue\n",
        "\n",
        "        shutil.copyfile(src_faiss, dst_faiss)\n",
        "        shutil.copyfile(src_pkl, dst_pkl)\n",
        "\n",
        "        # Insert new document row\n",
        "        new_doc_id = str(uuid4())\n",
        "        cur.execute(\n",
        "            \"\"\"\n",
        "            INSERT INTO documents (id, doc_name, cohort_name, index_id, n_chunks, embed_model, created_at)\n",
        "            VALUES (?, ?, ?, ?, ?, ?, ?)\n",
        "            \"\"\",\n",
        "            (new_doc_id, doc_name, target_cohort, new_index_id, int(n_chunks), embed_model, created_at),\n",
        "        )\n",
        "\n",
        "        # Map doc to cohort_docs\n",
        "        cur.execute(\n",
        "            \"INSERT INTO cohort_docs (cohort_name, doc_name) VALUES (?, ?)\",\n",
        "            (target_cohort, doc_name),\n",
        "        )\n",
        "\n",
        "        cloned_count += 1\n",
        "\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "    return f\"✅ Cloned cohort '{source_cohort}' into '{target_cohort}' with {cloned_count} documents.\"\n",
        "\n",
        "\n",
        "def cohort_exists(cohort_name: str) -> bool:\n",
        "    \"\"\"\n",
        "    Returns True if a cohort with this name already exists in cohort_docs.\n",
        "    This is global (not per-user) to avoid confusing duplicate names.\n",
        "    \"\"\"\n",
        "    if not cohort_name:\n",
        "        return False\n",
        "\n",
        "    ensure_cohort_table()\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "    cur.execute(\n",
        "        \"SELECT 1 FROM cohort_docs WHERE cohort_name = ? LIMIT 1\",\n",
        "        (cohort_name,),\n",
        "    )\n",
        "    row = cur.fetchone()\n",
        "    conn.close()\n",
        "    return row is not None\n",
        "\n",
        "\n",
        "def list_cohorts_for_user(user: SessionUser | dict | None) -> list[str]:\n",
        "    \"\"\"\n",
        "    Return list of cohort names visible to the given user.\n",
        "\n",
        "    Rules:\n",
        "      - Admins: all cohorts (global)\n",
        "      - Non-admin:\n",
        "          * Cohorts where they are owner (cohort_meta.owner_user_id)\n",
        "          * Cohorts marked is_shared = 1\n",
        "      - Anonymous: only shared cohorts (is_shared = 1)\n",
        "\n",
        "    If anything goes wrong with the metadata logic, falls back to global list_cohorts().\n",
        "    Also writes debug info to the v16 trace log.\n",
        "    \"\"\"\n",
        "    trace_log(f\"list_cohorts_for_user called with user={user!r}\")\n",
        "\n",
        "    ensure_cohort_table()\n",
        "    ensure_cohort_meta_table()\n",
        "\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "\n",
        "    try:\n",
        "        # Determine if user is admin, safely\n",
        "        is_admin = False\n",
        "        username = None\n",
        "\n",
        "        if isinstance(user, SessionUser):\n",
        "            is_admin = user.is_admin\n",
        "            username = user.username\n",
        "        elif isinstance(user, dict):\n",
        "            username = _extract_username_from_user(user)\n",
        "            role = user.get(\"role\")\n",
        "            is_admin = (role == \"admin\")\n",
        "\n",
        "        if is_admin:\n",
        "            # Admin sees all distinct cohorts\n",
        "            cur.execute(\n",
        "                \"\"\"\n",
        "                SELECT DISTINCT cohort_name\n",
        "                FROM cohort_docs\n",
        "                ORDER BY cohort_name\n",
        "                \"\"\"\n",
        "            )\n",
        "        else:\n",
        "            # Non-admin or anonymous\n",
        "            if username:\n",
        "                # Logged-in non-admin -> owner or shared\n",
        "                cur.execute(\n",
        "                    \"\"\"\n",
        "                    SELECT DISTINCT cd.cohort_name\n",
        "                    FROM cohort_docs cd\n",
        "                    LEFT JOIN cohort_meta cm\n",
        "                      ON cd.cohort_name = cm.cohort_name\n",
        "                    WHERE cm.owner_user_id = ?\n",
        "                       OR cm.is_shared = 1\n",
        "                       OR cm.cohort_name IS NULL   -- safety: cohorts without meta still appear\n",
        "                    ORDER BY cd.cohort_name\n",
        "                    \"\"\",\n",
        "                    (username,),\n",
        "                )\n",
        "            else:\n",
        "                # Anonymous -> only shared (or cohorts w/o meta as a fallback)\n",
        "                cur.execute(\n",
        "                    \"\"\"\n",
        "                    SELECT DISTINCT cd.cohort_name\n",
        "                    FROM cohort_docs cd\n",
        "                    LEFT JOIN cohort_meta cm\n",
        "                      ON cd.cohort_name = cm.cohort_name\n",
        "                    WHERE cm.is_shared = 1\n",
        "                       OR cm.cohort_name IS NULL\n",
        "                    ORDER BY cd.cohort_name\n",
        "                    \"\"\"\n",
        "                )\n",
        "\n",
        "        rows = cur.fetchall()\n",
        "        conn.close()\n",
        "        names = [r[0] for r in rows]\n",
        "\n",
        "        # Final safety net: do not leak global cohorts; return empty.\n",
        "        if not names:\n",
        "            trace_log(\"list_cohorts_for_user -> no rows; returning empty\")\n",
        "            names = []\n",
        "\n",
        "        trace_log(f\"list_cohorts_for_user returning {names}\")\n",
        "        return names\n",
        "\n",
        "    except Exception as e:\n",
        "        conn.close()\n",
        "        trace_log(f\"list_cohorts_for_user ERROR: {e}\")\n",
        "        return []\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# UI HELPERS FOR COHORT LISTS (used in STEP 10)\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "def get_cohorts_for_user(username: Optional[str]) -> List[str]:\n",
        "    \"\"\"\n",
        "    Convenience wrapper for the Gradio UI:\n",
        "    take a simple username string and delegate to list_cohorts_for_user().\n",
        "\n",
        "    Also logs to the v16 trace file for easier debugging of Refresh Cohorts.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        trace_log(f\"get_cohorts_for_user called with username={username!r}\")\n",
        "\n",
        "        if not username or not str(username).strip():\n",
        "            # No anonymous access: callers must be logged in.\n",
        "            return []\n",
        "        role = USERS.get(username, {}).get(\"role\", \"user\")\n",
        "        user = SessionUser(username=str(username).strip(), role=role)\n",
        "\n",
        "        names = list_cohorts_for_user(user)\n",
        "        trace_log(f\"get_cohorts_for_user returning {names}\")\n",
        "        return names\n",
        "\n",
        "    except Exception as e:\n",
        "        trace_log(f\"get_cohorts_for_user ERROR for username={username!r}: {e}\")\n",
        "        # No anonymous fallback; return empty to avoid leaking data.\n",
        "        return []\n",
        "\n",
        "def get_all_cohorts() -> List[List[str]]:\n",
        "    \"\"\"\n",
        "    For the Admin tab: return [[cohort_name, owner], ...].\n",
        "    \"\"\"\n",
        "    ensure_cohort_table()\n",
        "    ensure_cohort_meta_table()\n",
        "\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "    cur.execute(\n",
        "        \"\"\"\n",
        "        SELECT cd.cohort_name,\n",
        "               COALESCE(cm.owner_user_id, 'unknown') AS owner\n",
        "        FROM cohort_docs cd\n",
        "        LEFT JOIN cohort_meta cm\n",
        "          ON cd.cohort_name = cm.cohort_name\n",
        "        GROUP BY cd.cohort_name\n",
        "        ORDER BY cd.cohort_name\n",
        "        \"\"\"\n",
        "    )\n",
        "    rows = cur.fetchall()\n",
        "    conn.close()\n",
        "\n",
        "    return [[name, owner] for (name, owner) in rows]\n",
        "\n",
        "def get_allowed_setup_actions(current_user_dict: dict | None) -> tuple[list[str], str, str, bool]:\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      - action_choices\n",
        "      - action_value (safe default)\n",
        "      - info_message\n",
        "      - info_visible\n",
        "    \"\"\"\n",
        "    # Build a SessionUser safely\n",
        "    if not current_user_dict or not current_user_dict.get(\"username\"):\n",
        "        user_obj = SessionUser(username=None, role=\"anonymous\")\n",
        "    else:\n",
        "        user_obj = SessionUser(\n",
        "            username=current_user_dict[\"username\"],\n",
        "            role=current_user_dict.get(\"role\", \"user\"),\n",
        "        )\n",
        "\n",
        "    # Determine if clone is possible for this user\n",
        "    cloneable = list_cloneable_shared_cohorts_for_user(user_obj)  # you already have / can add this helper\n",
        "\n",
        "    choices = [ACTION_CREATE, ACTION_APPEND]\n",
        "    msg = \"\"\n",
        "    visible = False\n",
        "\n",
        "    if cloneable:\n",
        "        choices.append(ACTION_CLONE)\n",
        "    else:\n",
        "        # “Greyed out” effect: omit the choice entirely\n",
        "        msg = (\n",
        "            \"Clone is unavailable because no shared cohorts have been marked as clone-eligible for your account.\\n\\n\"\n",
        "            \"Ask the cohort owner (or an admin) to enable **Allow cloning** on a shared cohort.\"\n",
        "        )\n",
        "        visible = True\n",
        "\n",
        "    # Always return a valid selection (prevents the ValueError you hit on logout)\n",
        "    return choices, DEFAULT_ACTION, msg, visible\n",
        "\n",
        "# ============================================================\n",
        "\n",
        "\n",
        "def get_cohort_meta(cohort_name: str) -> dict:\n",
        "    \"\"\"Return cohort_meta fields (sharing + description/audience).\"\"\"\n",
        "    ensure_cohort_meta_table()\n",
        "    cn = (cohort_name or \"\").strip()\n",
        "    if not cn:\n",
        "        return {}\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "    cur.execute(\n",
        "        \"\"\"\n",
        "        SELECT cohort_name, owner_user_id, is_shared, allow_clone,\n",
        "               COALESCE(description,''), COALESCE(intended_audience,''), COALESCE(updated_ts,''), COALESCE(created_ts,'')\n",
        "        FROM cohort_meta\n",
        "        WHERE cohort_name = ?\n",
        "        \"\"\",\n",
        "        (cn,),\n",
        "    )\n",
        "    row = cur.fetchone()\n",
        "    conn.close()\n",
        "    if not row:\n",
        "        return {}\n",
        "    return {\n",
        "        \"cohort_name\": row[0],\n",
        "        \"owner_user_id\": row[1],\n",
        "        \"is_shared\": int(row[2] or 0),\n",
        "        \"allow_clone\": int(row[3] or 0),\n",
        "        \"description\": row[4] or \"\",\n",
        "        \"intended_audience\": row[5] or \"\",\n",
        "        \"updated_ts\": row[6] or \"\",\n",
        "        \"created_ts\": row[7] or \"\",\n",
        "    }\n",
        "\n",
        "def upsert_cohort_description_and_audience(cohort_name: str, description: str, intended_audience: str):\n",
        "    \"\"\"Upsert only description + intended_audience into cohort_meta for v18 EPIC 5.1/5.2.\"\"\"\n",
        "    ensure_cohort_meta_table()\n",
        "    cn = (cohort_name or \"\").strip()\n",
        "    if not cn:\n",
        "        raise ValueError(\"cohort_name is required\")\n",
        "    desc = (description or \"\").strip()\n",
        "    aud = (intended_audience or \"\").strip()\n",
        "    now = dt.datetime.now(dt.timezone.utc).isoformat()\n",
        "\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "    # Ensure row exists (preserve existing sharing fields if present)\n",
        "    cur.execute(\"SELECT cohort_name FROM cohort_meta WHERE cohort_name = ?\", (cn,))\n",
        "    exists = cur.fetchone() is not None\n",
        "    if not exists:\n",
        "        cur.execute(\n",
        "            \"\"\"\n",
        "            INSERT INTO cohort_meta (cohort_name, owner_user_id, is_shared, allow_clone, created_ts, updated_ts, description, intended_audience)\n",
        "            VALUES (?, NULL, 0, 0, ?, ?, ?, ?)\n",
        "            \"\"\",\n",
        "            (cn, now, now, desc, aud),\n",
        "        )\n",
        "    else:\n",
        "        cur.execute(\n",
        "            \"\"\"\n",
        "            UPDATE cohort_meta\n",
        "            SET description = ?,\n",
        "                intended_audience = ?,\n",
        "                updated_ts = ?\n",
        "            WHERE cohort_name = ?\n",
        "            \"\"\",\n",
        "            (desc, aud, now, cn),\n",
        "        )\n",
        "    conn.commit()\n",
        "    conn.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0fba085",
      "metadata": {
        "id": "f0fba085"
      },
      "source": [
        "## CELL 4.7 / STEP 4.7 – FAPs (Frequently Asked Prompts) (v16.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "1dc19370",
      "metadata": {
        "id": "1dc19370"
      },
      "outputs": [],
      "source": [
        "# CELL 4.7 / STEP 4.7 – FAPs (Frequently Asked Prompts) (v18)\n",
        "# Stores curated Frequently Asked Prompts (FAPs) per cohort in SQLite.\n",
        "# FAPs can be preloaded via admin file upload (CSV/TXT/JSON).\n",
        "\n",
        "def ensure_demo_prompts_table():\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "\n",
        "    cur.execute(\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS demo_prompts (\n",
        "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "            cohort_name TEXT NOT NULL,\n",
        "            prompt_text TEXT NOT NULL,\n",
        "            sort_order INTEGER DEFAULT 0,\n",
        "            display_order INTEGER DEFAULT 0,\n",
        "            admin_note TEXT,\n",
        "            created_at TEXT\n",
        "        )\n",
        "    \"\"\")\n",
        "    conn.commit()\n",
        "\n",
        "    # --- migrations ---\n",
        "    cur.execute(\"PRAGMA table_info(demo_prompts)\")\n",
        "    cols = {row[1] for row in cur.fetchall()}\n",
        "\n",
        "    if \"display_order\" not in cols:\n",
        "        cur.execute(\"ALTER TABLE demo_prompts ADD COLUMN display_order INTEGER DEFAULT 0\")\n",
        "    if \"admin_note\" not in cols:\n",
        "        cur.execute(\"ALTER TABLE demo_prompts ADD COLUMN admin_note TEXT\")\n",
        "\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "def clear_demo_prompts_for_cohort(cohort_name: str):\n",
        "    ensure_demo_prompts_table()\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "    cur.execute(\"DELETE FROM demo_prompts WHERE cohort_name = ?\", (cohort_name,))\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "def list_demo_prompts_for_cohort(cohort_name: str) -> list[str]:\n",
        "    ensure_demo_prompts_table()\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "    cur.execute(\n",
        "        \"\"\"\n",
        "        SELECT prompt_text\n",
        "        FROM demo_prompts\n",
        "        WHERE cohort_name = ?\n",
        "        ORDER BY COALESCE(sort_order,0), id\n",
        "        \"\"\",\n",
        "        (cohort_name,),\n",
        "    )\n",
        "    rows = cur.fetchall()\n",
        "    conn.close()\n",
        "    return [r[0] for r in rows if r and r[0]]\n",
        "\n",
        "def upsert_demo_prompts_for_cohort(cohort_name: str, prompts: list[str], replace: bool = True) -> int:\n",
        "    \"\"\"\n",
        "    Insert demo prompts for a cohort. If replace=True, clears existing prompts first.\n",
        "    Returns number of prompts inserted.\n",
        "    \"\"\"\n",
        "    ensure_demo_prompts_table()\n",
        "    cohort = (cohort_name or \"\").strip()\n",
        "    if not cohort:\n",
        "        return 0\n",
        "\n",
        "    cleaned: list[str] = []\n",
        "    seen: set[str] = set()\n",
        "    for p in (prompts or []):\n",
        "        t = (p or \"\").strip()\n",
        "        if not t:\n",
        "            continue\n",
        "        if t in seen:\n",
        "            continue\n",
        "        seen.add(t)\n",
        "        cleaned.append(t)\n",
        "\n",
        "    if replace:\n",
        "        clear_demo_prompts_for_cohort(cohort)\n",
        "\n",
        "    if not cleaned:\n",
        "        return 0\n",
        "\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "    for i, t in enumerate(cleaned, start=1):\n",
        "        cur.execute(\n",
        "            \"INSERT INTO demo_prompts (cohort_name, prompt_text, display_order) VALUES (?,?,?)\",\n",
        "            (cohort, t, i),\n",
        "        )\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "    return len(cleaned)\n",
        "\n",
        "def parse_demo_prompts_file(file_path: str) -> list[str]:\n",
        "    \"\"\"\n",
        "    Parse demo prompts from a local file path.\n",
        "    Supported:\n",
        "      - .txt: one question per line (blank lines ignored)\n",
        "      - .csv: one prompt per line OR header with columns 'prompt'/'question'/'text'\n",
        "      - .json: either list[str] or {\"prompts\":[...]}\n",
        "    \"\"\"\n",
        "    if not file_path:\n",
        "        return []\n",
        "    fp = str(file_path)\n",
        "    ext = os.path.splitext(fp)[1].lower()\n",
        "    try:\n",
        "        with open(fp, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "            raw = f.read()\n",
        "    except Exception:\n",
        "        return []\n",
        "\n",
        "    if ext == \".json\":\n",
        "        try:\n",
        "            obj = json.loads(raw)\n",
        "            if isinstance(obj, list):\n",
        "                return [str(x) for x in obj]\n",
        "            if isinstance(obj, dict) and isinstance(obj.get(\"prompts\"), list):\n",
        "                return [str(x) for x in obj.get(\"prompts\")]\n",
        "        except Exception:\n",
        "            return []\n",
        "        return []\n",
        "\n",
        "    lines = [ln.strip() for ln in raw.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\").split(\"\\n\")]\n",
        "    lines = [ln for ln in lines if ln]\n",
        "\n",
        "    if ext == \".txt\":\n",
        "        return lines\n",
        "\n",
        "    if ext == \".csv\":\n",
        "        if not lines:\n",
        "            return []\n",
        "        header = [h.strip().strip('\"').strip(\"'\").lower() for h in lines[0].split(\",\")]\n",
        "        if any(h in (\"prompt\", \"question\", \"text\") for h in header):\n",
        "            col_idx = None\n",
        "            for k in (\"prompt\", \"question\", \"text\"):\n",
        "                if k in header:\n",
        "                    col_idx = header.index(k)\n",
        "                    break\n",
        "            out = []\n",
        "            for row in lines[1:]:\n",
        "                parts = [p.strip().strip('\"').strip(\"'\") for p in row.split(\",\")]\n",
        "                if col_idx is not None and col_idx < len(parts):\n",
        "                    out.append(parts[col_idx])\n",
        "            return out\n",
        "        # no header: each row is a prompt (first column)\n",
        "        out = []\n",
        "        for row in lines:\n",
        "            parts = [p.strip().strip('\"').strip(\"'\") for p in row.split(\",\")]\n",
        "            if parts and parts[0]:\n",
        "                out.append(parts[0])\n",
        "        return out\n",
        "\n",
        "    return lines\n",
        "\n",
        "# ============================================================\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# v18 – FAPs (Naming Aliases)\n",
        "# ----------------------------\n",
        "# We keep the underlying SQLite table name 'demo_prompts' for backward compatibility,\n",
        "# but expose 'FAP' naming in the UI and in wrapper helpers.\n",
        "\n",
        "def ensure_faps_table():\n",
        "    return ensure_demo_prompts_table()\n",
        "\n",
        "def list_faps_for_cohort(cohort_name: str) -> list[str]:\n",
        "    return list_demo_prompts_for_cohort(cohort_name)\n",
        "\n",
        "def upsert_faps_for_cohort(cohort_name: str, prompts: list[str], replace: bool = True) -> int:\n",
        "    return upsert_demo_prompts_for_cohort(cohort_name, prompts, replace=replace)\n",
        "\n",
        "def clear_faps_for_cohort(cohort_name: str) -> None:\n",
        "    return clear_demo_prompts_for_cohort(cohort_name)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a07306c",
      "metadata": {
        "id": "6a07306c"
      },
      "source": [
        "## CELL 5.0 / STEP 5.0 – Model Registry Core (v16_1, with schema migration)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "43aef7bb",
      "metadata": {
        "id": "43aef7bb"
      },
      "outputs": [],
      "source": [
        "# CELL 5.0 / STEP 5.0 – Model Registry Core (v16_1, with schema migration)\n",
        "\n",
        "import sqlite3\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "def ensure_model_registry_table():\n",
        "    \"\"\"\n",
        "    Ensure the model_registry table exists with the v6_15 schema.\n",
        "    If an older schema is detected (missing model_id or other key fields),\n",
        "    we DROP and recreate the table.\n",
        "\n",
        "    This is safe for the MVP since we don't rely on persistent custom models yet.\n",
        "    \"\"\"\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "\n",
        "    # Does the table exist?\n",
        "    cur.execute(\n",
        "        \"SELECT name FROM sqlite_master WHERE type='table' AND name='model_registry'\"\n",
        "    )\n",
        "    row = cur.fetchone()\n",
        "\n",
        "    if row:\n",
        "        # Table exists – inspect its columns\n",
        "        cur.execute(\"PRAGMA table_info(model_registry)\")\n",
        "        cols_info = cur.fetchall()\n",
        "        existing_cols = {c[1] for c in cols_info}  # c[1] is the column name\n",
        "\n",
        "        required_cols = {\n",
        "            \"provider\",\n",
        "            \"model_id\",\n",
        "            \"display_name\",\n",
        "            \"model_type\",\n",
        "            \"enabled\",\n",
        "            \"is_default\",\n",
        "            \"cost_score\",\n",
        "            \"latency_score\",\n",
        "            \"max_context_tokens\",\n",
        "            \"api_base\",\n",
        "            \"notes\",\n",
        "        }\n",
        "\n",
        "        # If the key v16 columns are missing, drop and recreate\n",
        "        if not required_cols.issubset(existing_cols):\n",
        "            print(\"DEBUG: model_registry schema mismatch detected. Dropping old table.\")\n",
        "            cur.execute(\"DROP TABLE IF EXISTS model_registry\")\n",
        "            conn.commit()\n",
        "\n",
        "    # (Re)create the table with the correct v16 schema\n",
        "    cur.execute(\n",
        "        \"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS model_registry (\n",
        "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "            provider TEXT,\n",
        "            model_id TEXT,\n",
        "            display_name TEXT,\n",
        "            model_type TEXT,\n",
        "            enabled INTEGER DEFAULT 1,\n",
        "            is_default INTEGER DEFAULT 0,\n",
        "            cost_score INTEGER DEFAULT 2,\n",
        "            latency_score INTEGER DEFAULT 2,\n",
        "            max_context_tokens INTEGER,\n",
        "            api_base TEXT,\n",
        "            notes TEXT\n",
        "        )\n",
        "        \"\"\"\n",
        "    )\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "\n",
        "def seed_default_models():\n",
        "    \"\"\"\n",
        "    Seed the registry with standard defaults if empty.\n",
        "    \"\"\"\n",
        "    ensure_model_registry_table()\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "\n",
        "    cur.execute(\"SELECT COUNT(*) FROM model_registry\")\n",
        "    count = cur.fetchone()[0]\n",
        "\n",
        "    if count == 0:\n",
        "        defaults = [\n",
        "            # Default Chat Model\n",
        "            {\n",
        "                \"provider\": \"openai\",\n",
        "                \"model_id\": \"gpt-4.1-mini\",\n",
        "                \"display_name\": \"GPT-4.1 Mini\",\n",
        "                \"model_type\": \"chat\",\n",
        "                \"enabled\": 1,\n",
        "                \"is_default\": 1,\n",
        "                \"cost_score\": 1,\n",
        "                \"latency_score\": 1,\n",
        "                \"max_context_tokens\": 128_000,\n",
        "                \"api_base\": None,\n",
        "                \"notes\": \"Primary chat model\",\n",
        "            },\n",
        "\n",
        "            # Additional Chat Models (EPIC 5.2)\n",
        "            {\n",
        "                \"provider\": \"openai\",\n",
        "                \"model_id\": \"gpt-5\",\n",
        "                \"display_name\": \"GPT-5\",\n",
        "                \"model_type\": \"chat\",\n",
        "                \"enabled\": 1,\n",
        "                \"is_default\": 0,\n",
        "                \"cost_score\": 3,\n",
        "                \"latency_score\": 2,\n",
        "                \"max_context_tokens\": 128_000,\n",
        "                \"api_base\": None,\n",
        "                \"notes\": \"High-capability chat model\",\n",
        "            },\n",
        "            {\n",
        "                \"provider\": \"openai\",\n",
        "                \"model_id\": \"gpt-5-mini\",\n",
        "                \"display_name\": \"GPT-5 Mini\",\n",
        "                \"model_type\": \"chat\",\n",
        "                \"enabled\": 1,\n",
        "                \"is_default\": 0,\n",
        "                \"cost_score\": 2,\n",
        "                \"latency_score\": 1,\n",
        "                \"max_context_tokens\": 128_000,\n",
        "                \"api_base\": None,\n",
        "                \"notes\": \"Lower-cost GPT-5 variant\",\n",
        "            },\n",
        "            {\n",
        "                \"provider\": \"openai\",\n",
        "                \"model_id\": \"\",\n",
        "                \"display_name\": \"GPT-5 Chat\",\n",
        "                \"model_type\": \"chat\",\n",
        "                \"enabled\": 1,\n",
        "                \"is_default\": 0,\n",
        "                \"cost_score\": 2,\n",
        "                \"latency_score\": 1,\n",
        "                \"max_context_tokens\": 128_000,\n",
        "                \"api_base\": None,\n",
        "                \"notes\": \"Chat-optimized GPT-5 variant\",\n",
        "            },\n",
        "\n",
        "            # Default Embedding Model\n",
        "            {\n",
        "                \"provider\": \"openai\",\n",
        "                \"model_id\": \"text-embedding-3-small\",\n",
        "                \"display_name\": \"text-embedding-3-small\",\n",
        "                \"model_type\": \"embed\",\n",
        "                \"enabled\": 1,\n",
        "                \"is_default\": 1,\n",
        "                \"cost_score\": 1,\n",
        "                \"latency_score\": 1,\n",
        "                \"max_context_tokens\": None,\n",
        "                \"api_base\": None,\n",
        "                \"notes\": \"Primary embedding model\",\n",
        "            },\n",
        "        ]\n",
        "\n",
        "        for row in defaults:\n",
        "            cur.execute(\n",
        "                \"\"\"\n",
        "                INSERT INTO model_registry (\n",
        "                    provider, model_id, display_name, model_type,\n",
        "                    enabled, is_default, cost_score, latency_score,\n",
        "                    max_context_tokens, api_base, notes\n",
        "                )\n",
        "                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
        "                \"\"\",\n",
        "                (\n",
        "                    row[\"provider\"],\n",
        "                    row[\"model_id\"],\n",
        "                    row[\"display_name\"],\n",
        "                    row[\"model_type\"],\n",
        "                    row[\"enabled\"],\n",
        "                    row[\"is_default\"],\n",
        "                    row[\"cost_score\"],\n",
        "                    row[\"latency_score\"],\n",
        "                    row[\"max_context_tokens\"],\n",
        "                    row[\"api_base\"],\n",
        "                    row[\"notes\"],\n",
        "                ),\n",
        "            )\n",
        "\n",
        "        conn.commit()\n",
        "\n",
        "    conn.close()\n",
        "\n",
        "\n",
        "def list_models(model_type: str | None = None, only_enabled: bool = True) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Return raw dict rows from the model_registry table.\n",
        "\n",
        "    Args:\n",
        "        model_type: 'chat', 'embed', etc. (optional)\n",
        "        only_enabled: filter to enabled == 1\n",
        "    \"\"\"\n",
        "    ensure_model_registry_table()\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "\n",
        "    sql = \"\"\"\n",
        "        SELECT provider, model_id, display_name, model_type,\n",
        "               enabled, is_default, cost_score, latency_score,\n",
        "               max_context_tokens, api_base, notes\n",
        "        FROM model_registry\n",
        "        WHERE 1=1\n",
        "    \"\"\"\n",
        "    params: list[Any] = []\n",
        "\n",
        "    if model_type:\n",
        "        sql += \" AND model_type = ?\"\n",
        "        params.append(model_type)\n",
        "\n",
        "    if only_enabled:\n",
        "        sql += \" AND enabled = 1\"\n",
        "\n",
        "    sql += \" ORDER BY model_type, is_default DESC, model_id\"\n",
        "\n",
        "    cur.execute(sql, params)\n",
        "    rows = cur.fetchall()\n",
        "    conn.close()\n",
        "\n",
        "    keys = [\n",
        "        \"provider\", \"model_id\", \"display_name\", \"model_type\", \"enabled\",\n",
        "        \"is_default\", \"cost_score\", \"latency_score\", \"max_context_tokens\",\n",
        "        \"api_base\", \"notes\",\n",
        "    ]\n",
        "\n",
        "    return [dict(zip(keys, r)) for r in rows]\n",
        "\n",
        "\n",
        "def add_or_update_model(\n",
        "    model_type: str,\n",
        "    model_id: str,\n",
        "    provider: str = \"openai\",\n",
        "    display_name: str | None = None,\n",
        "    enabled: bool = True,\n",
        "    is_default: bool = False,\n",
        "    cost_score: int = 2,\n",
        "    latency_score: int = 2,\n",
        "    max_context_tokens: int | None = None,\n",
        "    api_base: str | None = None,\n",
        "    notes: str | None = None,\n",
        ") -> None:\n",
        "    \"\"\"Upsert a model row into model_registry.\n",
        "\n",
        "    This is the single, canonical write-path used by:\n",
        "      - EPIC 5.2 seeding\n",
        "      - Admin \"Model Settings\" enable/disable + default selection\n",
        "\n",
        "    Notes:\n",
        "      - display_name defaults to model_id\n",
        "      - enabled/is_default are stored as INTEGER 0/1\n",
        "      - If is_default=True, this also enforces a single default per model_type.\n",
        "    \"\"\"\n",
        "    ensure_model_registry_table()\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "\n",
        "    mid = (model_id or \"\").strip()\n",
        "    mtype = (model_type or \"\").strip()\n",
        "    if not mid or not mtype:\n",
        "        conn.close()\n",
        "        raise ValueError(\"model_type and model_id are required\")\n",
        "\n",
        "    dn = (display_name or mid).strip()\n",
        "\n",
        "    cur.execute(\n",
        "        \"\"\"\n",
        "        SELECT id FROM model_registry\n",
        "        WHERE model_type = ? AND model_id = ?\n",
        "        \"\"\",\n",
        "        (mtype, mid),\n",
        "    )\n",
        "    row = cur.fetchone()\n",
        "\n",
        "    if row:\n",
        "        cur.execute(\n",
        "            \"\"\"\n",
        "            UPDATE model_registry\n",
        "            SET provider=?,\n",
        "                display_name=?,\n",
        "                enabled=?,\n",
        "                is_default=?,\n",
        "                cost_score=?,\n",
        "                latency_score=?,\n",
        "                max_context_tokens=?,\n",
        "                api_base=?,\n",
        "                notes=?\n",
        "            WHERE model_type=? AND model_id=?\n",
        "            \"\"\",\n",
        "            (\n",
        "                provider,\n",
        "                dn,\n",
        "                1 if enabled else 0,\n",
        "                1 if is_default else 0,\n",
        "                int(cost_score) if cost_score is not None else 2,\n",
        "                int(latency_score) if latency_score is not None else 2,\n",
        "                max_context_tokens,\n",
        "                api_base,\n",
        "                notes,\n",
        "                mtype,\n",
        "                mid,\n",
        "            ),\n",
        "        )\n",
        "    else:\n",
        "        cur.execute(\n",
        "            \"\"\"\n",
        "            INSERT INTO model_registry\n",
        "                (provider, model_id, display_name, model_type, enabled, is_default,\n",
        "                 cost_score, latency_score, max_context_tokens, api_base, notes)\n",
        "            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
        "            \"\"\",\n",
        "            (\n",
        "                provider,\n",
        "                mid,\n",
        "                dn,\n",
        "                mtype,\n",
        "                1 if enabled else 0,\n",
        "                1 if is_default else 0,\n",
        "                int(cost_score) if cost_score is not None else 2,\n",
        "                int(latency_score) if latency_score is not None else 2,\n",
        "                max_context_tokens,\n",
        "                api_base,\n",
        "                notes,\n",
        "            ),\n",
        "        )\n",
        "\n",
        "    conn.commit()\n",
        "\n",
        "    # Enforce single default per type (and avoid leaving the DB in a multi-default state)\n",
        "    if is_default:\n",
        "        conn.close()\n",
        "        set_default_model(mtype, mid)\n",
        "        return\n",
        "\n",
        "    conn.close()\n",
        "\n",
        "\n",
        "def set_default_model(model_type: str, model_id: str) -> None:\n",
        "    \"\"\"Mark exactly one model as default for a given model_type.\"\"\"\n",
        "    ensure_model_registry_table()\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "\n",
        "    mid = (model_id or \"\").strip()\n",
        "    mtype = (model_type or \"\").strip()\n",
        "    if not mid or not mtype:\n",
        "        conn.close()\n",
        "        raise ValueError(\"model_type and model_id are required\")\n",
        "\n",
        "    # Clear existing defaults for this type\n",
        "    cur.execute(\n",
        "        \"\"\"\n",
        "        UPDATE model_registry\n",
        "        SET is_default = 0\n",
        "        WHERE model_type = ?\n",
        "        \"\"\",\n",
        "        (mtype,),\n",
        "    )\n",
        "\n",
        "    # Set requested default (ensure it is enabled as well)\n",
        "    cur.execute(\n",
        "        \"\"\"\n",
        "        UPDATE model_registry\n",
        "        SET is_default = 1,\n",
        "            enabled = 1\n",
        "        WHERE model_type = ? AND model_id = ?\n",
        "        \"\"\",\n",
        "        (mtype, mid),\n",
        "    )\n",
        "\n",
        "    # If the model doesn't exist yet, create it as enabled default.\n",
        "    if cur.rowcount == 0:\n",
        "        conn.commit()\n",
        "        conn.close()\n",
        "        add_or_update_model(model_type=mtype, model_id=mid, enabled=True, is_default=True)\n",
        "        return\n",
        "\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "\n",
        "def load_model_registry() -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Loads all models from the registry and ensures defaults exist.\n",
        "    \"\"\"\n",
        "    ensure_model_registry_table()\n",
        "    seed_default_models()  # <-- CRITICAL LINE (missing previously)\n",
        "\n",
        "    conn = get_db_conn()\n",
        "    conn.row_factory = sqlite3.Row\n",
        "    cur = conn.cursor()\n",
        "\n",
        "    cur.execute(\n",
        "        \"\"\"\n",
        "        SELECT\n",
        "            id,\n",
        "            provider,\n",
        "            model_id,\n",
        "            model_id AS model,          -- backward compatibility\n",
        "            display_name,\n",
        "            model_type AS type,         -- backward compatibility\n",
        "            enabled,\n",
        "            is_default,\n",
        "            cost_score,\n",
        "            latency_score,\n",
        "            max_context_tokens,\n",
        "            api_base,\n",
        "            notes\n",
        "        FROM model_registry\n",
        "        ORDER BY model_type, is_default DESC, model_id\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    rows = [dict(r) for r in cur.fetchall()]\n",
        "    conn.close()\n",
        "    return rows\n",
        "\n",
        "# ============================================================"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c712ced",
      "metadata": {
        "id": "9c712ced"
      },
      "source": [
        "## CELL 5.1 / STEP 5.1 – ModelConfig & Lookup Helpers (v16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "713b7c60",
      "metadata": {
        "id": "713b7c60"
      },
      "outputs": [],
      "source": [
        "# CELL 5.1 / STEP 5.1 – ModelConfig & Lookup Helpers (v16)\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, List, Dict, Any\n",
        "\n",
        "@dataclass\n",
        "class ModelConfig:\n",
        "    model_id: str\n",
        "    provider: str = \"openai\"\n",
        "    display_name: str | None = None\n",
        "    model_type: str = \"chat\"   # 'chat', 'embed', 'rerank', etc.\n",
        "    is_default: bool = False\n",
        "    enabled: bool = True\n",
        "    cost_score: int = 2\n",
        "    latency_score: int = 2\n",
        "    max_context_tokens: int | None = None\n",
        "    api_base: str | None = None\n",
        "    notes: str | None = None\n",
        "\n",
        "    def as_kwargs(self) -> dict:\n",
        "        \"\"\"\n",
        "        Common kwargs we might pass into a client, e.g. OpenAI.\n",
        "        For now this is mostly for future extensibility.\n",
        "        \"\"\"\n",
        "        kw = {\"model\": self.model_id}\n",
        "        if self.api_base:\n",
        "            kw[\"api_base\"] = self.api_base\n",
        "        return kw\n",
        "\n",
        "\n",
        "def load_model_configs(model_type: str, only_enabled: bool = True) -> list[ModelConfig]:\n",
        "    \"\"\"\n",
        "    Load models from the registry as ModelConfig objects.\n",
        "    Relies on list_models(...) from STEP 5.0.\n",
        "    \"\"\"\n",
        "    rows = list_models(model_type=model_type, only_enabled=only_enabled)\n",
        "    configs: list[ModelConfig] = []\n",
        "    for row in rows:\n",
        "        configs.append(\n",
        "            ModelConfig(\n",
        "                model_id=row[\"model_id\"],\n",
        "                provider=row[\"provider\"],\n",
        "                display_name=row[\"display_name\"],\n",
        "                model_type=row[\"model_type\"],\n",
        "                is_default=row[\"is_default\"],\n",
        "                enabled=row[\"enabled\"],\n",
        "                cost_score=row[\"cost_score\"],\n",
        "                latency_score=row[\"latency_score\"],\n",
        "                max_context_tokens=row[\"max_context_tokens\"],\n",
        "                api_base=row[\"api_base\"],\n",
        "                notes=row[\"notes\"],\n",
        "            )\n",
        "        )\n",
        "    return configs\n",
        "\n",
        "\n",
        "def get_default_model(model_type: str) -> Optional[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Low-level helper used by the *Config() helpers below.\n",
        "\n",
        "    Looks in the model_registry for the default model of a given type.\n",
        "    Strategy:\n",
        "      1) Use list_models(model_type, only_enabled=True)\n",
        "      2) Return the one where is_default = True, if present\n",
        "      3) Otherwise return the first enabled model\n",
        "      4) If none exist, return None\n",
        "    \"\"\"\n",
        "    rows = list_models(model_type=model_type, only_enabled=True)\n",
        "    if not rows:\n",
        "        return None\n",
        "\n",
        "def get_default_chat_model_id() -> str:\n",
        "    \"\"\"Return the registry default chat model_id (fallback to gpt-4.1-mini).\"\"\"\n",
        "    row = get_default_model(\"chat\")\n",
        "    if row and row.get(\"model_id\"):\n",
        "        return str(row[\"model_id\"])\n",
        "    return \"gpt-4.1-mini\"\n",
        "\n",
        "\n",
        "    for row in rows:\n",
        "        if row.get(\"is_default\"):\n",
        "            return row\n",
        "\n",
        "    # Fallback: first enabled of that type\n",
        "    return rows[0]\n",
        "\n",
        "\n",
        "def get_default_chat_model_config() -> ModelConfig:\n",
        "    \"\"\"\n",
        "    Returns a ModelConfig for the default chat model.\n",
        "    If none is explicitly set, falls back to the first enabled chat model.\n",
        "    \"\"\"\n",
        "    default_row = get_default_model(\"chat\")\n",
        "    if default_row:\n",
        "        return ModelConfig(\n",
        "            model_id=default_row[\"model_id\"],\n",
        "            provider=default_row[\"provider\"],\n",
        "            display_name=default_row[\"display_name\"],\n",
        "            model_type=default_row[\"model_type\"],\n",
        "            is_default=default_row[\"is_default\"],\n",
        "            enabled=default_row[\"enabled\"],\n",
        "            cost_score=default_row[\"cost_score\"],\n",
        "            latency_score=default_row[\"latency_score\"],\n",
        "            max_context_tokens=default_row[\"max_context_tokens\"],\n",
        "            api_base=default_row[\"api_base\"],\n",
        "            notes=default_row[\"notes\"],\n",
        "        )\n",
        "\n",
        "    # Fallback: first enabled chat model\n",
        "    configs = load_model_configs(\"chat\", only_enabled=True)\n",
        "    if configs:\n",
        "        return configs[0]\n",
        "\n",
        "    # Last resort hard-coded default (should not happen due to seeding)\n",
        "    return ModelConfig(\n",
        "        model_id=\"gpt-4.1-mini\",\n",
        "        provider=\"openai\",\n",
        "        display_name=\"GPT-4.1 Mini (fallback)\",\n",
        "        model_type=\"chat\",\n",
        "        is_default=True,\n",
        "        enabled=True,\n",
        "        cost_score=1,\n",
        "        latency_score=1,\n",
        "        max_context_tokens=128_000,\n",
        "    )\n",
        "\n",
        "\n",
        "def get_default_embed_model_config() -> ModelConfig:\n",
        "    \"\"\"\n",
        "    Returns a ModelConfig for the default embedding model.\n",
        "    \"\"\"\n",
        "    default_row = get_default_model(\"embed\")\n",
        "    if default_row:\n",
        "        return ModelConfig(\n",
        "            model_id=default_row[\"model_id\"],\n",
        "            provider=default_row[\"provider\"],\n",
        "            display_name=default_row[\"display_name\"],\n",
        "            model_type=default_row[\"model_type\"],\n",
        "            is_default=default_row[\"is_default\"],\n",
        "            enabled=default_row[\"enabled\"],\n",
        "            cost_score=default_row[\"cost_score\"],\n",
        "            latency_score=default_row[\"latency_score\"],\n",
        "            max_context_tokens=default_row[\"max_context_tokens\"],\n",
        "            api_base=default_row[\"api_base\"],\n",
        "            notes=default_row[\"notes\"],\n",
        "        )\n",
        "\n",
        "    # Fallback: first enabled embed model\n",
        "    configs = load_model_configs(\"embed\", only_enabled=True)\n",
        "    if configs:\n",
        "        return configs[0]\n",
        "\n",
        "    # Last resort hard-coded default\n",
        "    return ModelConfig(\n",
        "        model_id=\"text-embedding-3-small\",\n",
        "        provider=\"openai\",\n",
        "        display_name=\"text-embedding-3-small (fallback)\",\n",
        "        model_type=\"embed\",\n",
        "        is_default=True,\n",
        "        enabled=True,\n",
        "        cost_score=1,\n",
        "        latency_score=1,\n",
        "    )\n",
        "def list_chat_models() -> list[str]:\n",
        "    \"\"\"Return enabled chat models for the Ask tab dropdown.\n",
        "\n",
        "    Source of truth:\n",
        "      1) If the EPIC 5.2 model registry exists, return enabled chat models from it.\n",
        "      2) Otherwise, fall back to the preloaded EPIC52_CHAT_MODELS list.\n",
        "    \"\"\"\n",
        "    # Preferred: EPIC 5.2 registry (persisted enabled flags)\n",
        "    try:\n",
        "        enabled = [m.get(\"model_id\") for m in (list_models(\"chat\", only_enabled=True) or []) if m.get(\"model_id\")]\n",
        "        enabled = [m for m in enabled if m]\n",
        "        if enabled:\n",
        "            return enabled\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # Fallback: static list\n",
        "    return list(EPIC52_CHAT_MODELS)\n",
        "\n",
        "def list_all_chat_models() -> list[str]:\n",
        "    \"\"\"\n",
        "    Returns the complete set of chat models that admins can enable/disable.\n",
        "    This is the EPIC 5.2 curated list (not an API discovery).\n",
        "    \"\"\"\n",
        "    return EPIC52_CHAT_MODELS[:]\n",
        "\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# EPIC 5.2 — Robust Chat-Model Registry Helpers (override/fix)\n",
        "# ============================================================\n",
        "# NOTE: These helpers intentionally override any earlier partial implementations.\n",
        "EPIC52_CHAT_MODELS = [\"gpt-4.1-mini\", \"gpt-5\", \"gpt-5-mini\", \"\"]\n",
        "\n",
        "def list_all_chat_models() -> list[str]:\n",
        "    \"\"\"Canonical list of chat models exposed in EPIC 5.2.\"\"\"\n",
        "    return list(EPIC52_CHAT_MODELS)\n",
        "\n",
        "def seed_epic52_chat_models(default_model_id: str = \"gpt-4.1-mini\") -> None:\n",
        "    \"\"\"Ensure EPIC 5.2 models exist in model_registry and set a sane default.\"\"\"\n",
        "    try:\n",
        "        ensure_model_registry_table()\n",
        "    except Exception as e:\n",
        "        trace_log(f\"seed_epic52_chat_models ensure table ERROR: {e}\")\n",
        "        return\n",
        "\n",
        "    for mid in EPIC52_CHAT_MODELS:\n",
        "        try:  # Prefer existing helper if present\n",
        "            add_or_update_model(model_type=\"chat\", model_id=mid, enabled=True, is_default=(mid == default_model_id))\n",
        "        except Exception:\n",
        "            # Fallback: write directly\n",
        "            try:\n",
        "                conn = get_db_conn()\n",
        "                cur = conn.cursor()\n",
        "                cur.execute(\n",
        "                    \"\"\"\n",
        "                    INSERT INTO model_registry (model_type, model_id, enabled, is_default)\n",
        "                    VALUES (?, ?, ?, ?)\n",
        "                    ON CONFLICT(model_type, model_id) DO UPDATE SET\n",
        "                        enabled=excluded.enabled,\n",
        "                        is_default=excluded.is_default\n",
        "                    \"\"\",\n",
        "                    (\"chat\", mid, 1, 1 if mid == default_model_id else 0),\n",
        "                )\n",
        "                conn.commit()\n",
        "                conn.close()\n",
        "            except Exception as e:\n",
        "                trace_log(f\"seed_epic52_chat_models DB write ERROR model={mid}: {e}\")\n",
        "\n",
        "    # Ensure only one default (if helper exists)\n",
        "    try:\n",
        "        set_default_model(\"chat\", default_model_id)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "def get_allowed_chat_models() -> list[str]:\n",
        "    \"\"\"Enabled chat models, falling back to EPIC 5.2 defaults.\"\"\"\n",
        "    try:\n",
        "        rows = list_models(\"chat\", only_enabled=True)\n",
        "    except Exception as e:\n",
        "        trace_log(f\"get_allowed_chat_models list_models ERROR: {e}\")\n",
        "        return list(EPIC52_CHAT_MODELS)\n",
        "\n",
        "    if not rows:\n",
        "        return list(EPIC52_CHAT_MODELS)\n",
        "\n",
        "    out: list[str] = []\n",
        "    for r in rows:\n",
        "        if isinstance(r, dict):\n",
        "            mid = r.get(\"model_id\")\n",
        "        else:\n",
        "            mid = r[1] if isinstance(r, (list, tuple)) and len(r) > 1 else None\n",
        "        if mid:\n",
        "            out.append(str(mid))\n",
        "    return out or list(EPIC52_CHAT_MODELS)\n",
        "\n",
        "def get_default_model(model_type: str = \"chat\") -> dict | None:\n",
        "    \"\"\"Return the default model row (enabled preferred); otherwise first enabled.\"\"\"\n",
        "    try:\n",
        "        rows = list_models(model_type, only_enabled=True)\n",
        "    except Exception as e:\n",
        "        trace_log(f\"get_default_model list_models ERROR: {e}\")\n",
        "        rows = []\n",
        "\n",
        "    if not rows:\n",
        "        return None\n",
        "\n",
        "    for r in rows:\n",
        "        if isinstance(r, dict) and r.get(\"is_default\"):\n",
        "            return r\n",
        "\n",
        "    return rows[0] if isinstance(rows[0], dict) else None\n",
        "\n",
        "def get_default_chat_model_id() -> str:\n",
        "    row = get_default_model(\"chat\")\n",
        "    if isinstance(row, dict) and row.get(\"model_id\"):\n",
        "        return str(row[\"model_id\"])\n",
        "    return EPIC52_CHAT_MODELS[0]\n",
        "\n",
        "# Seed models on import/run (idempotent)\n",
        "try:\n",
        "    seed_epic52_chat_models(default_model_id=\"gpt-4.1-mini\")\n",
        "except Exception as _e:\n",
        "    trace_log(f\"EPIC 5.2 seed models ERROR: {_e}\")\n",
        "\n",
        "# ============================================================\n",
        "# EPIC 5.2 — list_chat_models() override (Ask tab dropdown)\n",
        "# ============================================================\n",
        "def list_chat_models_static() -> list[str]:\n",
        "    \"\"\"Return enabled chat models for the Ask tab dropdown.\"\"\"\n",
        "    try:\n",
        "        return get_allowed_chat_models()\n",
        "    except Exception:\n",
        "        return [\"gpt-4.1-mini\", \"gpt-5\", \"gpt-5-mini\", \"\"]\n",
        "\n",
        "\n",
        "# Ensure EPIC 5.2 chat models exist on startup\n",
        "try:\n",
        "    seed_epic52_chat_models()\n",
        "except Exception as e:\n",
        "    trace_log(f\"seed_epic52_chat_models WARNING: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf8df8fc",
      "metadata": {
        "id": "cf8df8fc"
      },
      "source": [
        "## CELL 5.2 / STEP 5.2 – RAG Retrieval Over a Cohort (v16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "41fd48df",
      "metadata": {
        "id": "41fd48df"
      },
      "outputs": [],
      "source": [
        "# CELL 5.2 / STEP 5.2 – RAG Retrieval Over a Cohort (v16)\n",
        "\n",
        "# ============================================================\n",
        "# EPIC 2.3 (v17_1) – Retrieval Quality Controls\n",
        "\n",
        "# ============================================================\n",
        "\n",
        "RAG_TOPK_SMALL = 3\n",
        "RAG_TOPK_DEFAULT = 5\n",
        "RAG_TOPK_LARGE = 8\n",
        "\n",
        "# Since you normalize embeddings, FAISS scores here are cosine-like similarity.\n",
        "# Start conservative; tune after observing scores in Why panel.\n",
        "RAG_SCORE_THRESHOLD_DEFAULT = 0.25\n",
        "\n",
        "# ============================================================\n",
        "# EPIC 4.0.1 (v17_4) – Optional Query Rewrite for Retrieval\n",
        "# ============================================================\n",
        "EPIC40_QUERY_REWRITE_ENABLED = True  # set False to disable\n",
        "EPIC40_QUERY_REWRITE_MAX_TOKENS = 80\n",
        "\n",
        "\n",
        "# Optional diversity control (leave None to disable)\n",
        "RAG_MAX_CHUNKS_PER_DOC_DEFAULT = 2\n",
        "\n",
        "\n",
        "def choose_top_k_for_query(query: str) -> int:\n",
        "    \"\"\"\n",
        "    Heuristic: choose top_k based on query length/complexity.\n",
        "    Simple + explainable; no extra dependencies.\n",
        "    \"\"\"\n",
        "    q = (query or \"\").strip().lower()\n",
        "    if not q:\n",
        "        return RAG_TOPK_DEFAULT\n",
        "\n",
        "    words = q.split()\n",
        "    n_words = len(words)\n",
        "\n",
        "    # \"Complex\" indicators\n",
        "    complex_markers = [\n",
        "        \"compare\", \"contrast\", \"summarize\", \"synthesize\", \"evaluate\",\n",
        "        \"pros and cons\", \"tradeoff\", \"why\", \"how\", \"explain\",\n",
        "        \"include\", \"including\", \"as well as\"\n",
        "    ]\n",
        "    has_complex = any(m in q for m in complex_markers) or q.count(\"?\") >= 2\n",
        "\n",
        "    if n_words <= 10 and not has_complex:\n",
        "        return RAG_TOPK_SMALL\n",
        "\n",
        "    if n_words >= 25 or has_complex:\n",
        "        return RAG_TOPK_LARGE\n",
        "\n",
        "    return RAG_TOPK_DEFAULT\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def build_context_from_index(\n",
        "    api_key: str,\n",
        "    chat_model: str,\n",
        "    embed_model: str,\n",
        "    cohort_name: str,\n",
        "    query: str,\n",
        "    top_k: int = 5,\n",
        "    score_threshold: float | None = None,\n",
        "    max_chunks_per_doc: int | None = None,\n",
        "):\n",
        "\n",
        "    # EPIC 3.3.1 — Session-scope questions may run without a cohort\n",
        "    if not cohort_name:\n",
        "        return \"\", [], []\n",
        "\n",
        "    \"\"\"\n",
        "    Given a cohort and query:\n",
        "    - Load all documents for that cohort\n",
        "    - For each doc's index, perform similarity search\n",
        "    - Aggregate top_k results across docs\n",
        "    Returns:\n",
        "      - concatenated context string\n",
        "      - list of (doc_name, rank, score) for citations\n",
        "    \"\"\"\n",
        "\n",
        "    ensure_docs_table()\n",
        "    resolved_chat, resolved_embed = resolve_models(chat_model, embed_model)\n",
        "    client = build_openai_client(api_key)\n",
        "\n",
        "    # 1. Load all docs & their index references\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "    cur.execute(\n",
        "        \"\"\"\n",
        "        SELECT doc_name, index_id\n",
        "        FROM documents\n",
        "        WHERE cohort_name = ?\n",
        "        \"\"\",\n",
        "        (cohort_name,),\n",
        "    )\n",
        "    rows = cur.fetchall()\n",
        "    conn.close()\n",
        "\n",
        "    if not rows:\n",
        "        return \"\", [], []\n",
        "\n",
        "    # 2. Embed query once\n",
        "    q_embed_resp = client.embeddings.create(model=resolved_embed, input=[query])\n",
        "    q_vec = np.array(q_embed_resp.data[0].embedding, dtype=\"float32\")\n",
        "    q_vec = q_vec / (np.linalg.norm(q_vec) + 1e-10)\n",
        "\n",
        "    all_hits = []  # (doc_name, idx, score, text)\n",
        "\n",
        "    # 3. Perform similarity search in each doc index\n",
        "    for doc_name, index_id in rows:\n",
        "        try:\n",
        "            index = load_index(index_id)\n",
        "            meta = load_metadata(index_id)  # {\"chunks\": [...]}\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "        D, I = index.search(q_vec[np.newaxis, :], top_k)\n",
        "        scores = D[0]\n",
        "        idxs = I[0]\n",
        "\n",
        "        for score, idx in zip(scores, idxs):\n",
        "            if idx < 0:\n",
        "                continue\n",
        "\n",
        "            score_f = float(score)\n",
        "\n",
        "            # EPIC 2.3.2 — Similarity thresholding (skip weak hits)\n",
        "            if score_threshold is not None and score_f < float(score_threshold):\n",
        "                continue\n",
        "\n",
        "            chunks = meta.get(\"chunks\", [])\n",
        "            if idx >= len(chunks):\n",
        "                continue\n",
        "\n",
        "            text_chunk = chunks[idx]\n",
        "            if not text_chunk or not str(text_chunk).strip():\n",
        "                continue\n",
        "\n",
        "            all_hits.append((doc_name, idx, score_f, text_chunk))\n",
        "\n",
        "\n",
        "    if not all_hits:\n",
        "        return \"\", [], []\n",
        "\n",
        "    # 4. Sort & select top results\n",
        "    # 4. Sort & select top results\n",
        "    all_hits.sort(key=lambda x: x[2], reverse=True)\n",
        "\n",
        "    # EPIC 2.3 — Optional diversity control (limit chunks per document)\n",
        "    if max_chunks_per_doc is not None and max_chunks_per_doc > 0:\n",
        "        selected = []\n",
        "        per_doc_counts = {}\n",
        "        for (dn, idx, sc, txt) in all_hits:\n",
        "            per_doc_counts[dn] = per_doc_counts.get(dn, 0) + 1\n",
        "            if per_doc_counts[dn] > max_chunks_per_doc:\n",
        "                continue\n",
        "            selected.append((dn, idx, sc, txt))\n",
        "            if len(selected) >= top_k:\n",
        "                break\n",
        "        top_hits = selected\n",
        "    else:\n",
        "        top_hits = all_hits[:top_k]\n",
        "\n",
        "\n",
        "    context_parts = []\n",
        "    citations = []\n",
        "\n",
        "    for rank, (doc_name, idx, score, text) in enumerate(top_hits, start=1):\n",
        "        header = f\"[{rank}] From {doc_name} (chunk #{idx}, score={score:.3f})\"\n",
        "        context_parts.append(header + \"\\n\" + text)\n",
        "        citations.append((doc_name, rank, score))\n",
        "\n",
        "    context = \"\\n\\n\".join(context_parts)\n",
        "    return context, citations, top_hits\n",
        "\n",
        "# ============================================================\n",
        "# UPDATED answer_with_rag() — Now uses Routing Brain (v16)\n",
        "\n",
        "def answer_with_rag(\n",
        "    api_key: str,\n",
        "    chat_model: str,\n",
        "    embed_model: str,\n",
        "    cohort_name: str,\n",
        "    query: str,\n",
        "    system_prompt: str = \"\",\n",
        "    user_pref: str | None = None,\n",
        "    top_k: int | None = None,\n",
        "    score_threshold: float | None = None,\n",
        "    max_chunks_per_doc: int | None = None,\n",
        "    provenance_mode: str = 'Off',  # EPIC 3.4\n",
        "    session_mem: dict | None = None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Perform RAG retrieval and return:\n",
        "        - answer_markdown\n",
        "        - raw_answer_text\n",
        "        - model_used\n",
        "        - why_md_text\n",
        "        - footnote_text\n",
        "    \"\"\"\n",
        "\n",
        "    answer_source = \"Cohort Context\"  # EPIC 3.3 — provenance label\n",
        "\n",
        "\n",
        "    # EPIC 2.3 — Retrieval policy\n",
        "    chosen_top_k = top_k if top_k is not None else choose_top_k_for_query(query)\n",
        "    chosen_threshold = score_threshold if score_threshold is not None else RAG_SCORE_THRESHOLD_DEFAULT\n",
        "    chosen_max_per_doc = (\n",
        "        max_chunks_per_doc\n",
        "        if max_chunks_per_doc is not None\n",
        "        else RAG_MAX_CHUNKS_PER_DOC_DEFAULT\n",
        "    )\n",
        "\n",
        "    # 1) Build RAG context\n",
        "    context, citations, top_hits = build_context_from_index(\n",
        "        api_key=api_key,\n",
        "        chat_model=chat_model,\n",
        "        embed_model=embed_model,\n",
        "        cohort_name=cohort_name,\n",
        "        query=query,\n",
        "        top_k=chosen_top_k,\n",
        "        score_threshold=chosen_threshold,\n",
        "        max_chunks_per_doc=chosen_max_per_doc,\n",
        "    )\n",
        "\n",
        "    if not context:\n",
        "\n",
        "\n",
        "        # EPIC 3.2 Option 1: If session memory is enabled and has turns, fall back to session memory only.\n",
        "\n",
        "\n",
        "        if session_mem and session_mem.get('enabled') and session_mem.get('turns'):\n",
        "\n",
        "\n",
        "            context = format_session_memory_prompt(session_mem.get('turns') or [])\n",
        "            answer_source = \"Session Memory (fallback)\"  # EPIC 3.3\n",
        "\n",
        "\n",
        "            citations, top_hits = [], []\n",
        "\n",
        "\n",
        "        else:\n",
        "\n",
        "\n",
        "            return (\n",
        "\n",
        "\n",
        "                \"I could not find any context for this query in the selected cohort.\",\n",
        "\n",
        "\n",
        "                \"\",\n",
        "\n",
        "\n",
        "                \"N/A\",\n",
        "\n",
        "\n",
        "                \"\",\n",
        "\n",
        "\n",
        "                \"\",\n",
        "\n",
        "\n",
        "            )\n",
        "\n",
        "    # 2) Default system prompt\n",
        "    if not system_prompt:\n",
        "        system_prompt = (\n",
        "            \"You are a helpful assistant answering questions based on the provided context.\\n\"\n",
        "            \"If the answer cannot be found in the context, say you do not know.\"\n",
        "        )\n",
        "\n",
        "\n",
        "    # EPIC 3.4 — Provenance-mode prompt shaping\n",
        "    _prov = (provenance_mode or \"Off\").strip()\n",
        "    _prov_l = _prov.lower()\n",
        "    if _prov_l.startswith(\"explain\"):\n",
        "        system_prompt += (\n",
        "            \"\\n\\n\"\n",
        "            \"EPIC 3.4 — When you answer, separate information by source.\\n\"\n",
        "            \"Provide two labeled sections:\\n\"\n",
        "            \"1) 'From retrieved documents' — only statements supported by the provided context. Tag each bullet with [DOC].\\n\"\n",
        "            \"2) 'Model reasoning / general knowledge' — any inference, interpretation, or external knowledge. Tag each bullet with [MODEL].\\n\"\n",
        "            \"Do not present [MODEL] statements as if they were stated in the documents.\\n\"\n",
        "        )\n",
        "    elif \"strict\" in _prov_l:\n",
        "        system_prompt += (\n",
        "            \"\\n\\n\"\n",
        "            \"EPIC 3.4 — Strict Evidence Only: Use ONLY the provided context.\\n\"\n",
        "            \"If the context does not support the answer, say so clearly and do not add external facts.\\n\"\n",
        "        )\n",
        "\n",
        "    # 3) Construct chat messages\n",
        "\n",
        "        # 2.5) Response Constraints (MUST FOLLOW)\n",
        "    # Enforce user-requested output constraints such as \"only one\", \"exactly N\", etc.\n",
        "    # Keep this in the system prompt so it applies regardless of provenance mode.\n",
        "    constraints_block = \"\"\"Response Constraints (MUST FOLLOW):\n",
        "1) If the user requests a specific count (e.g., 'one', '2', 'top 3'), return exactly that count—no more.\n",
        "2) If the user requests 'only X' (e.g., only an identifier), do not include additional commentary, explanation, or extra items.\n",
        "3) If multiple candidates exist, select the single best candidate using the strongest support in the retrieved context.\n",
        "4) If constraints conflict with providing a correct answer from context, say: 'Insufficient information in the provided context.' Do not speculate.\n",
        "\"\"\".strip()\n",
        "\n",
        "    system_prompt = (system_prompt or \"\").rstrip() + \"\\n\\n\" + constraints_block\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": (\n",
        "                \"\"\"Use the retrieved context to answer the question.\n",
        "You may use prior session context only to resolve references (e.g., \"it\", \"they\", \"that council\").\n",
        "\n",
        "If the context explicitly states the answer, quote or paraphrase it.\n",
        "If the context implies an answer at a high level but does not list details, provide a clearly labeled:\n",
        "\"Summary based on context:\" and do not invent specifics.\n",
        "In the summary, only describe what the retrieved context suggests; do not add general background or “typically” statements.\n",
        "Only say \"I do not know\" if neither an explicit answer nor a reasonable high-level summary is possible.\n",
        "\n",
        "=== CONTEXT START ===\n",
        "\"\"\"\n",
        "                f\"{context}\\n\"\n",
        "                \"\"\"=== CONTEXT END ===\n",
        "\n",
        "QUESTION: \"\"\"\n",
        "                f\"{query}\"\n",
        "            ),\n",
        "        },\n",
        "    ]\n",
        "\n",
        "# 4) Call the routing brain\n",
        "    answer_text, raw_answer_text, model_used = call_chat_model(\n",
        "        api_key=api_key,\n",
        "        messages=messages,\n",
        "        task_type=\"rag_answer\",\n",
        "        user_pref=user_pref,\n",
        "        context_size=len(context),\n",
        "    )\n",
        "\n",
        "    # 5) Build answer markdown with citations + model info\n",
        "    md = answer_text + \"\\n\\n---\\n\\n**Cited sources:**\\n\"\n",
        "    for doc_name, rank, score in citations:\n",
        "        md += f\"- [{rank}] `{doc_name}` (score={score:.3f})\\n\"\n",
        "    md += f\"\\n\\n**Model Used:** `{model_used}`\\n\"\n",
        "\n",
        "    # EPIC 2.1/2.2 — Why panel + footnote\n",
        "    doc_names = [h[0] for h in top_hits] if top_hits else []\n",
        "    unique_docs = sorted(set(doc_names))\n",
        "\n",
        "    why_lines = []\n",
        "    # EPIC 3.3 — show whether answer used cohort context or session memory\n",
        "    why_lines.append(f\"**Answer source:** {answer_source}\")\n",
        "    why_lines.append(f\"**Model used:** `{model_used}`\")\n",
        "    why_lines.append(f\"**Provenance mode:** `{(provenance_mode or 'Off')}`\")  # EPIC 3.4\n",
        "    why_lines.append(\n",
        "        f\"**Retrieved chunks:** {len(top_hits) if top_hits else 0} \"\n",
        "        f\"(requested_top_k={chosen_top_k}, score_threshold={chosen_threshold}, max_per_doc={chosen_max_per_doc})\"\n",
        "    )\n",
        "    why_lines.append(\"\")\n",
        "\n",
        "    if top_hits:\n",
        "        why_lines.append(\"**Top retrieved evidence:**\")\n",
        "        for rank, (dn, idx, score, _txt) in enumerate(top_hits, start=1):\n",
        "            _ex = (_txt or \"\").strip().replace(\"\\n\", \" \")\n",
        "            _ex = (_ex[:240] + \"…\") if len(_ex) > 240 else _ex\n",
        "            why_lines.append(f\"- {rank}. `{dn}` • chunk `{idx}` • score `{score:.4f}`\")\n",
        "            if _ex:\n",
        "                why_lines.append(f\"  - _Excerpt:_ “{_ex}”\")  # EPIC 3.4\n",
        "    else:\n",
        "        why_lines.append(\"_No retrieval evidence available (no chunks returned)._\")\n",
        "\n",
        "    why_md_text = \"\\n\".join(why_lines)\n",
        "\n",
        "    footnote_text = (\n",
        "        f\"*Grounded in {len(unique_docs)} doc(s) • Retrieved {len(top_hits) if top_hits else 0} chunk(s) • \"\n",
        "        f\"Model: `{model_used}`*\"\n",
        "    )\n",
        "\n",
        "    return md, raw_answer_text, model_used, why_md_text, footnote_text\n",
        "\n",
        "    # Build session memory prompt (EPIC 3.1)\n",
        "    mem_turns = (session_mem or {}).get(\"turns\") or []\n",
        "    mem_prompt = format_session_memory_prompt(mem_turns)\n",
        "\n",
        "    # Base system prompt (keep your existing default behavior, but make it explicit)\n",
        "    base_prompt = (\n",
        "        \"You are a helpful assistant answering questions using retrieved context.\\n\"\n",
        "        \"You may use prior session context only to resolve references \"\n",
        "        \"(such as pronouns or shorthand).\\n\\n\"\n",
        "        \"If the context explicitly states an answer, present it as such.\\n\"\n",
        "        \"If the context does not list details explicitly but clearly describes a structure \"\n",
        "        \"or role at a high level, provide a labeled summary of what it suggests.\\n\"\n",
        "        \"You MUST clearly label summarized or interpreted information.\\n\"\n",
        "        \"Do NOT present inferred details as direct statements from the context.\\n\\n\"\n",
        "        \"Only say you do not know if neither explicit information nor a reasonable \"\n",
        "        \"high-level summary is possible.\"\n",
        "    )\n",
        "\n",
        "    system_prompt = base_prompt if not mem_prompt else (base_prompt + \"\\n\\n\" + mem_prompt)\n",
        "\n",
        "\n",
        "def epic40_rewrite_query_for_retrieval(api_key: str, chat_model: str, question: str) -> str:\n",
        "    \"\"\"EPIC 4.0.1: Rewrite user question into a retrieval-optimized query.\n",
        "    Falls back to the original question on any failure.\n",
        "\n",
        "    Notes:\n",
        "      - Some newer models reject `max_tokens` and require `max_completion_tokens`.\n",
        "        We try the broad-compatible call first, then fall back only on that error.\n",
        "    \"\"\"\n",
        "    q = (question or \"\").strip()\n",
        "    if not api_key or not q:\n",
        "        return q\n",
        "\n",
        "    try:\n",
        "        client = build_openai_client(api_key)\n",
        "        resolved_chat, _ = resolve_models(\n",
        "            chat_model,\n",
        "            EMBED_MODEL_DEFAULT if \"EMBED_MODEL_DEFAULT\" in globals() else \"text-embedding-3-small\",\n",
        "        )\n",
        "\n",
        "        base_kwargs = {\n",
        "            \"model\": resolved_chat,\n",
        "            \"messages\": [\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": (\n",
        "                        \"Rewrite the user question into a short, keyword-rich search query \"\n",
        "                        \"for retrieving relevant passages. Output ONLY the rewritten query.\"\n",
        "                    ),\n",
        "                },\n",
        "                {\"role\": \"user\", \"content\": q},\n",
        "            ],\n",
        "        }\n",
        "        max_out = EPIC40_QUERY_REWRITE_MAX_TOKENS\n",
        "\n",
        "        try:\n",
        "            resp = client.chat.completions.create(\n",
        "                **base_kwargs,\n",
        "                temperature=0.0,\n",
        "                max_tokens=max_out,\n",
        "            )\n",
        "        except Exception as e:\n",
        "            msg = str(e)\n",
        "            if (\"max_tokens\" in msg) and (\"max_completion_tokens\" in msg):\n",
        "                resp = client.chat.completions.create(\n",
        "                    **base_kwargs,\n",
        "                    max_completion_tokens=max_out,\n",
        "                )\n",
        "            else:\n",
        "                raise\n",
        "\n",
        "        out = (resp.choices[0].message.content or \"\").strip()\n",
        "        if len(out) < 3:\n",
        "            return q\n",
        "        return out\n",
        "\n",
        "    except Exception:\n",
        "        return q\n",
        "\n",
        "def answer_question_over_cohort(\n",
        "    api_key: str,\n",
        "    username: str,\n",
        "    cohort_name: str,\n",
        "    question: str,\n",
        "    model_id: str,\n",
        "    provenance_mode: str = \"Off\",\n",
        "    system_prompt: str = \"\",\n",
        "    session_mem: dict | None = None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Wrapper for the RAG retrieval + LLM step.\n",
        "    Returns:\n",
        "      (answer_markdown, raw_answer_text, model_used, why_md_text, footnote_text)\n",
        "    \"\"\"\n",
        "\n",
        "    # Normalize inputs\n",
        "    final_question = (question or \"\").strip()\n",
        "    cohort_name = (cohort_name or \"\").strip()\n",
        "    chat_model, embed_model = resolve_models(model_id, EMBED_MODEL_DEFAULT)\n",
        "\n",
        "    if not final_question:\n",
        "        return \"❌ Please enter a question.\", \"\", chat_model, \"\", \"\"\n",
        "\n",
        "    # EPIC 4.0.1 — build retrieval query (optional rewrite)\n",
        "    retrieval_query = epic40_build_retrieval_query(api_key, chat_model, final_question)\n",
        "\n",
        "    # If we have a cohort, do RAG\n",
        "    if cohort_name:\n",
        "        return answer_with_rag(\n",
        "            api_key=api_key,\n",
        "            chat_model=chat_model,\n",
        "            embed_model=embed_model,\n",
        "            cohort_name=cohort_name,\n",
        "            query=retrieval_query,\n",
        "            system_prompt=system_prompt or \"\",\n",
        "            user_pref=model_id,\n",
        "            provenance_mode=provenance_mode or \"Off\",\n",
        "            session_mem=session_mem,\n",
        "        )\n",
        "\n",
        "    # Cohortless path (allowed only when UI lets it through)\n",
        "    # Use session memory prompt if available\n",
        "    mem_turns = (session_mem or {}).get(\"turns\") or []\n",
        "    mem_prompt = format_session_memory_prompt(mem_turns) if mem_turns else \"\"\n",
        "\n",
        "    base_prompt = (system_prompt or \"\").strip() or (\n",
        "        \"You are a helpful assistant. Answer the user's question clearly and accurately.\"\n",
        "    )\n",
        "    sys_prompt = base_prompt if not mem_prompt else (base_prompt + \"\\n\\n\" + mem_prompt)\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": sys_prompt},\n",
        "        {\"role\": \"user\", \"content\": final_question},\n",
        "    ]\n",
        "\n",
        "    answer_text, raw_answer_text, model_used = call_chat_model(\n",
        "        api_key=api_key,\n",
        "        messages=messages,\n",
        "        task_type=\"rag_answer\",\n",
        "        user_pref=model_id,\n",
        "        context_size=len(final_question),\n",
        "        dry_run=False,\n",
        "    )\n",
        "\n",
        "    answer_md = (answer_text or raw_answer_text or \"\").strip()\n",
        "\n",
        "    why_md = \"\\n\".join(\n",
        "        [\n",
        "            \"**Answer source:** No cohort selected (no document retrieval).\",\n",
        "            f\"**Model used:** `{model_used}`\",\n",
        "            f\"**Provenance mode:** `{(provenance_mode or 'Off')}`\",\n",
        "        ]\n",
        "    )\n",
        "    footnote = \"No cited sources available because no cohort was selected.\"\n",
        "\n",
        "    return answer_md, (raw_answer_text or \"\"), model_used, why_md, footnote\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# EPIC 4.0.1 – Build retrieval query (optional rewrite)\n",
        "# ============================================================\n",
        "# EPIC 4.0.1 – Retrieval query builder helper\n",
        "def epic40_build_retrieval_query(\n",
        "    api_key: str,\n",
        "    chat_model: str,\n",
        "    question: str,\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Build the retrieval query from the user question.\n",
        "    Optionally applies EPIC 4.0.1 query rewriting.\n",
        "    \"\"\"\n",
        "    retrieval_query = (question or \"\").strip()\n",
        "    if EPIC40_QUERY_REWRITE_ENABLED and retrieval_query:\n",
        "        retrieval_query = epic40_rewrite_query_for_retrieval(\n",
        "            api_key, chat_model, retrieval_query\n",
        "        )\n",
        "    return retrieval_query\n",
        "\n",
        "\n",
        "# ============================================================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "07164c25",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07164c25",
        "outputId": "a30926d3-657d-41d9-fa40-f319cff79e1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ STEP 5.9 loaded: EPIC 4.1 eval harness helpers available.\n"
          ]
        }
      ],
      "source": [
        "# CELL 5.9 / STEP 5.9 – EPIC 4.1 Evaluation Harness (Admin/Notebook)\n",
        "\n",
        "import json\n",
        "from uuid import uuid4\n",
        "from datetime import datetime as _dt\n",
        "\n",
        "def _now_iso():\n",
        "    return _dt.utcnow().isoformat(timespec=\"seconds\") + \"Z\"\n",
        "\n",
        "def epic41_create_eval_set(name: str, description: str = \"\") -> str:\n",
        "    eval_set_id = str(uuid4())\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "    cur.execute(\n",
        "        \"INSERT INTO eval_sets (eval_set_id, name, description, created_at) VALUES (?, ?, ?, ?)\",\n",
        "        (eval_set_id, name.strip(), description.strip(), _now_iso()),\n",
        "    )\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "    return eval_set_id\n",
        "\n",
        "def epic41_add_eval_question(\n",
        "    eval_set_id: str,\n",
        "    question: str,\n",
        "    cohort_name: str | None = None,\n",
        "    provenance_mode: str = \"Explain Sources\",\n",
        "    expect_evidence: bool = True,\n",
        "    expect_refusal: bool = False,\n",
        ") -> str:\n",
        "    eval_q_id = str(uuid4())\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "    cur.execute(\n",
        "        \"\"\"INSERT INTO eval_questions\n",
        "            (eval_q_id, eval_set_id, cohort_name, question, provenance_mode, expect_evidence, expect_refusal, created_at)\n",
        "            VALUES (?, ?, ?, ?, ?, ?, ?, ?)\"\"\",\n",
        "        (\n",
        "            eval_q_id,\n",
        "            eval_set_id,\n",
        "            cohort_name,\n",
        "            question.strip(),\n",
        "            provenance_mode,\n",
        "            1 if expect_evidence else 0,\n",
        "            1 if expect_refusal else 0,\n",
        "            _now_iso(),\n",
        "        ),\n",
        "    )\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "    return eval_q_id\n",
        "\n",
        "def epic41_list_eval_sets():\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "    cur.execute(\"SELECT eval_set_id, name, description, created_at FROM eval_sets ORDER BY created_at DESC\")\n",
        "    rows = cur.fetchall()\n",
        "    conn.close()\n",
        "    return rows\n",
        "\n",
        "def epic41_list_eval_questions(eval_set_id: str):\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "    cur.execute(\n",
        "        \"\"\"SELECT eval_q_id, cohort_name, question, provenance_mode, expect_evidence, expect_refusal\n",
        "             FROM eval_questions WHERE eval_set_id=? ORDER BY created_at ASC\"\"\",\n",
        "        (eval_set_id,),\n",
        "    )\n",
        "    rows = cur.fetchall()\n",
        "    conn.close()\n",
        "    return rows\n",
        "\n",
        "def epic37_log_trace_event(\n",
        "    user_id: str | None,\n",
        "    cohort_name: str | None,\n",
        "    model_id: str | None,\n",
        "    provenance_mode: str | None,\n",
        "    memory_scope: str | None,\n",
        "    memory_used: bool,\n",
        "    rag_used: bool,\n",
        "    retrieved_k: int | None,\n",
        "    retrieval_query: str | None,\n",
        "    note: str = \"\",\n",
        "):\n",
        "    try:\n",
        "        conn = get_db_conn()\n",
        "        cur = conn.cursor()\n",
        "        cur.execute(\n",
        "            \"\"\"INSERT INTO rag_trace_events\n",
        "               (trace_id, created_at, user_id, cohort_name, model_id, provenance_mode, memory_scope, memory_used, rag_used, retrieved_k, retrieval_query, note)\n",
        "               VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\"\"\",\n",
        "            (\n",
        "                str(uuid4()),\n",
        "                _now_iso(),\n",
        "                user_id,\n",
        "                cohort_name,\n",
        "                model_id,\n",
        "                provenance_mode,\n",
        "                memory_scope,\n",
        "                1 if memory_used else 0,\n",
        "                1 if rag_used else 0,\n",
        "                retrieved_k,\n",
        "                retrieval_query,\n",
        "                note[:500],\n",
        "            ),\n",
        "        )\n",
        "        conn.commit()\n",
        "        conn.close()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "def epic41_run_eval_set(\n",
        "    eval_set_id: str,\n",
        "    api_key: str,\n",
        "    username: str,\n",
        "    default_cohort: str | None,\n",
        "    model_id: str,\n",
        "    session_mem: dict | None,\n",
        ") -> str:\n",
        "    \"\"\"Runs eval questions by calling answer_question_over_cohort (EPIC 3.4/3.5 pipeline).\n",
        "    Produces a markdown summary + stores a JSON summary row.\n",
        "    \"\"\"\n",
        "    qs = epic41_list_eval_questions(eval_set_id)\n",
        "    if not qs:\n",
        "        return \"No questions found for this eval set.\"\n",
        "\n",
        "    results = []\n",
        "    hit_retrieval = 0\n",
        "    refused_ok = 0\n",
        "    total = len(qs)\n",
        "\n",
        "    for (eval_q_id, cohort_name, q, prov_mode, expect_evidence, expect_refusal) in qs:\n",
        "        c = cohort_name or default_cohort\n",
        "        # If no cohort and strict, expect refusal\n",
        "        answer_md, why_md, foot_md = answer_question_over_cohort(\n",
        "            api_key=api_key,\n",
        "            username=username,\n",
        "            cohort_name=c or \"\",\n",
        "            question=q,\n",
        "            model_id=model_id,\n",
        "            provenance_mode=prov_mode,\n",
        "            session_mem=session_mem,\n",
        "        )\n",
        "\n",
        "        # Heuristics from Why panel\n",
        "        rag_used = (\"Retrieved\" in (why_md or \"\")) or (\"chunks\" in (why_md or \"\").lower())\n",
        "        if rag_used:\n",
        "            hit_retrieval += 1\n",
        "\n",
        "        refused = (\"could not find any context\" in (answer_md or \"\").lower()) or (\"strict evidence\" in (answer_md or \"\").lower()) or (\"no supporting\" in (answer_md or \"\").lower())\n",
        "        if bool(expect_refusal) == refused:\n",
        "            refused_ok += 1\n",
        "\n",
        "        results.append({\n",
        "            \"eval_q_id\": eval_q_id,\n",
        "            \"cohort_name\": c,\n",
        "            \"question\": q,\n",
        "            \"provenance_mode\": prov_mode,\n",
        "            \"answer_head\": (answer_md or \"\")[:200],\n",
        "            \"rag_used\": rag_used,\n",
        "            \"refused\": refused,\n",
        "            \"expect_refusal\": bool(expect_refusal),\n",
        "        })\n",
        "\n",
        "    summary = {\n",
        "        \"total\": total,\n",
        "        \"retrieval_hit_rate\": hit_retrieval / max(total, 1),\n",
        "        \"refusal_match_rate\": refused_ok / max(total, 1),\n",
        "        \"results\": results,\n",
        "    }\n",
        "\n",
        "    # persist run\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "    run_id = str(uuid4())\n",
        "    cur.execute(\n",
        "        \"INSERT INTO eval_runs (eval_run_id, eval_set_id, model_id, created_at, summary_json) VALUES (?, ?, ?, ?, ?)\",\n",
        "        (run_id, eval_set_id, model_id, _now_iso(), json.dumps(summary)),\n",
        "    )\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "    # markdown report\n",
        "    md = []\n",
        "    md.append(f\"## EPIC 4.1 Eval Run\\n\")\n",
        "    md.append(f\"- **Eval Set:** `{eval_set_id}`\\n- **Model:** `{model_id}`\\n- **Total Questions:** {total}\\n\")\n",
        "    md.append(f\"- **Retrieval hit rate (heuristic):** {summary['retrieval_hit_rate']:.2%}\\n\")\n",
        "    md.append(f\"- **Refusal correctness:** {summary['refusal_match_rate']:.2%}\\n\")\n",
        "    md.append(\"\\n---\\n\")\n",
        "    for r in results:\n",
        "        md.append(f\"### Q: {r['question']}\")\n",
        "        md.append(f\"- Cohort: `{r['cohort_name']}` | Provenance: `{r['provenance_mode']}`\")\n",
        "        md.append(f\"- RAG used: `{r['rag_used']}` | Refused: `{r['refused']}` (expected `{r['expect_refusal']}`)\")\n",
        "        md.append(f\"- Answer head: {r['answer_head']}\")\n",
        "        md.append(\"\")\n",
        "    return \"\\n\".join(md)\n",
        "\n",
        "print(\"✅ STEP 5.9 loaded: EPIC 4.1 eval harness helpers available.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f052493",
      "metadata": {
        "id": "6f052493"
      },
      "source": [
        "## CELL 6 / STEP 6 – Build Cohort from Uploaded Docs (v16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "e7ae74e2",
      "metadata": {
        "id": "e7ae74e2"
      },
      "outputs": [],
      "source": [
        "# CELL 6 / STEP 6 – Build Cohort from Uploaded Docs (v16)\n",
        "def build_cohort_from_files(\n",
        "    api_key: str,\n",
        "    embed_model: str,\n",
        "    cohort_name: str,\n",
        "    files: List[Any],\n",
        "    overwrite_existing: bool = False,\n",
        ") -> str:\n",
        "    \"\"\"Ingest a list of uploaded files into a *single* cohort.\n",
        "\n",
        "    For each file we:\n",
        "      - load the text\n",
        "      - chunk it\n",
        "      - embed with the given embedding model\n",
        "      - build & save a FAISS index\n",
        "      - save metadata (including the chunks)\n",
        "      - register the document in the SQLite `documents` table\n",
        "\n",
        "    Returns a human-readable summary string for the UI.\n",
        "    \"\"\"\n",
        "    ensure_docs_table()\n",
        "    ensure_cohort_table()\n",
        "    os.makedirs(INDEX_DIR, exist_ok=True)\n",
        "\n",
        "    success_count = 0\n",
        "    total_chunks = 0\n",
        "    messages: List[str] = []\n",
        "\n",
        "    for file_obj in files:\n",
        "        try:\n",
        "            # 1) Load text & derive a stable doc_name\n",
        "            text, doc_name = load_file_to_text(file_obj)\n",
        "            # Optional overwrite-by-filename (MVP): if a doc with the same name exists in this cohort,\n",
        "            # remove prior DB rows + on-disk index artifacts, then rebuild this doc's index.\n",
        "            if overwrite_existing:\n",
        "                try:\n",
        "                    removed = overwrite_document_in_cohort(cohort_name=cohort_name, doc_name=doc_name)\n",
        "                    if removed:\n",
        "                        messages.append(f\"ℹ️ {doc_name}: existing document overwritten; rebuilding index.\")\n",
        "                except Exception as e:\n",
        "                    trace_log(f\"overwrite_document_in_cohort WARNING cohort={cohort_name} doc={doc_name}: {e}\")\n",
        "\n",
        "            if not text or not text.strip():\n",
        "                messages.append(f\"⚠️ {doc_name}: no extractable text, skipped.\")\n",
        "                continue\n",
        "\n",
        "            # 2) Chunk\n",
        "            chunks = chunk_text(text)\n",
        "            if not chunks:\n",
        "                messages.append(f\"⚠️ {doc_name}: produced 0 chunks, skipped.\")\n",
        "                continue\n",
        "\n",
        "            # 3) Embed\n",
        "            vectors = embed_texts(\n",
        "                api_key=api_key,\n",
        "                embed_model=embed_model,\n",
        "                texts=chunks,\n",
        "            )\n",
        "\n",
        "            # 4) Build FAISS index\n",
        "            index = build_faiss_index(vectors)\n",
        "\n",
        "            # 5) Save index & metadata\n",
        "            index_id = str(uuid4())\n",
        "            save_index(index, index_id)\n",
        "\n",
        "            meta = {\n",
        "                \"cohort_name\": cohort_name,\n",
        "                \"doc_name\": doc_name,\n",
        "                \"chunks\": chunks,\n",
        "                \"embed_model\": embed_model,\n",
        "            }\n",
        "            save_metadata(index_id, meta)\n",
        "\n",
        "            # 6) Register in SQLite\n",
        "            register_document(\n",
        "                doc_name=doc_name,\n",
        "                cohort_name=cohort_name,\n",
        "                index_id=index_id,\n",
        "                n_chunks=len(chunks),\n",
        "                embed_model=embed_model,\n",
        "            )\n",
        "\n",
        "            success_count += 1\n",
        "            total_chunks += len(chunks)\n",
        "            messages.append(f\"✅ {doc_name}: {len(chunks)} chunks embedded.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            # Best-effort error capture per-file\n",
        "            name = getattr(file_obj, \"name\", str(file_obj))\n",
        "            messages.append(f\"❌ {name}: {e}\")\n",
        "\n",
        "    if success_count == 0:\n",
        "        detail = \"\\n\".join(messages) if messages else \"\"\n",
        "        return \"❌ No documents were successfully processed.\" + (f\"\\n{detail}\" if detail else \"\")\n",
        "\n",
        "    summary = (\n",
        "        f\"✅ Built cohort '{cohort_name}' with {success_count} document(s) \"\n",
        "        f\"and {total_chunks} total chunks.\"\n",
        "    )\n",
        "    if messages:\n",
        "        summary += \"\\n\" + \"\\n\".join(messages)\n",
        "    return summary\n",
        "\n",
        "\n",
        "def build_cohort_index(\n",
        "    api_key: str,\n",
        "    cohort_name: str,\n",
        "    files: List[Any],\n",
        "    owner: Optional[str] = None,\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    High-level helper used by the Gradio UI (v16):\n",
        "\n",
        "      - Selects default embedding model (from model registry)\n",
        "      - Builds the cohort\n",
        "      - Stores cohort ownership\n",
        "    \"\"\"\n",
        "    if not api_key or not api_key.strip():\n",
        "        return \"❌ OpenAI API key is required.\"\n",
        "\n",
        "    if not cohort_name or not cohort_name.strip():\n",
        "        return \"❌ Cohort name is required.\"\n",
        "\n",
        "    if (not owner) or (not str(owner).strip()) or (str(owner).strip().lower() in (\"anonymous\",\"none\",\"null\")):\n",
        "        return \"❌ You must be logged in to create a cohort (owner is required).\"\n",
        "\n",
        "    if not files:\n",
        "        return \"❌ Please upload at least one file.\"\n",
        "\n",
        "    # ✔️ NEW — correct default embedding model call\n",
        "    embed_cfg = get_default_embed_model_config()\n",
        "    embed_model = embed_cfg.model_id\n",
        "\n",
        "    # Build FAISS index + metadata\n",
        "    result_msg = build_cohort_from_files(\n",
        "        api_key=api_key,\n",
        "        embed_model=embed_model,\n",
        "        cohort_name=cohort_name.strip(),\n",
        "        files=files,\n",
        "    )\n",
        "\n",
        "    # Save cohort owner metadata (required)\n",
        "\n",
        "\n",
        "    # Save cohort owner metadata (required)\n",
        "    try:\n",
        "        role = USERS.get(owner, {}).get(\"role\", \"user\")\n",
        "        user_obj = SessionUser(username=str(owner).strip(), role=role)\n",
        "        set_cohort_owner(cohort_name.strip(), user_obj)\n",
        "    except Exception as e:\n",
        "        trace_log(f\"build_cohort_index set_cohort_owner ERROR: {e}\")\n",
        "        return f\"❌ Failed to set cohort owner (cohort will not be treated as valid): {e}\"\n",
        "\n",
        "    return result_msg\n",
        "\n",
        "# ============================================================"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a09f9db9",
      "metadata": {
        "id": "a09f9db9"
      },
      "source": [
        "## CELL 6.1 / STEP 6.1 – Routing Brain (v16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "38ab6e55",
      "metadata": {
        "id": "38ab6e55"
      },
      "outputs": [],
      "source": [
        "def call_chat_model(*args, **kwargs):\n",
        "    \"\"\"\n",
        "    EPIC 5.2 — Unified model call used by the app's routing interface.\n",
        "\n",
        "    Backward-compatible with v18 call sites:\n",
        "      call_chat_model(api_key=..., messages=..., task_type=..., user_pref=...)\n",
        "\n",
        "    Returns:\n",
        "      (answer_text, raw_answer_text, model_used)\n",
        "\n",
        "    Model constraints handled:\n",
        "      - GPT-5 family uses max_completion_tokens (not max_tokens)\n",
        "      - gpt-5-mini does NOT support custom temperature values; omit temperature entirely\n",
        "    \"\"\"\n",
        "\n",
        "    api_key = kwargs.get(\"api_key\")\n",
        "    messages = kwargs.get(\"messages\")\n",
        "    task_type = (kwargs.get(\"task_type\") or \"rag_answer\").strip()\n",
        "    user_pref = kwargs.get(\"user_pref\") or kwargs.get(\"model_name\") or kwargs.get(\"model\") or kwargs.get(\"chat_model\")\n",
        "\n",
        "    if not api_key:\n",
        "        raise ValueError(\"Missing api_key for call_chat_model()\")\n",
        "    if not messages:\n",
        "        raise ValueError(\"Missing messages for call_chat_model()\")\n",
        "\n",
        "    client = build_openai_client(api_key)\n",
        "\n",
        "    # Choose model\n",
        "    try:\n",
        "        if user_pref and isinstance(user_pref, str) and user_pref.strip():\n",
        "            model_used = user_pref.strip()\n",
        "        else:\n",
        "            model_used = get_default_model_name(\"chat\")\n",
        "            if not model_used:\n",
        "                enabled = list_chat_models()\n",
        "                model_used = enabled[0] if enabled else None\n",
        "        if not model_used:\n",
        "            model_used = \"gpt-4.1-mini\"\n",
        "    except Exception:\n",
        "        model_used = \"gpt-4.1-mini\"\n",
        "\n",
        "    model_used = str(model_used).strip()\n",
        "    is_gpt5 = model_used.startswith(\"gpt-5\")\n",
        "    is_gpt5_mini = (model_used == \"gpt-5-mini\")\n",
        "\n",
        "    # Defaults (caller may override)\n",
        "    temperature = kwargs.get(\"temperature\", 0.6 if is_gpt5 else 0.3)\n",
        "\n",
        "    token_budget = kwargs.get(\"max_completion_tokens\") or kwargs.get(\"max_tokens\")\n",
        "    if not isinstance(token_budget, int) or token_budget <= 0:\n",
        "        token_budget = 3000 if is_gpt5 else 1200\n",
        "\n",
        "    def _do_call(omit_temperature: bool = False):\n",
        "        if is_gpt5:\n",
        "            params = dict(\n",
        "                model=model_used,\n",
        "                messages=messages,\n",
        "                max_completion_tokens=token_budget,\n",
        "            )\n",
        "            # gpt-5-mini: omit temperature entirely\n",
        "            if (not omit_temperature) and (not is_gpt5_mini):\n",
        "                params[\"temperature\"] = temperature\n",
        "            return client.chat.completions.create(**params)\n",
        "        else:\n",
        "            params = dict(\n",
        "                model=model_used,\n",
        "                messages=messages,\n",
        "                max_tokens=token_budget,\n",
        "            )\n",
        "            if not omit_temperature:\n",
        "                params[\"temperature\"] = temperature\n",
        "            return client.chat.completions.create(**params)\n",
        "\n",
        "    try:\n",
        "        resp = _do_call(omit_temperature=is_gpt5_mini)\n",
        "        answer_text = (resp.choices[0].message.content or \"\").strip()\n",
        "        return answer_text, answer_text, model_used\n",
        "\n",
        "    except Exception as e:\n",
        "        msg = str(e)\n",
        "\n",
        "        # If temperature is rejected (parameter or value), retry once without temperature.\n",
        "        if \"temperature\" in msg and (\"Unsupported\" in msg or \"unsupported\" in msg):\n",
        "            resp = _do_call(omit_temperature=True)\n",
        "            answer_text = (resp.choices[0].message.content or \"\").strip()\n",
        "            return answer_text, answer_text, model_used\n",
        "\n",
        "        raise\n",
        "\n",
        "\n",
        "def select_chat_model(*args, **kwargs):\n",
        "    \"\"\"\n",
        "    Backward-compatible helper.\n",
        "\n",
        "    Supports two legacy patterns:\n",
        "    1) select_chat_model() -> returns a default model name (string)\n",
        "    2) select_chat_model(client, model_name, messages, **kwargs) -> executes the model call\n",
        "    \"\"\"\n",
        "    # Pattern 1: called with no args during validation or UI init\n",
        "    if len(args) == 0 and not {\"client\", \"model_name\", \"messages\"}.issubset(set(kwargs.keys())):\n",
        "        try:\n",
        "            dm = get_default_model_name(\"chat\")\n",
        "            if dm:\n",
        "                return dm\n",
        "        except Exception:\n",
        "            pass\n",
        "        try:\n",
        "            models = list_chat_models()\n",
        "            if models:\n",
        "                return models[0]\n",
        "        except Exception:\n",
        "            pass\n",
        "        return \"gpt-4o-mini\"\n",
        "\n",
        "    # Pattern 2: called with explicit client/model/messages\n",
        "    if len(args) >= 3:\n",
        "        client, model_name, messages = args[0], args[1], args[2]\n",
        "        return call_chat_model(client, model_name, messages)\n",
        "\n",
        "    if {\"client\", \"model_name\", \"messages\"}.issubset(set(kwargs.keys())):\n",
        "        return call_chat_model(kwargs[\"client\"], kwargs[\"model_name\"], kwargs[\"messages\"])\n",
        "\n",
        "    raise TypeError(\"select_chat_model called with unsupported arguments\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "798e66b0",
      "metadata": {
        "id": "798e66b0"
      },
      "source": [
        "## CELL 7 / STEP 7 – Admin Helpers (Stats & Maintenance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "95658564",
      "metadata": {
        "id": "95658564"
      },
      "outputs": [],
      "source": [
        "# CELL 7 / STEP 7 – Admin Helpers (Stats & Maintenance)\n",
        "\n",
        "def get_db_stats() -> str:\n",
        "    ensure_docs_table()\n",
        "    ensure_cohort_table()\n",
        "    ensure_user_table()\n",
        "    ensure_chat_history_table()\n",
        "\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "\n",
        "    cur.execute(\"SELECT COUNT(*) FROM documents\")\n",
        "    n_docs = cur.fetchone()[0]\n",
        "\n",
        "    cur.execute(\"SELECT COUNT(DISTINCT cohort_name) FROM cohort_docs\")\n",
        "    n_cohorts = cur.fetchone()[0]\n",
        "\n",
        "    cur.execute(\"SELECT COUNT(*) FROM users\")\n",
        "    n_users = cur.fetchone()[0]\n",
        "\n",
        "    cur.execute(\"SELECT COUNT(*) FROM chat_history\")\n",
        "    n_chats = cur.fetchone()[0]\n",
        "\n",
        "    conn.close()\n",
        "\n",
        "    return (\n",
        "        f\"**DB Stats**\\n\\n\"\n",
        "        f\"- Documents: {n_docs}\\n\"\n",
        "        f\"- Cohorts: {n_cohorts}\\n\"\n",
        "        f\"- Users: {n_users}\\n\"\n",
        "        f\"- Chat records (last 7 days enforced on write): {n_chats}\\n\"\n",
        "    )\n",
        "\n",
        "def describe_users() -> str:\n",
        "    rows = list_users()\n",
        "    if not rows:\n",
        "        return \"No users have been recorded yet.\"\n",
        "    lines = [\"**Known Users**\\n\"]\n",
        "    for user_id, display_name, role in rows:\n",
        "        disp = display_name or \"(no display name)\"\n",
        "        r = role or \"(no role)\"\n",
        "        lines.append(f\"- `{user_id}` – {disp} – role: `{r}`\")\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "def describe_cohorts() -> str:\n",
        "    \"\"\"\n",
        "    Returns a markdown summary of cohorts, including:\n",
        "      - name\n",
        "      - owner\n",
        "      - visibility (private/shared)\n",
        "      - document count\n",
        "    \"\"\"\n",
        "    ensure_docs_table()\n",
        "    ensure_cohort_table()\n",
        "    ensure_cohort_meta_table()\n",
        "\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "    cur.execute(\n",
        "        \"\"\"\n",
        "        SELECT DISTINCT\n",
        "            cd.cohort_name,\n",
        "            COALESCE(cm.owner_user_id, '(none)') AS owner,\n",
        "            COALESCE(cm.is_shared, 0)            AS is_shared,\n",
        "            COUNT(DISTINCT cd.doc_name)          AS num_docs\n",
        "        FROM cohort_docs cd\n",
        "        LEFT JOIN cohort_meta cm\n",
        "          ON cd.cohort_name = cm.cohort_name\n",
        "        GROUP BY cd.cohort_name, owner, is_shared\n",
        "        ORDER BY cd.cohort_name\n",
        "        \"\"\"\n",
        "    )\n",
        "    rows = cur.fetchall()\n",
        "    conn.close()\n",
        "\n",
        "    if not rows:\n",
        "        return \"No cohorts found.\"\n",
        "\n",
        "    lines = [\"**Cohorts**\", \"\"]\n",
        "    for name, owner, is_shared, num_docs in rows:\n",
        "        share_label = \"shared\" if is_shared else \"private\"\n",
        "        lines.append(\n",
        "            f\"- **{name}** — owner: `{owner}`, visibility: {share_label}, docs: {num_docs}\"\n",
        "        )\n",
        "\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "def list_demo_prompts_with_notes(cohort_name: str):\n",
        "    ensure_demo_prompts_table()\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "    cur.execute(\"\"\"\n",
        "        SELECT id, prompt_text, admin_note\n",
        "        FROM demo_prompts\n",
        "        WHERE cohort_name = ?\n",
        "        ORDER BY COALESCE(display_order, sort_order, 0) ASC, id ASC\n",
        "    \"\"\", ((cohort_name or \"\").strip(),))\n",
        "    rows = cur.fetchall()\n",
        "    conn.close()\n",
        "    return rows\n",
        "\n",
        "\n",
        "def save_demo_prompt_note(prompt_id: int | str, note: str):\n",
        "    ensure_demo_prompts_table()\n",
        "    pid = int(str(prompt_id).strip())  # robust for gradio dropdown strings\n",
        "    conn = get_db_conn()\n",
        "    cur = conn.cursor()\n",
        "    cur.execute(\"\"\"\n",
        "        UPDATE demo_prompts\n",
        "        SET admin_note = ?\n",
        "        WHERE id = ?\n",
        "    \"\"\", ((note or \"\").strip(), pid))\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "#====================================================\n",
        "# Add a formatter for the “Why This Answer” panel EPIC 2.1\n",
        "\n",
        "#====================================================\n",
        "def format_why_this_answer_md(trace: dict | None) -> str:\n",
        "    \"\"\"\n",
        "    Render a provenance/trace dict into a compact, human-readable markdown block.\n",
        "    Expected keys (best-effort):\n",
        "      - chat_model\n",
        "      - cohort_name\n",
        "      - top_k\n",
        "      - retrieved: list[ {doc_name, score, chunk_preview} ]\n",
        "    \"\"\"\n",
        "    if not trace:\n",
        "        return \"_No provenance captured for this answer._\"\n",
        "\n",
        "    chat_model = trace.get(\"chat_model\") or \"(unknown)\"\n",
        "    cohort_name = trace.get(\"cohort_name\") or \"(none)\"\n",
        "    top_k = trace.get(\"top_k\") or \"\"\n",
        "    retrieved = trace.get(\"retrieved\") or []\n",
        "\n",
        "    lines = []\n",
        "    lines.append(f\"**Model:** `{chat_model}`\")\n",
        "    lines.append(f\"**Cohort:** `{cohort_name}`\")\n",
        "    if top_k:\n",
        "        lines.append(f\"**Top-K chunks:** `{top_k}`\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    if not retrieved:\n",
        "        lines.append(\"_No retrieval chunks were returned._\")\n",
        "        return \"\\n\".join(lines)\n",
        "\n",
        "    lines.append(\"### Retrieved evidence\")\n",
        "    for i, r in enumerate(retrieved, start=1):\n",
        "        doc = r.get(\"doc_name\") or \"(unknown doc)\"\n",
        "        score = r.get(\"score\")\n",
        "        preview = (r.get(\"chunk_preview\") or \"\").strip()\n",
        "\n",
        "        score_str = f\"{score:.4f}\" if isinstance(score, (int, float)) else (str(score) if score is not None else \"n/a\")\n",
        "        lines.append(f\"**{i}. {doc}**  \\nScore: `{score_str}`\")\n",
        "        if preview:\n",
        "            # Keep previews short; this is a “why” panel, not a data dump.\n",
        "            lines.append(f\"> {preview[:400]}{'…' if len(preview) > 400 else ''}\")\n",
        "        lines.append(\"\")\n",
        "\n",
        "    return \"\\n\".join(lines).strip()\n",
        "\n",
        "# ============================================================"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d0dcb3a",
      "metadata": {
        "id": "0d0dcb3a"
      },
      "source": [
        "## CELL 8 / STEP 8 – Prompt Coach (Optional Query Improvement) – v16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "5a956726",
      "metadata": {
        "id": "5a956726"
      },
      "outputs": [],
      "source": [
        "# CELL 8 / STEP 8 – Prompt Coach (Optional Query Improvement) – v16\n",
        "#\n",
        "# Uses the v16 Routing Brain (call_chat_model) instead of calling OpenAI directly.\n",
        "# Signature is kept the same so STEP 10's on_improve_query(...) still works:\n",
        "#     improve_query(api_key, chat_model, original_query)\n",
        "#\n",
        "# In a future step, we can optionally add a user-selected model override and\n",
        "# pass it into call_chat_model(user_pref=...).\n",
        "\n",
        "def improve_query(\n",
        "    api_key: str,\n",
        "    chat_model: str,       # kept for backward compatibility; routing ignores it\n",
        "    original_query: str,\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Prompt coach to re-write the user's query for better RAG retrieval.\n",
        "\n",
        "    Behavior:\n",
        "    - If the original query is empty/whitespace, returns \"\".\n",
        "    - Otherwise, uses the Routing Brain (task_type='question_improve') to pick\n",
        "      an appropriate chat model and rewrite the question to be clearer, more\n",
        "      explicit, and RAG-friendly.\n",
        "    \"\"\"\n",
        "    if not original_query.strip():\n",
        "        return \"\"\n",
        "\n",
        "    system_prompt = (\n",
        "        \"You are a prompt coach helping the user improve questions for a RAG system. \"\n",
        "        \"Rewrite the query to be explicit, concise, and focused on key details for retrieval. \"\n",
        "        \"Preserve the user's intent AND preserve all explicit user constraints verbatim \"\n",
        "        '(quantity limits, formatting requirements, \"only\", \"exactly\", \"do not\", etc.). '\n",
        "        \"Do not broaden scope, and do not add new requests. \"\n",
        "        \"If the user requested a specific number of items (e.g., 1), include that number explicitly. \"\n",
        "        \"Return ONLY the improved query text.\"\n",
        "    )\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": original_query},\n",
        "    ]\n",
        "\n",
        "    # Use the v16 Routing Brain instead of calling OpenAI directly\n",
        "    improved, _, model_used = call_chat_model(\n",
        "        api_key=api_key,\n",
        "        messages=messages,\n",
        "        task_type=\"question_improve\",\n",
        "        user_pref=None,                     # (optional override will come from UI later)\n",
        "        context_size=len(original_query),   # small, but available for routing heuristics\n",
        "    )\n",
        "\n",
        "    # For now we just return the improved text. If desired later, we can:\n",
        "    # - log model_used to audit_log\n",
        "    # - display which model did the improvement in the UI.\n",
        "    return improved.strip()\n",
        "\n",
        "# ============================================================"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6e1d515",
      "metadata": {
        "id": "e6e1d515"
      },
      "source": [
        "## CELL 9 / STEP 9 – Chat History Viewer (User-Facing)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "228fa96b",
      "metadata": {
        "id": "228fa96b"
      },
      "source": [
        "## STEP 9 — Core viewer, auth, admin wrappers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "4fe3c65b",
      "metadata": {
        "id": "4fe3c65b"
      },
      "outputs": [],
      "source": [
        "# CELL 9 / STEP 9 – Chat History Viewer (User-Facing)\n",
        "\n",
        "ABOUT_TOOL_MD = \"### About This Tool\\n\\n#### What this tool is\\n- A cohort-based Q&A assistant over your uploaded documents.\\n- It retrieves relevant passages and generates answers grounded in those passages.\\n- Intended for demos, internal reviews, and rapid prototyping of RAG workflows.\\n\\n#### What this tool is not\\n- It is not authoritative beyond the documents you upload.\\n- It may miss information if it is not present in the cohort (or retrieval is weak).\\n- It should not be used as the sole basis for high-stakes decisions.\\n\\n---\\n\\n### How to Create and Use Cohorts\\n\\n#### What a cohort is\\nA cohort is a named collection of documents that are indexed together for retrieval. When you ask a question, the tool searches within the selected cohort and cites the strongest matching passages.\\n\\n#### When to use each action\\n- **New Cohort**: Use when you are starting a new topic area or a new demo dataset.\\n- **Append Files**: Use when you want to add documents to an existing cohort (e.g., new requirements, updated guidance).\\n- **Clone Cohort**: Use when you want a copy of an existing cohort as a starting point (e.g., create a variant for a different audience or demo).\\n\\n#### Suggested naming conventions\\n- Use clear, stable names: `WIC_Online_Shopping_BRD`, `WIC_TIG_Guidance_2024`, `SNAP_Fraud_Prevention`\\n- Avoid dates in the cohort name unless the cohort is time-bound; put dates in document names instead.\\n\\n---\\n\\n### How to Get Better Answers\\n- Ask specific questions that include key terms used in the documents.\\n- If the tool returns \\u201cno context,\\u201d try rephrasing with document terminology.\\n- Use **Improve Question** to turn a rough question into a more retrieval-friendly version.\\n- Use provenance modes (if enabled) when you need strict traceability to the documents.\\n\\n---\\n\\n### Coming Next\\n- A guided walkthrough experience and, eventually, an avatar-based onboarding mode.\"\n",
        "\n",
        "def format_history_markdown(\n",
        "    user_id: Optional[str],\n",
        "    cohort_name: Optional[str],\n",
        "    limit: int = 50,\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Turn recent history into markdown for display.\n",
        "    \"\"\"\n",
        "    hist = get_recent_history(user_id=user_id, cohort_name=cohort_name, limit=limit)\n",
        "    if not hist:\n",
        "        return \"No chat history found for the given filters (within retention window).\"\n",
        "\n",
        "    lines = []\n",
        "    lines.append(\n",
        "        f\"**Showing up to {limit} most recent interactions** \"\n",
        "        f\"{'(filtered)' if user_id or cohort_name else ''}\\n\"\n",
        "    )\n",
        "\n",
        "    for h in hist:\n",
        "        ts = h[\"created_at\"]\n",
        "        u = h[\"user_id\"] or \"(anonymous)\"\n",
        "        r = h[\"role\"] or \"(none)\"\n",
        "        c = h[\"cohort_name\"] or \"(none)\"\n",
        "        which = h[\"which_prompt\"] or \"(unknown)\"\n",
        "\n",
        "        lines.append(f\"---\\n**User:** `{u}`  |  **Role:** `{r}`  |  **Cohort:** `{c}`  |  **When:** {ts}\")\n",
        "        lines.append(f\"**Prompt used:** `{which}`\")\n",
        "        lines.append(f\"**Original query:**\\n{h['original_query']}\\n\")\n",
        "        if h[\"improved_query\"]:\n",
        "            lines.append(f\"**Improved query:**\\n{h['improved_query']}\\n\")\n",
        "        lines.append(\"**Answer:**\")\n",
        "        lines.append(h[\"answer\"])\n",
        "        lines.append(\"\")\n",
        "\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "#============================================================\n",
        "# NOTE: If RUN_PREFLIGHT_ONLY=True, do not launch Gradio\n",
        "\n",
        "#============================================================\n",
        "if globals().get(\"STOP_AFTER_PREFLIGHT\", False):\n",
        "    print(\"✅ Pre-flight only: skipping Gradio launch.\")\n",
        "else:\n",
        "    # CELL 9.5 / STEP 9.5 – Identity & Admin Ops (v14)\n",
        "    # ============================================================\n",
        "    def authenticate(username: str, password: str) -> SessionUser | None:\n",
        "        \"\"\"\n",
        "        MVP auth: checks against local USERS dict.\n",
        "        Returns SessionUser or None if invalid.\n",
        "        \"\"\"\n",
        "        record = USERS.get(username)\n",
        "        if not record:\n",
        "            return None\n",
        "        if password != record[\"password\"]:\n",
        "            return None\n",
        "        return SessionUser(username=username, role=record[\"role\"])\n",
        "\n",
        "\n",
        "    def authenticate_credentials(username: str, password: str):\n",
        "        \"\"\"\n",
        "        Wrapper used by the Gradio login logic in STEP 10.\n",
        "\n",
        "        It calls authenticate(...) which returns a SessionUser, then:\n",
        "          - Upserts the user into the `users` table (for admin/history views)\n",
        "          - Returns a simple dict {username, role} that the UI expects.\n",
        "        \"\"\"\n",
        "\n",
        "        # Use the existing MVP auth\n",
        "        user = authenticate(username, password)\n",
        "        if not user:\n",
        "            return None\n",
        "\n",
        "        # Make sure the user exists in the DB's `users` table\n",
        "        try:\n",
        "            upsert_user(\n",
        "                user_id=user.username,\n",
        "                role=user.role,\n",
        "                display_name=user.username,\n",
        "            )\n",
        "        except Exception as e:\n",
        "            # Don't break login if DB write fails; just log it\n",
        "            trace_log(f\"authenticate_credentials upsert_user ERROR: {e}\")\n",
        "\n",
        "        # UI login code in STEP 10 expects a dict-like object\n",
        "        return {\"username\": user.username, \"role\": user.role}\n",
        "\n",
        "\n",
        "\n",
        "    def require_admin(user: SessionUser):\n",
        "        \"\"\"\n",
        "        Helper for admin-only actions. Raises PermissionError if not admin.\n",
        "        \"\"\"\n",
        "        if not user or not user.is_admin:\n",
        "            raise PermissionError(\"Admin privileges required for this action.\")\n",
        "\n",
        "\n",
        "    def admin_delete_cohort(user: SessionUser, cohort_name: str) -> str:\n",
        "        \"\"\"\n",
        "        Admin-only wrapper around delete_cohort().\n",
        "        Uses the existing delete_cohort function from STEP 4.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            require_admin(user)\n",
        "        except PermissionError as e:\n",
        "            return f\"❌ Not authorized: {e}\"\n",
        "\n",
        "        if not cohort_name:\n",
        "            return \"❌ Please select a cohort to delete.\"\n",
        "\n",
        "        try:\n",
        "            # Use existing v13 delete_cohort logic (no reassignment in this MVP).\n",
        "            msg = delete_cohort(cohort_name, reassign_to=None)\n",
        "            log_audit(user.username, user.role, \"delete_cohort\", f\"cohort={cohort_name}\")\n",
        "            return msg\n",
        "        except Exception as e:\n",
        "            return f\"❌ Error deleting cohort: {e}\"\n",
        "\n",
        "    def admin_view_audit_log(user: SessionUser, limit: int = 50) -> str:\n",
        "        \"\"\"\n",
        "        Admin-only view of recent audit log entries.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            require_admin(user)\n",
        "        except PermissionError as e:\n",
        "            return f\"❌ Not authorized: {e}\"\n",
        "\n",
        "        ensure_audit_table()\n",
        "        conn = get_db_conn()\n",
        "        cur = conn.cursor()\n",
        "        cur.execute(\n",
        "            \"\"\"\n",
        "            SELECT ts, username, role, action, details\n",
        "            FROM audit_log\n",
        "            ORDER BY id DESC\n",
        "            LIMIT ?\n",
        "            \"\"\",\n",
        "            (limit,),\n",
        "        )\n",
        "        rows = cur.fetchall()\n",
        "        conn.close()\n",
        "\n",
        "        if not rows:\n",
        "            return \"No audit log entries.\"\n",
        "\n",
        "        lines = [\"**Recent Audit Log Entries**\\n\"]\n",
        "        for ts, username, role, action, details in rows:\n",
        "            u = username or \"-\"\n",
        "            r = role or \"-\"\n",
        "            d = details or \"\"\n",
        "            lines.append(f\"- {ts} | user=`{u}` | role=`{r}` | action=`{action}` | {d}\")\n",
        "\n",
        "        return \"\\n\".join(lines)\n",
        "\n",
        "    # ============================================================\n",
        "    # STEP 10 — Gradio App, Tabs & Startup (v16_1 with cohort sharing)\n",
        "    # ============================================================\n",
        "\n",
        "    # -----------------------------\n",
        "    # Setup & Cohorts Tab\n",
        "    # -----------------------------\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb8476fd",
      "metadata": {
        "id": "fb8476fd"
      },
      "source": [
        "## STEP 10 — Setup & Cohorts tab\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "91f4a831",
      "metadata": {
        "id": "91f4a831"
      },
      "outputs": [],
      "source": [
        "def build_setup_tab(session_state, api_key_state):\n",
        "        \"\"\"\n",
        "        Setup tab (v16_1 Phase 1):\n",
        "          - Create a new cohort from uploaded files (authenticated users only)\n",
        "          - Append files to an existing cohort (owner/admin only)\n",
        "          - Clone a shared cohort when (is_shared=1 AND allow_clone=1)\n",
        "\n",
        "        IMPORTANT POLICY:\n",
        "          - No access is allowed unless the user is logged in.\n",
        "          - Cohorts must always have a valid owner; 'anonymous' is never permitted.\n",
        "        \"\"\"\n",
        "        gr.Markdown(\"### Build or Manage a Cohort\")\n",
        "\n",
        "        def _require_session_user(session_user, action: str) -> SessionUser:\n",
        "            # require_login() is defined earlier in the codebase; this wrapper keeps UI callbacks tidy.\n",
        "            return require_login(session_user, action)\n",
        "\n",
        "        def _is_admin(su: SessionUser) -> bool:\n",
        "            return bool(su and su.is_admin)\n",
        "\n",
        "        def _get_owner(cohort_name: str) -> str | None:\n",
        "            ensure_cohort_meta_table()\n",
        "            conn = get_db_conn()\n",
        "            cur = conn.cursor()\n",
        "            cur.execute(\"SELECT owner_user_id FROM cohort_meta WHERE cohort_name = ? LIMIT 1\", (cohort_name,))\n",
        "            row = cur.fetchone()\n",
        "            conn.close()\n",
        "            return row[0] if row else None\n",
        "\n",
        "        def _can_write(session_user, cohort_name: str) -> bool:\n",
        "            try:\n",
        "                su = _require_session_user(session_user, \"modify cohorts\")\n",
        "            except PermissionError:\n",
        "                return False\n",
        "            if _is_admin(su):\n",
        "                return True\n",
        "            owner = _get_owner(cohort_name)\n",
        "            return bool(owner and su.username and owner == su.username)\n",
        "\n",
        "        def _list_cloneable_shared_cohorts() -> list[str]:\n",
        "            \"\"\"\n",
        "            Cloneable cohorts = is_shared=1 AND allow_clone=1.\n",
        "            We join through `documents` to ensure the cohort has content.\n",
        "            \"\"\"\n",
        "            ensure_cohort_meta_table()\n",
        "            ensure_docs_table()\n",
        "            conn = get_db_conn()\n",
        "            cur = conn.cursor()\n",
        "            cur.execute(\n",
        "                \"\"\"\n",
        "                SELECT DISTINCT cm.cohort_name\n",
        "                FROM cohort_meta cm\n",
        "                JOIN documents d\n",
        "                  ON d.cohort_name = cm.cohort_name\n",
        "                WHERE cm.is_shared = 1\n",
        "                  AND COALESCE(cm.allow_clone,0) = 1\n",
        "                  AND d.cohort_name IS NOT NULL\n",
        "                  AND TRIM(d.cohort_name) <> ''\n",
        "                ORDER BY cm.cohort_name\n",
        "                \"\"\"\n",
        "            )\n",
        "            rows = cur.fetchall()\n",
        "            conn.close()\n",
        "            return [r[0] for r in rows]\n",
        "\n",
        "        # ---------- UI ----------\n",
        "        with gr.Row():\n",
        "            action_radio = gr.Radio(\n",
        "                choices=[ACTION_CREATE, ACTION_APPEND, ACTION_CLONE],\n",
        "                value=DEFAULT_ACTION,\n",
        "                label=\"Action\",\n",
        "            )\n",
        "\n",
        "        clone_info_md = gr.Markdown(\"\", visible=False)\n",
        "\n",
        "        with gr.Row():\n",
        "            new_cohort_name = gr.Textbox(label=\"New Cohort Name\", placeholder=\"e.g., WIC_Security_2025\")\n",
        "            existing_cohort_dropdown = gr.Dropdown(\n",
        "                label=\"Existing Cohort\",\n",
        "                choices=[],\n",
        "                interactive=True,\n",
        "                visible=False,\n",
        "            )\n",
        "\n",
        "        with gr.Row():\n",
        "            share_checkbox = gr.Checkbox(label=\"Shared (visible to all logged-in users)\", value=False)\n",
        "            allow_clone_checkbox = gr.Checkbox(label=\"Allow other users to clone this cohort\", value=False)\n",
        "\n",
        "        # EPIC 5.1 / 5.2 — Cohort Description + Intended Audience (Admin edit)\n",
        "        with gr.Accordion(\"Cohort Description & Intended Audience\", open=False):\n",
        "            cohort_desc_tb = gr.Textbox(label=\"Cohort Description\", lines=4, visible=False)\n",
        "            cohort_aud_tb  = gr.Textbox(label=\"Intended Audience\", lines=2, visible=False)\n",
        "            save_meta_btn  = gr.Button(\"Save Cohort Metadata\", variant=\"primary\", visible=False)\n",
        "            meta_status_md = gr.Markdown(\"\", visible=False)\n",
        "\n",
        "\n",
        "        file_uploader = gr.File(\n",
        "            label=\"Upload Documents (PDF, DOCX, TXT)\",\n",
        "            file_count=\"multiple\",\n",
        "            type=\"filepath\",\n",
        "        )\n",
        "\n",
        "        overwrite_confirm = gr.Checkbox(\n",
        "            label=\"If uploaded files include names that already exist in the cohort, OVERWRITE the existing documents and rebuild the cohort index\",\n",
        "            value=False,\n",
        "        )\n",
        "\n",
        "        with gr.Row():\n",
        "            build_btn = gr.Button(\"Run\", variant=\"primary\")\n",
        "\n",
        "        gr.Markdown(\"#### Clone a Shared Cohort\")\n",
        "        with gr.Row():\n",
        "            clone_source_dropdown = gr.Dropdown(\n",
        "                label=\"Source Cohort (shared + cloning enabled)\",\n",
        "                choices=[],\n",
        "                interactive=True,\n",
        "                visible=False,\n",
        "            )\n",
        "            clone_target_name = gr.Textbox(\n",
        "                label=\"New Cohort Name (clone target)\",\n",
        "                placeholder=\"e.g., My_Copy_of_WIC_Security_2025\",\n",
        "                visible=False,\n",
        "            )\n",
        "\n",
        "        with gr.Row():\n",
        "            clone_btn = gr.Button(\"Clone Cohort\", visible=False)\n",
        "\n",
        "        build_status = gr.Markdown(\"\")\n",
        "\n",
        "        # ---------- dynamic visibility ----------\n",
        "        def _toggle_fields(action: str, session_user):\n",
        "            # ✅ FIX 1: Handle None/empty action by defaulting to CREATE\n",
        "            if not action or action not in [ACTION_CREATE, ACTION_APPEND,       ACTION_CLONE]:\n",
        "                action = ACTION_CREATE\n",
        "\n",
        "            try:\n",
        "                su = _require_session_user(session_user, \"use cohort management\")\n",
        "            except PermissionError:\n",
        "                # If not logged in, hide everything. (Login gating policy)\n",
        "                return (\n",
        "                    gr.update(visible=False),  # new_cohort_name\n",
        "                    gr.update(visible=False, choices=[], value=None),  # existing_cohort_dropdown\n",
        "                    gr.update(visible=False),  # share_checkbox\n",
        "                    gr.update(visible=False),  # allow_clone_checkbox\n",
        "                    gr.update(visible=False),  # file_uploader\n",
        "                    gr.update(visible=False),  # build_btn\n",
        "                    gr.update(visible=False, choices=[], value=None),  # clone_source_dropdown\n",
        "                    gr.update(visible=False),  # clone_target_name\n",
        "                    gr.update(visible=False),  # clone_btn\n",
        "                    gr.update(value=\"❌ Please log in to manage cohorts.\", visible=True),  # clone_info_md\n",
        "                    gr.update(visible=False, value=\"\"),               # cohort_desc_tb\n",
        "                    gr.update(visible=False, value=\"\"),               # cohort_aud_tb\n",
        "                    gr.update(visible=False),                          # save_meta_btn\n",
        "                    gr.update(visible=False, value=\"\"),               # meta_status_md\n",
        "                )\n",
        "\n",
        "            is_create = action == ACTION_CREATE\n",
        "            is_append = action == ACTION_APPEND\n",
        "            is_clone = action == ACTION_CLONE\n",
        "            meta_vis = bool((is_create or is_append))\n",
        "            meta_save_vis = bool(is_append)\n",
        "\n",
        "            clone_sources = _list_cloneable_shared_cohorts() if is_clone else []\n",
        "            has_sources = len(clone_sources) > 0\n",
        "\n",
        "            msg = \"\"\n",
        "            if is_clone and not has_sources:\n",
        "                msg = \"ℹ️ No shared cohorts are available for you to clone yet.\"\n",
        "\n",
        "            return (\n",
        "                gr.update(visible=is_create),\n",
        "                gr.update(visible=is_append),\n",
        "                gr.update(visible=is_create or is_append),\n",
        "                gr.update(visible=is_create or is_append),\n",
        "                gr.update(visible=is_create or is_append),\n",
        "                gr.update(visible=is_create or is_append),\n",
        "                gr.update(visible=is_clone and has_sources, choices=clone_sources, value=None),\n",
        "                gr.update(visible=is_clone and has_sources),\n",
        "                gr.update(visible=is_clone and has_sources),\n",
        "                gr.update(value=msg, visible=bool(msg)),\n",
        "                gr.update(visible=meta_vis),                          # cohort_desc_tb\n",
        "                gr.update(visible=meta_vis),                          # cohort_aud_tb\n",
        "                gr.update(visible=meta_save_vis),                          # save_meta_btn\n",
        "                gr.update(visible=meta_vis),                          # meta_status_md\n",
        "            )\n",
        "\n",
        "        action_radio.change(\n",
        "            fn=safe_ui_call(_toggle_fields, fallback=(\n",
        "                gr.update(visible=True),   # new_cohort_name\n",
        "                gr.update(visible=False),  # existing_cohort_dropdown\n",
        "                gr.update(visible=True),   # share_checkbox\n",
        "                gr.update(visible=True),   # allow_clone_checkbox\n",
        "                gr.update(visible=True),   # file_uploader\n",
        "                gr.update(visible=True),   # build_btn\n",
        "                gr.update(visible=False, choices=[], value=None),  # clone_source_dropdown\n",
        "                gr.update(visible=False, value=\"\"),              # clone_target_name\n",
        "                gr.update(visible=False),                         # clone_btn\n",
        "                gr.update(value=\"❌ UI error. See trace log.\", visible=True),  # clone_info_md\n",
        "                gr.update(visible=False, value=\"\"),               # cohort_desc_tb\n",
        "                gr.update(visible=False, value=\"\"),               # cohort_aud_tb\n",
        "                gr.update(visible=False),                          # save_meta_btn\n",
        "                gr.update(visible=False, value=\"\"),               # meta_status_md\n",
        "            )),\n",
        "            inputs=[action_radio, session_state],\n",
        "            outputs=[\n",
        "                new_cohort_name,\n",
        "                existing_cohort_dropdown,\n",
        "                share_checkbox,\n",
        "                allow_clone_checkbox,\n",
        "                file_uploader,\n",
        "                build_btn,\n",
        "                clone_source_dropdown,\n",
        "                clone_target_name,\n",
        "                clone_btn,\n",
        "                clone_info_md,\n",
        "                cohort_desc_tb,\n",
        "                cohort_aud_tb,\n",
        "                save_meta_btn,\n",
        "                meta_status_md,\n",
        "            ],\n",
        "        )\n",
        "\n",
        "        # ✅ FIX 2: REMOVE THIS ENTIRE BLOCK (it causes the bug by re-triggering with action=None)\n",
        "        # Refresh the cohort-management UI when login/logout changes session_state.\n",
        "        # This prevents stale \"Please log in\" messages from persisting after a successful login.\n",
        "        session_state.change(\n",
        "            fn=safe_ui_call(_toggle_fields, fallback=(\n",
        "                gr.update(visible=True),   # new_cohort_name\n",
        "                gr.update(visible=False),  # existing_cohort_dropdown\n",
        "                gr.update(visible=True),   # share_checkbox\n",
        "                gr.update(visible=True),   # allow_clone_checkbox\n",
        "                gr.update(visible=True),   # file_uploader\n",
        "                gr.update(visible=True),   # build_btn\n",
        "                gr.update(visible=False, choices=[], value=None),  # clone_source_dropdown\n",
        "                gr.update(visible=False, value=\"\"),                # clone_target_name\n",
        "                gr.update(visible=False),                          # clone_btn\n",
        "                gr.update(value=\"\", visible=False),                # clone_info_md  (CLEAR)\n",
        "            )),\n",
        "            inputs=[action_radio, session_state],\n",
        "            outputs=[\n",
        "                new_cohort_name,\n",
        "                existing_cohort_dropdown,\n",
        "                share_checkbox,\n",
        "                allow_clone_checkbox,\n",
        "                file_uploader,\n",
        "                build_btn,\n",
        "                clone_source_dropdown,\n",
        "                clone_target_name,\n",
        "                clone_btn,\n",
        "                clone_info_md,\n",
        "                cohort_desc_tb,\n",
        "                cohort_aud_tb,\n",
        "                save_meta_btn,\n",
        "                meta_status_md,\n",
        "            ],\n",
        "            queue=False,\n",
        "        )\n",
        "\n",
        "\n",
        "        # ---------- refresh helpers ----------\n",
        "\n",
        "\n",
        "        def _refresh_existing_dropdown(session_user):\n",
        "            try:\n",
        "                su = _require_session_user(session_user, \"view cohorts\")\n",
        "            except PermissionError:\n",
        "                return gr.update(choices=[], value=None)\n",
        "            return gr.update(choices=get_cohorts_for_user(su.username), value=None)\n",
        "\n",
        "        def _refresh_clone_sources(session_user):\n",
        "            try:\n",
        "                _require_session_user(session_user, \"view cloneable cohorts\")\n",
        "            except PermissionError:\n",
        "                return gr.update(choices=[], value=None)\n",
        "            choices = _list_cloneable_shared_cohorts()\n",
        "            return gr.update(choices=choices, value=None)\n",
        "        def _normalize_cohort_name(selected: str | None) -> str:\n",
        "            s = (selected or \"\").strip()\n",
        "            if not s:\n",
        "                return \"\"\n",
        "            # If dropdown shows \"name/owner\", keep only the cohort name\n",
        "            return s.split(\"/\", 1)[0].strip()\n",
        "\n",
        "        # ---------- action handlers ----------\n",
        "       # ---------- action handlers ----------\n",
        "        def _run_action(action, api_key, session_user, new_name, existing_name, files, overwrite_confirm, is_shared, allow_clone, description, intended_audience):\n",
        "            \"\"\"\n",
        "            MUST return exactly 3 outputs (matches build_btn.click outputs):\n",
        "              1) build_status (markdown)\n",
        "              2) file_uploader (File component)\n",
        "              3) existing_cohort_dropdown (Dropdown component)\n",
        "            \"\"\"\n",
        "\n",
        "            # Helper: refresh existing cohort dropdown choices for this user\n",
        "            def _refresh_existing_dd(owner_username: str):\n",
        "                try:\n",
        "                    choices = get_cohorts_for_user(owner_username)\n",
        "                except Exception as e:\n",
        "                    trace_log(f\"SETUP refresh existing dropdown ERROR: {e}\")\n",
        "                    choices = []\n",
        "                return gr.update(choices=choices, value=None)\n",
        "\n",
        "            # 1) Require session user\n",
        "            try:\n",
        "                su = _require_session_user(session_user, \"create or modify cohorts\")\n",
        "            except PermissionError as e:\n",
        "                return f\"❌ {e}\", gr.update(), gr.update()\n",
        "\n",
        "            owner = (su.username or \"\").strip()\n",
        "            if not owner or owner.lower() == \"anonymous\":\n",
        "                return \"❌ You must be logged in to create or modify cohorts (owner is required).\", gr.update(), gr.update()\n",
        "\n",
        "            if not api_key or not str(api_key).strip():\n",
        "                return \"❌ Please provide an OpenAI API key.\", gr.update(), gr.update()\n",
        "\n",
        "            # -------------------------\n",
        "            # CREATE\n",
        "            # -------------------------\n",
        "            if action == ACTION_CREATE:\n",
        "                cohort_name = (new_name or \"\").strip()\n",
        "\n",
        "                if not cohort_name:\n",
        "                    return \"❌ Please provide a new cohort name.\", gr.update(), gr.update()\n",
        "                if cohort_exists(cohort_name):\n",
        "                    return f\"❌ Cohort '{cohort_name}' already exists.\", gr.update(), gr.update()\n",
        "                if not files:\n",
        "                    return \"❌ Please upload at least one file.\", gr.update(), gr.update()\n",
        "\n",
        "                # Build index first\n",
        "                try:\n",
        "                    msg = build_cohort_index(api_key=api_key, cohort_name=cohort_name, files=files, owner=owner)\n",
        "                except Exception as e:\n",
        "                    trace_log(f\"Create build_cohort_index ERROR: {e}\")\n",
        "                    return f\"❌ Error building cohort: {e}\", gr.update(), gr.update()\n",
        "\n",
        "                # Persist metadata (REQUIRED)\n",
        "                try:\n",
        "                    ensure_cohort_meta_table()\n",
        "                    set_cohort_owner(cohort_name, su)  # required\n",
        "                    set_cohort_sharing(cohort_name, bool(is_shared))\n",
        "                    set_cohort_allow_clone(cohort_name, bool(allow_clone))\n",
        "                    upsert_cohort_description_and_audience(cohort_name, description, intended_audience)\n",
        "                    upsert_cohort_description_and_audience(cohort_name, description, intended_audience)\n",
        "                except Exception as e:\n",
        "                    trace_log(f\"Create cohort meta update ERROR: {e}\")\n",
        "                    return f\"❌ Failed to persist cohort ownership/sharing metadata: {e}\", gr.update(), gr.update()\n",
        "\n",
        "                try:\n",
        "                    log_audit(owner, su.role, \"create_cohort\", f\"cohort={cohort_name}, shared={is_shared}, allow_clone={allow_clone}\")\n",
        "                except Exception as e:\n",
        "                    trace_log(f\"log_audit create_cohort ERROR: {e}\")\n",
        "\n",
        "                # SUCCESS: clear uploader + refresh dropdown\n",
        "                refreshed_dd = _refresh_existing_dd(owner)\n",
        "                return msg, gr.update(value=None), refreshed_dd\n",
        "\n",
        "            # -------------------------\n",
        "            # APPEND\n",
        "            # -------------------------\n",
        "            if action == ACTION_APPEND:\n",
        "                cohort_name = _normalize_cohort_name(existing_name)\n",
        "\n",
        "                if not cohort_name:\n",
        "                    return \"❌ Please select an existing cohort.\", gr.update(), gr.update()\n",
        "                if not cohort_exists(cohort_name):\n",
        "                    return f\"❌ Cohort '{cohort_name}' does not exist.\", gr.update(), gr.update()\n",
        "                if not files:\n",
        "                    return \"❌ Please upload at least one file.\", gr.update(), gr.update()\n",
        "                if not _can_write(session_user, cohort_name):\n",
        "                    return \"❌ Not authorized: only the cohort owner or an admin may append files.\", gr.update(), gr.update()\n",
        "\n",
        "                # Detect filename collisions (overwrite-by-filename requires explicit confirmation)\n",
        "                try:\n",
        "                    existing_docs = set([d.strip().lower() for d in (list_docs_in_cohort(cohort_name) or []) if d])\n",
        "                except Exception as e:\n",
        "                    trace_log(f\"Append list_docs_in_cohort WARNING cohort={cohort_name}: {e}\")\n",
        "                    existing_docs = set()\n",
        "\n",
        "                uploaded_names = []\n",
        "                for f in (files or []):\n",
        "                    n = getattr(f, \"name\", None) or getattr(f, \"orig_name\", None) or \"\"\n",
        "                    n = os.path.basename(str(n)).strip()\n",
        "                    if n:\n",
        "                        uploaded_names.append(n)\n",
        "\n",
        "                dupes = sorted({n for n in uploaded_names if n.strip().lower() in existing_docs})\n",
        "\n",
        "                if dupes and not bool(overwrite_confirm):\n",
        "                    msg_lines = [\n",
        "                        \"⚠️ **Potential overwrite detected**\",\n",
        "                        \"\",\n",
        "                        \"The following file name(s) already exist in this cohort:\",\n",
        "                    ] + [f\"- `{d}`\" for d in dupes] + [\n",
        "                        \"\",\n",
        "                        \"To proceed, check the overwrite confirmation box and click **Run** again. Otherwise, clear the upload selection to back out (no changes have been made).\",\n",
        "                    ]\n",
        "                    return \"\\n\".join(msg_lines), gr.update(), gr.update()\n",
        "\n",
        "\n",
        "                embed_cfg = get_default_embed_model_config()\n",
        "\n",
        "                try:\n",
        "                    msg = build_cohort_from_files(\n",
        "                        api_key=api_key,\n",
        "                        embed_model=embed_cfg.model_id,\n",
        "                        cohort_name=cohort_name,\n",
        "                        files=files,\n",
        "                        overwrite_existing=bool(overwrite_confirm)\n",
        "                    )\n",
        "                except Exception as e:\n",
        "                    trace_log(f\"Append build_cohort_from_files ERROR: {e}\")\n",
        "                    return f\"❌ Error appending files to cohort: {e}\", gr.update(), gr.update()\n",
        "\n",
        "                # Owner/admin may update sharing flags during append\n",
        "                try:\n",
        "                    ensure_cohort_meta_table()\n",
        "                    set_cohort_owner(cohort_name, su)  # repairs invalid owners if needed\n",
        "                    set_cohort_sharing(cohort_name, bool(is_shared))\n",
        "                    set_cohort_allow_clone(cohort_name, bool(allow_clone))\n",
        "                    upsert_cohort_description_and_audience(cohort_name, description, intended_audience)\n",
        "                except Exception as e:\n",
        "                    trace_log(f\"Append cohort meta update ERROR: {e}\")\n",
        "                    return f\"❌ Failed to update cohort metadata: {e}\", gr.update(), gr.update()\n",
        "\n",
        "                try:\n",
        "                    log_audit(owner, su.role, \"append_files\", f\"cohort={cohort_name}, shared={is_shared}, allow_clone={allow_clone}\")\n",
        "                except Exception as e:\n",
        "                    trace_log(f\"log_audit append_files ERROR: {e}\")\n",
        "\n",
        "                # SUCCESS: clear uploader + refresh dropdown\n",
        "                refreshed_dd = _refresh_existing_dd(owner)\n",
        "                return msg, gr.update(value=None), refreshed_dd\n",
        "\n",
        "            # -------------------------\n",
        "            # Unsupported\n",
        "            # -------------------------\n",
        "            return \"❌ Unsupported action selected.\", gr.update(), gr.update()\n",
        "\n",
        "        build_btn.click(\n",
        "            fn=safe_ui_call(_run_action, fallback=(\"❌ UI error. See trace log.\", gr.update(), gr.update())),\n",
        "            inputs=[\n",
        "                action_radio,\n",
        "                api_key_state,\n",
        "                session_state,\n",
        "                new_cohort_name,\n",
        "                existing_cohort_dropdown,\n",
        "                file_uploader,\n",
        "                overwrite_confirm,\n",
        "                share_checkbox,\n",
        "                allow_clone_checkbox,\n",
        "                cohort_desc_tb,\n",
        "                cohort_aud_tb,\n",
        "            ],\n",
        "            outputs=[build_status, file_uploader, existing_cohort_dropdown],\n",
        "\n",
        "        )\n",
        "\n",
        "\n",
        "        def _run_clone(api_key, session_user, source_cohort, target_cohort):\n",
        "            try:\n",
        "                su = _require_session_user(session_user, \"clone cohorts\")\n",
        "            except PermissionError as e:\n",
        "                return f\"❌ {e}\"\n",
        "\n",
        "            if not api_key or not str(api_key).strip():\n",
        "                return \"❌ Please provide an OpenAI API key.\"\n",
        "            if not source_cohort:\n",
        "                return \"❌ Please select a source cohort.\"\n",
        "            if not target_cohort or not target_cohort.strip():\n",
        "                return \"❌ Please provide a new cohort name for the clone.\"\n",
        "\n",
        "            allowed = set(_list_cloneable_shared_cohorts())\n",
        "            if (source_cohort not in allowed) and not _is_admin(su):\n",
        "                return \"❌ Not authorized: cloning is not enabled for this cohort.\"\n",
        "\n",
        "            msg = clone_cohort(source_cohort=source_cohort, target_cohort=target_cohort.strip(), new_owner=su.username)\n",
        "            try:\n",
        "                log_audit(su.username, su.role, \"clone_cohort\", f\"{source_cohort} → {target_cohort.strip()}\")\n",
        "            except Exception as e:\n",
        "                trace_log(f\"log_audit clone_cohort ERROR: {e}\")\n",
        "\n",
        "            return msg\n",
        "\n",
        "        clone_btn.click(\n",
        "            fn=safe_ui_call(_run_clone),\n",
        "            inputs=[api_key_state, session_state, clone_source_dropdown, clone_target_name],\n",
        "            outputs=[build_status],\n",
        "        )\n",
        "\n",
        "        # ---------- EPIC 5.1 / 5.2 — Admin cohort meta editor handlers ----------\n",
        "\n",
        "# ---------- Cohort metadata (Description / Intended Audience) ----------\n",
        "# Policy: cohort OWNER or ADMIN may edit; anyone with access may view.\n",
        "        def _load_cohort_meta(session_user, cohort_name):\n",
        "            try:\n",
        "                su = _require_session_user(session_user, \"edit cohort metadata\")\n",
        "            except PermissionError:\n",
        "                return (\n",
        "                    gr.update(value=\"\", visible=True),\n",
        "                    gr.update(value=\"\", visible=True),\n",
        "                    gr.update(visible=False),\n",
        "                    gr.update(value=\"❌ Please log in to edit cohort metadata.\", visible=True),\n",
        "                )\n",
        "\n",
        "            cn = (cohort_name or \"\").strip()\n",
        "            if not cn:\n",
        "                return (\n",
        "                    gr.update(value=\"\", visible=True),\n",
        "                    gr.update(value=\"\", visible=True),\n",
        "                    gr.update(visible=False),\n",
        "                    gr.update(value=\"\", visible=True),\n",
        "                )\n",
        "\n",
        "            # Only cohort owner or admin can edit metadata for an existing cohort.\n",
        "            if not _can_write(session_user, cn):\n",
        "                meta = get_cohort_meta(cn) or {}\n",
        "                desc = meta.get(\"description\", \"\") or \"\"\n",
        "                aud  = meta.get(\"intended_audience\", \"\") or \"\"\n",
        "                return (\n",
        "                    gr.update(value=desc, visible=True),\n",
        "                    gr.update(value=aud, visible=True),\n",
        "                    gr.update(visible=False),\n",
        "                    gr.update(value=\"ℹ️ Read-only: only the cohort owner or an admin may edit this metadata.\", visible=True),\n",
        "                )\n",
        "\n",
        "            meta = get_cohort_meta(cn) or {}\n",
        "            return (\n",
        "                gr.update(value=meta.get(\"description\", \"\"), visible=True),\n",
        "                gr.update(value=meta.get(\"intended_audience\", \"\"), visible=True),\n",
        "                gr.update(visible=True),\n",
        "                gr.update(value=\"\", visible=True),\n",
        "            )\n",
        "\n",
        "        def _save_cohort_meta(session_user, cohort_name, description, intended_audience):\n",
        "            try:\n",
        "                _require_session_user(session_user, \"save cohort metadata\")\n",
        "            except PermissionError:\n",
        "                return \"❌ Please log in to save cohort metadata.\"\n",
        "\n",
        "            cn = (cohort_name or \"\").strip()\n",
        "            if not cn:\n",
        "                return \"❌ Please select a cohort first.\"\n",
        "\n",
        "            if not _can_write(session_user, cn):\n",
        "                return \"❌ Not authorized: only the cohort owner or an admin may edit metadata.\"\n",
        "\n",
        "            try:\n",
        "                upsert_cohort_description_and_audience(cn, description, intended_audience)\n",
        "                try:\n",
        "                    su = _require_session_user(session_user, \"audit cohort metadata save\")\n",
        "                    log_audit(su.username, su.role, \"save_cohort_meta\", f\"{cn}\")\n",
        "                except Exception as e:\n",
        "                    trace_log(f\"log_audit save_cohort_meta ERROR: {e}\")\n",
        "                return \"✅ Cohort metadata saved.\"\n",
        "            except Exception as e:\n",
        "                trace_log(f\"_save_cohort_meta ERROR: {e}\")\n",
        "                return f\"❌ Error saving cohort metadata: {e}\"\n",
        "\n",
        "        existing_cohort_dropdown.change(\n",
        "                    fn=safe_ui_call(_load_cohort_meta),\n",
        "                    inputs=[session_state, existing_cohort_dropdown],\n",
        "                    outputs=[cohort_desc_tb, cohort_aud_tb, save_meta_btn, meta_status_md],\n",
        "                    queue=False,\n",
        "                )\n",
        "\n",
        "        save_meta_btn.click(\n",
        "            fn=safe_ui_call(_save_cohort_meta),\n",
        "            inputs=[api_key_state, session_state, existing_cohort_dropdown, cohort_desc_tb, cohort_aud_tb],\n",
        "            outputs=[meta_status_md],\n",
        "            queue=False,\n",
        "        )\n",
        "\n",
        "        # Return handles expected by build_interface()\n",
        "        return {\n",
        "            \"action_radio\": action_radio,\n",
        "            \"new_cohort_name\": new_cohort_name,\n",
        "            \"existing_cohort_dropdown\": existing_cohort_dropdown,\n",
        "            \"share_checkbox\": share_checkbox,\n",
        "            \"allow_clone_checkbox\": allow_clone_checkbox,\n",
        "            \"file_uploader\": file_uploader,\n",
        "            \"build_btn\": build_btn,\n",
        "            \"clone_source_dropdown\": clone_source_dropdown,\n",
        "            \"clone_target_name\": clone_target_name,\n",
        "            \"clone_btn\": clone_btn,\n",
        "            \"build_status\": build_status,\n",
        "            \"clone_info_md\": clone_info_md,\n",
        "            \"refresh_existing_dropdown\": _refresh_existing_dropdown,\n",
        "            \"refresh_clone_sources\": _refresh_clone_sources,\n",
        "        }\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6aa3030a",
      "metadata": {
        "id": "6aa3030a"
      },
      "source": [
        "## STEP 11 — Ask tab\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "39b401be",
      "metadata": {
        "id": "39b401be"
      },
      "outputs": [],
      "source": [
        "def build_ask_tab(session_state, api_key_state, session_memory_state):\n",
        "        \"\"\"\n",
        "        Ask tab:\n",
        "          - Choose cohort + chat model\n",
        "          - Optional prompt improvement\n",
        "          - Ask question over cohort with RAG\n",
        "          - View documents in selected cohort\n",
        "        \"\"\"\n",
        "        gr.Markdown(\"### Ask Questions Over a Cohort\")\n",
        "        # EPIC 4.1 — Expectation Management (\"What this is / isn't\")\n",
        "\n",
        "        # EPIC 5.1 / 5.2 — Cohort context (read-only display on Ask tab)\n",
        "        cohort_desc_md = gr.Markdown(\"**Cohort Description:** (not set)\", visible=False)\n",
        "        cohort_aud_md  = gr.Markdown(\"**Intended Audience:** (not set)\", visible=False)\n",
        "\n",
        "\n",
        "        with gr.Row():\n",
        "            cohort_dropdown = gr.Dropdown(\n",
        "                label=\"Cohort\",\n",
        "                choices=[],\n",
        "                elem_id=\"ask_cohort_dropdown\",\n",
        "            )\n",
        "            model_dropdown = gr.Dropdown(\n",
        "                label=\"Choose Chat Model\",\n",
        "                choices=list_chat_models(),\n",
        "                value=get_default_chat_model_id(),\n",
        "                elem_id=\"ask_model_dropdown\",\n",
        "            )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            # EPIC 3.4 — Provenance / Evidence mode\n",
        "            provenance_mode_radio = gr.Radio(\n",
        "                label=\"Provenance Mode (EPIC 3.4)\",\n",
        "                choices=[\"Off\", \"Explain Sources\", \"Strict Evidence Only\"],\n",
        "                value=\"Off\",\n",
        "                interactive=True,\n",
        "                elem_id=\"ask_provenance_mode_radio\",\n",
        "            )\n",
        "        with gr.Row():\n",
        "            demo_prompt_dropdown = gr.Dropdown(\n",
        "                label=\"FAP (optional)\",\n",
        "                choices=[],\n",
        "                value=None,\n",
        "                interactive=True,\n",
        "                visible=False,\n",
        "                elem_id=\"ask_demo_prompt_dropdown\",\n",
        "            )\n",
        "\n",
        "        with gr.Row():\n",
        "            question_box = gr.Textbox(\n",
        "                label=\"Your Question\",\n",
        "                lines=3,\n",
        "                elem_id=\"ask_question_box\",\n",
        "            )\n",
        "            improved_box = gr.Textbox(\n",
        "                label=\"Improved Prompt (optional, from Prompt Coach)\",\n",
        "                lines=3,\n",
        "                elem_id=\"ask_improved_box\",\n",
        "            )\n",
        "\n",
        "\n",
        "        with gr.Row():\n",
        "            improve_btn = gr.Button(\"Improve Prompt\", elem_id=\"ask_improve_btn\")\n",
        "            ask_btn = gr.Button(\"Ask Question\", elem_id=\"ask_ask_btn\")\n",
        "            reset_demo_btn = gr.Button(\"Reset Demo\", elem_id=\"ask_reset_demo_btn\")  # EPIC 1.3\n",
        "            refresh_cohorts_btn = gr.Button(\"Refresh Cohorts\", elem_id=\"ask_refresh_btn\")\n",
        "            refresh_models_btn = gr.Button(\"Refresh Models\", elem_id=\"ask_refresh_models_btn\")\n",
        "\n",
        "\n",
        "\n",
        "        with gr.Accordion(\"Documents in Selected Cohort\", open=False):\n",
        "            docs_md = gr.Markdown(\"_No cohort selected._\")\n",
        "\n",
        "\n",
        "        gr.Markdown(\"#### Answer\")\n",
        "        answer_md = gr.Markdown()\n",
        "\n",
        "        # EPIC 2.1 — Trust/Provenance panel (collapsed by default)\n",
        "        with gr.Accordion(\"Why This Answer\", open=False):\n",
        "            why_md = gr.Markdown(\"\")\n",
        "\n",
        "        # EPIC 2.2 — Always-visible footnote (short provenance)\n",
        "        answer_footnote_md = gr.Markdown(\"\")\n",
        "\n",
        "        # ---- Callbacks ----\n",
        "\n",
        "        def _refresh_cohorts(session_user):\n",
        "            \"\"\"Refresh cohort list for the logged-in user only (no anonymous access).\"\"\"\n",
        "            try:\n",
        "                su = require_login(session_user, \"view cohorts\")\n",
        "                names = get_cohorts_for_user(su.username)\n",
        "                return gr.update(choices=names, value=None)\n",
        "            except PermissionError:\n",
        "                return gr.update(choices=[], value=None)\n",
        "            except Exception as e:\n",
        "                trace_log(f\"ASK _refresh_cohorts ERROR: {e}\")\n",
        "                return gr.update(choices=[], value=None)\n",
        "\n",
        "        def _refresh_models(session_user):\n",
        "            \"\"\"Refresh chat model list from the model registry (login required).\"\"\"\n",
        "            try:\n",
        "                require_login(session_user, \"view chat models\")\n",
        "            except PermissionError:\n",
        "                return gr.update(choices=[], value=None)\n",
        "            try:\n",
        "                choices = list_chat_models()\n",
        "                default_id = get_default_chat_model_id()\n",
        "                value = default_id if default_id in choices else (choices[0] if choices else None)\n",
        "                return gr.update(choices=choices, value=value)\n",
        "            except Exception as e:\n",
        "                trace_log(f\"ASK _refresh_models ERROR: {e}\")\n",
        "                return gr.update(choices=list_chat_models(), value=get_default_chat_model_id())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        def _refresh_demo_prompts_ui(cohort_name: str, session_user):\n",
        "            \"\"\"\n",
        "            If selected cohort has FAPs, show the Demo Prompt dropdown and populate choices.\n",
        "            Always clears the question box on cohort change to avoid stale text between demos.\n",
        "            \"\"\"\n",
        "            try:\n",
        "                su = require_login(session_user, \"view FAPs\")\n",
        "            except PermissionError:\n",
        "                return gr.update(visible=False, choices=[], value=None), gr.update(value=\"\")\n",
        "\n",
        "            c = (cohort_name or \"\").strip()\n",
        "            if not c:\n",
        "                return gr.update(visible=False, choices=[], value=None), gr.update(value=\"\")\n",
        "\n",
        "            prompts = list_demo_prompts_for_cohort(c)\n",
        "            if not prompts:\n",
        "                return gr.update(visible=False, choices=[], value=None), gr.update(value=\"\")\n",
        "\n",
        "            return gr.update(visible=True, choices=prompts, value=None), gr.update(value=\"\")\n",
        "\n",
        "        def _demo_prompt_pick_cb(selected_prompt: str):\n",
        "            # Populate \"Your Question\" from the selected demo prompt.\n",
        "            return gr.update(value=(selected_prompt or \"\").strip())\n",
        "\n",
        "        def _reset_demo_cb():\n",
        "            \"\"\"\n",
        "            EPIC 1.3 — Reset Demo\n",
        "            Clears only UI state (no DB changes):\n",
        "              - Demo Prompt selection\n",
        "              - Your Question\n",
        "              - Improved Prompt\n",
        "              - Answer\n",
        "            \"\"\"\n",
        "            return (\n",
        "                    gr.update(value=None),  # demo_prompt_dropdown\n",
        "                    \"\",                     # question_box\n",
        "                    \"\",                     # improved_box\n",
        "                    \"\",                     # answer_md\n",
        "                    \"\",                     # why_md\n",
        "                    \"\",                     # answer_footnote_md\n",
        "                )\n",
        "\n",
        "\n",
        "        def _improve_query_cb(original_query: str, api_key: str):\n",
        "            if not api_key or not api_key.strip():\n",
        "                return \"❌ OpenAI API key is required.\"\n",
        "            try:\n",
        "                improved = improve_query(\n",
        "                    api_key=api_key,\n",
        "                    chat_model=CHAT_MODEL_DEFAULT,\n",
        "                    original_query=original_query or \"\",\n",
        "                )\n",
        "                if not improved:\n",
        "                    return original_query\n",
        "                return improved\n",
        "            except Exception as e:\n",
        "                trace_log(f\"ASK _improve_query_cb ERROR: {e}\")\n",
        "                return f\"❌ Error improving query: {e}\"\n",
        "\n",
        "        def _ask_cb(\n",
        "            cohort_name: str,\n",
        "            question: str,\n",
        "            improved_prompt: str,\n",
        "            model_id: str,\n",
        "            provenance_mode: str,  # EPIC 3.4\n",
        "            session_user,\n",
        "            api_key: str,\n",
        "            session_mem: dict,           # NEW\n",
        "        ):\n",
        "\n",
        "        # EPIC 3.4.1 — Ensure session_mem always exists for consistent returns\n",
        "            if not session_mem:\n",
        "                session_mem = init_session_memory_state()\n",
        "\n",
        "            def _ret(ans, why=\"\", foot=\"\"):\n",
        "                return ans, why, foot, session_mem\n",
        "\n",
        "            try:\n",
        "                su = require_login(session_user, \"ask questions\")\n",
        "            except PermissionError as e:\n",
        "                return _ret(f\"❌ Error while answering question: {e}\")\n",
        "\n",
        "            username = (su.get(\"username\") if isinstance(su, dict) else getattr(su, \"username\", \"\")).strip()\n",
        "\n",
        "            # Authorization: user must have visibility to cohort\n",
        "            if cohort_name and cohort_name not in set(get_cohorts_for_user(username)):\n",
        "                return _ret(\"❌ Not authorized to access this cohort.\")\n",
        "\n",
        "            if not api_key or not api_key.strip():\n",
        "                return _ret(\"❌ OpenAI API key is required.\")\n",
        "            # EPIC 3.3.1 — Allow session-scope questions without a cohort\n",
        "           # EPIC 3.4.1 — Cohort selection rules\n",
        "            cohort_selected = bool((cohort_name or \"\").strip())\n",
        "\n",
        "            if not cohort_selected:\n",
        "                # Strict Evidence Only requires a cohort (evidence source)\n",
        "                if provenance_mode == \"Strict Evidence Only\":\n",
        "                    return _ret(\"❌ Strict Evidence Only requires a cohort selection (document evidence).\")\n",
        "\n",
        "                # Allow cohortless questions only when memory scope is session\n",
        "                if session_mem.get(\"scope\") != \"session\":\n",
        "                    return _ret(\"❌ Please select a cohort, or switch Memory Scope to 'session' to ask without a cohort.\")\n",
        "                # else: allowed to proceed without cohort\n",
        "\n",
        "\n",
        "            # Decide which prompt is used\n",
        "            improved_clean = (improved_prompt or \"\").strip()\n",
        "            question_clean = (question or \"\").strip()\n",
        "            used_improved = bool(improved_clean)\n",
        "\n",
        "            final_question = improved_clean if used_improved else question_clean\n",
        "            if not final_question:\n",
        "                return _ret(\"❌ Please enter a question.\")\n",
        "\n",
        "            try:\n",
        "                answer_markdown, raw_answer_text, used_model, why_md_text, footnote_text = answer_question_over_cohort(\n",
        "                    api_key=api_key,\n",
        "                    username=username,\n",
        "                    cohort_name=cohort_name,\n",
        "                    question=final_question,\n",
        "                    model_id=model_id or CHAT_MODEL_DEFAULT,\n",
        "                    provenance_mode=provenance_mode or 'Off',  # EPIC 3.4\n",
        "                    session_mem=session_mem,\n",
        "                )\n",
        "\n",
        "                # Log to chat_history\n",
        "                try:\n",
        "                    save_chat_history(\n",
        "                        user=session_user,\n",
        "                        cohort=cohort_name,\n",
        "                        question=final_question,\n",
        "                        answer=raw_answer_text,\n",
        "                        model_used=used_model,\n",
        "                    )\n",
        "                except Exception as log_e:\n",
        "                    trace_log(f\"ASK save_chat_history ERROR: {log_e}\")                # EPIC 3.2 — update in-session memory (ephemeral; not persisted)\n",
        "                try:\n",
        "                    if session_mem and session_mem.get(\"enabled\"):\n",
        "                        session_mem = update_session_memory_state(\n",
        "                            mem=session_mem,\n",
        "                            cohort_name=cohort_name,\n",
        "                            q=final_question,\n",
        "                            a=raw_answer_text or \"\",\n",
        "                            max_turns=int((session_mem or {}).get('max_turns') or SESSION_MEM_MAX_TURNS),  # EPIC 3.3\n",
        "\n",
        "                        )\n",
        "                except Exception as mem_e:\n",
        "                    trace_log(f\"ASK session memory update ERROR: {mem_e}\")\n",
        "\n",
        "\n",
        "\n",
        "                # Add a visible note about which prompt was used\n",
        "                if used_improved:\n",
        "                    note = \"**Note:** Used the *Improved Prompt* for this answer.\\n\\n\"\n",
        "                else:\n",
        "                    note = \"**Note:** Used your *Original Question* for this answer.\\n\\n\"\n",
        "\n",
        "                return (note + (answer_markdown or \"\")), (why_md_text or \"\"), (footnote_text or \"\"), session_mem\n",
        "\n",
        "\n",
        "            except Exception as e:\n",
        "                trace_log(f\"ASK _ask_cb ERROR: {e}\")\n",
        "                return _ret(f\"❌ Error while answering question: {e}\")\n",
        "\n",
        "        def _load_docs_for_cohort(session_user, cohort_name: str, session_mem: dict):  # EPIC 3.3\n",
        "            \"\"\"Show documents for the selected cohort (login required).\"\"\"\n",
        "            try:\n",
        "                su = require_login(session_user, \"view cohort documents\")\n",
        "            except PermissionError:\n",
        "                return \"❌ Login required to view cohort documents.\", session_mem\n",
        "            if not cohort_name:\n",
        "                return \"_No cohort selected._\", session_mem\n",
        "            # Authorization: user must have visibility to this cohort\n",
        "            if cohort_name not in set(get_cohorts_for_user(su.username)):\n",
        "                return \"❌ Not authorized to view this cohort.\", session_mem\n",
        "            # EPIC 3.3 — Cohort-scope session memory: clear turns when cohort changes\n",
        "            session_mem = session_mem if isinstance(session_mem, dict) else init_session_memory_state()\n",
        "            scope_val = (session_mem.get('scope') or 'cohort').lower()\n",
        "            prev_cohort = session_mem.get('cohort_name')\n",
        "            if scope_val == 'cohort' and cohort_name and prev_cohort and cohort_name != prev_cohort:\n",
        "                enabled = bool(session_mem.get('enabled', True))\n",
        "                max_turns = int(session_mem.get('max_turns') or SESSION_MEM_MAX_TURNS)\n",
        "                session_mem = init_session_memory_state()\n",
        "                session_mem['enabled'] = enabled\n",
        "                session_mem['max_turns'] = max_turns\n",
        "                session_mem['scope'] = 'cohort'\n",
        "            session_mem['cohort_name'] = cohort_name\n",
        "\n",
        "            try:\n",
        "                conn = get_db_conn()\n",
        "                cur = conn.cursor()\n",
        "                cur.execute(\n",
        "                    \"\"\"\n",
        "                    SELECT doc_name, created_at\n",
        "                    FROM documents\n",
        "                    WHERE cohort_name = ?\n",
        "                    ORDER BY created_at\n",
        "                    \"\"\",\n",
        "                    (cohort_name,),\n",
        "                )\n",
        "                rows = cur.fetchall()\n",
        "                conn.close()\n",
        "\n",
        "                if not rows:\n",
        "                    return f\"_No documents found for cohort `{cohort_name}`._\", session_mem\n",
        "\n",
        "                lines = [f\"**Documents in cohort `{cohort_name}`:**\"]\n",
        "                for doc_name, created_at in rows:\n",
        "                    if created_at:\n",
        "                        lines.append(f\"- `{doc_name}` (added {created_at})\")\n",
        "                    else:\n",
        "                        lines.append(f\"- `{doc_name}`\")\n",
        "                return \"\\n\".join(lines), session_mem\n",
        "            except Exception as e:\n",
        "                trace_log(f\"ASK _load_docs_for_cohort ERROR: {e}\")\n",
        "                return f\"❌ Error loading documents for cohort `{cohort_name}`.\", session_mem\n",
        "\n",
        "        # Wire buttons\n",
        "        refresh_cohorts_btn.click(\n",
        "            _refresh_cohorts,\n",
        "            inputs=[session_state],\n",
        "            outputs=[cohort_dropdown],\n",
        "        )\n",
        "\n",
        "        refresh_models_btn.click(\n",
        "            _refresh_models,\n",
        "            inputs=[session_state],\n",
        "            outputs=[model_dropdown],\n",
        "        )\n",
        "\n",
        "\n",
        "        improve_btn.click(\n",
        "            _improve_query_cb,\n",
        "            inputs=[question_box, api_key_state],\n",
        "            outputs=[improved_box],\n",
        "        )\n",
        "\n",
        "        ask_btn.click(\n",
        "            _ask_cb,\n",
        "            inputs=[\n",
        "                cohort_dropdown,\n",
        "                question_box,\n",
        "                improved_box,\n",
        "                model_dropdown,\n",
        "                provenance_mode_radio,  # EPIC 3.4\n",
        "                session_state,\n",
        "                api_key_state,\n",
        "                session_memory_state,\n",
        "            ],\n",
        "            outputs=[answer_md, why_md, answer_footnote_md, session_memory_state],\n",
        "        )\n",
        "\n",
        "\n",
        "        reset_demo_btn.click(\n",
        "            fn=safe_ui_call(\n",
        "                _reset_demo_cb,\n",
        "                fallback=(gr.update(value=None), \"\", \"\", \"\", \"\", \"\"),\n",
        "            ),\n",
        "            inputs=[],\n",
        "            outputs=[\n",
        "                demo_prompt_dropdown,\n",
        "                question_box,\n",
        "                improved_box,\n",
        "                answer_md,\n",
        "                why_md,\n",
        "                answer_footnote_md,\n",
        "            ],\n",
        "            queue=False,\n",
        "        )\n",
        "\n",
        "\n",
        "        def _refresh_cohort_meta_md(cohort_name):\n",
        "            cn = (cohort_name or \"\").strip()\n",
        "            if not cn:\n",
        "                # Hide meta until a cohort is selected (Option A)\n",
        "                return (\n",
        "                    gr.update(value=\"\", visible=False),\n",
        "                    gr.update(value=\"\", visible=False),\n",
        "                )\n",
        "\n",
        "            meta = get_cohort_meta(cn) or {}\n",
        "            desc = (meta.get(\"description\", \"\") or \"\").strip()\n",
        "            aud  = (meta.get(\"intended_audience\", \"\") or \"\").strip()\n",
        "            desc_out = desc if desc else \"(not set)\"\n",
        "            aud_out  = aud if aud else \"(not set)\"\n",
        "\n",
        "            return (\n",
        "                gr.update(value=f\"**Cohort Description:** {desc_out}\", visible=True),\n",
        "                gr.update(value=f\"**Intended Audience:** {aud_out}\", visible=True),\n",
        "            )\n",
        "\n",
        "        cohort_dropdown.change(\n",
        "            _load_docs_for_cohort,\n",
        "            inputs=[session_state, cohort_dropdown, session_memory_state],  # EPIC 3.3\n",
        "            outputs=[docs_md, session_memory_state],  # EPIC 3.3\n",
        "        )\n",
        "\n",
        "        cohort_dropdown.change(\n",
        "            _refresh_demo_prompts_ui,\n",
        "            inputs=[cohort_dropdown, session_state],\n",
        "            outputs=[demo_prompt_dropdown, question_box],\n",
        "        )\n",
        "\n",
        "        cohort_dropdown.change(\n",
        "            _refresh_cohort_meta_md,\n",
        "            inputs=[cohort_dropdown],\n",
        "            outputs=[cohort_desc_md, cohort_aud_md],\n",
        "            queue=False,\n",
        "        )\n",
        "\n",
        "\n",
        "        demo_prompt_dropdown.change(\n",
        "            _demo_prompt_pick_cb,\n",
        "            inputs=[demo_prompt_dropdown],\n",
        "            outputs=[question_box],\n",
        "        )\n",
        "\n",
        "        # ============================================================\n",
        "        # EPIC 3.3 — Session Memory Controls (User-Self-Service)\n",
        "        # (Additive UI; minimal impact to existing layout)\n",
        "        # ============================================================\n",
        "        with gr.Accordion(\"Session Memory (EPIC 3.3)\", open=False):\n",
        "            session_mem_user_status_md = gr.Markdown(\"\")\n",
        "            session_mem_user_scope_radio = gr.Radio(\n",
        "                choices=[\"cohort\", \"session\"],\n",
        "                value=\"cohort\",\n",
        "                label=\"Memory scope (EPIC 3.3)\",\n",
        "                info=\"cohort = resets on cohort change; session = persists across cohorts\",\n",
        "            )\n",
        "            session_mem_user_max_turns = gr.Number(\n",
        "                label=\"Max turns (EPIC 3.3)\",\n",
        "                value=SESSION_MEM_MAX_TURNS,\n",
        "                precision=0,\n",
        "            )\n",
        "            with gr.Row():\n",
        "                session_mem_user_clear_btn = gr.Button(\"Clear My Session Memory (EPIC 3.3)\")\n",
        "                session_mem_user_refresh_btn = gr.Button(\"Refresh Viewer (EPIC 3.3)\")\n",
        "            session_mem_user_view_md = gr.Markdown(\"_Session memory viewer will appear here._\")\n",
        "\n",
        "        def _format_session_mem_view_user(mem: dict) -> str:\n",
        "            if not isinstance(mem, dict):\n",
        "                return \"_(no session memory state)_\"\n",
        "            enabled = mem.get(\"enabled\", True)\n",
        "            scope = (mem.get(\"scope\") or \"cohort\").upper()\n",
        "            max_turns = int(mem.get(\"max_turns\") or SESSION_MEM_MAX_TURNS)\n",
        "            cohort_name = mem.get(\"cohort_name\")\n",
        "            turns = mem.get(\"turns\") or []\n",
        "            if not enabled:\n",
        "                return f\"_(OFF; scope={scope}; max_turns={max_turns}; cohort={cohort_name})_\"\n",
        "            if not turns:\n",
        "                return f\"_(ON; empty; scope={scope}; max_turns={max_turns}; cohort={cohort_name})_\"\n",
        "            lines = [f\"**Session Memory (ON)** — scope={scope}, max_turns={max_turns}, cohort={cohort_name}, turns={len(turns)}\\n\"]\n",
        "            for i, t in enumerate(turns[-max_turns:], start=max(1, len(turns)-max_turns+1)):\n",
        "                q = (t.get(\"q\") or \"\").strip()\n",
        "                a = (t.get(\"a\") or \"\").strip()\n",
        "                lines.append(f\"**{i}. Q:** {q}\\n**A:** {a}\\n\")\n",
        "            return \"\\n\".join(lines)\n",
        "\n",
        "        def _user_clear_session_memory(mem: dict):\n",
        "            mem = mem if isinstance(mem, dict) else init_session_memory_state()\n",
        "            mem[\"turns\"] = []\n",
        "            return mem, \"✅ Cleared your session memory.\", _format_session_mem_view_user(mem)\n",
        "\n",
        "        def _user_set_session_mem_max_turns(max_turns_val, mem: dict):\n",
        "            mem = mem if isinstance(mem, dict) else init_session_memory_state()\n",
        "            try:\n",
        "                mv = int(max_turns_val or SESSION_MEM_MAX_TURNS)\n",
        "            except Exception:\n",
        "                mv = SESSION_MEM_MAX_TURNS\n",
        "            mv = max(1, min(200, mv))\n",
        "            mem[\"max_turns\"] = mv\n",
        "            return mem, f\"✅ Max turns set to {mv}.\", _format_session_mem_view_user(mem), gr.update(value=mv)\n",
        "\n",
        "        def _user_set_session_mem_scope(scope_val, mem: dict):\n",
        "            mem = mem if isinstance(mem, dict) else init_session_memory_state()\n",
        "            scope_val = (scope_val or \"cohort\").lower()\n",
        "            if scope_val not in (\"cohort\", \"session\"):\n",
        "                scope_val = \"cohort\"\n",
        "            mem[\"scope\"] = scope_val\n",
        "            return mem, f\"✅ Scope set to {scope_val.upper()}.\", _format_session_mem_view_user(mem), gr.update(value=scope_val)\n",
        "\n",
        "        def _user_refresh_session_mem_view(mem: dict):\n",
        "            mem = mem if isinstance(mem, dict) else init_session_memory_state()\n",
        "            return _format_session_mem_view_user(mem)\n",
        "\n",
        "        # Wire user controls (no admin gating)\n",
        "        session_mem_user_clear_btn.click(\n",
        "            _user_clear_session_memory,\n",
        "            inputs=[session_memory_state],\n",
        "            outputs=[session_memory_state, session_mem_user_status_md, session_mem_user_view_md],\n",
        "            queue=False,\n",
        "        )\n",
        "        session_mem_user_max_turns.change(\n",
        "            _user_set_session_mem_max_turns,\n",
        "            inputs=[session_mem_user_max_turns, session_memory_state],\n",
        "            outputs=[session_memory_state, session_mem_user_status_md, session_mem_user_view_md, session_mem_user_max_turns],\n",
        "            queue=False,\n",
        "            trigger_mode=\"once\",\n",
        "        )\n",
        "        session_mem_user_scope_radio.change(\n",
        "            _user_set_session_mem_scope,\n",
        "            inputs=[session_mem_user_scope_radio, session_memory_state],\n",
        "            outputs=[session_memory_state, session_mem_user_status_md, session_mem_user_view_md, session_mem_user_scope_radio],\n",
        "            queue=False,\n",
        "            trigger_mode=\"once\",\n",
        "        )\n",
        "        session_mem_user_refresh_btn.click(\n",
        "            _user_refresh_session_mem_view,\n",
        "            inputs=[session_memory_state],\n",
        "            outputs=[session_mem_user_view_md],\n",
        "            queue=False,\n",
        "        )\n",
        "\n",
        "        # EPIC 3.3.2 — Keep viewer in sync with state updates (e.g., cohort-scope clear)\n",
        "        session_memory_state.change(\n",
        "            _user_refresh_session_mem_view,\n",
        "            inputs=[session_memory_state],\n",
        "            outputs=[session_mem_user_view_md],\n",
        "            queue=False,\n",
        "        )\n",
        "\n",
        "\n",
        "        return {\n",
        "            \"cohort_dropdown\": cohort_dropdown,\n",
        "            \"model_dropdown\": model_dropdown,\n",
        "            \"question_box\": question_box,\n",
        "            \"improved_box\": improved_box,\n",
        "            \"answer_markdown\": answer_md,\n",
        "            \"docs_markdown\": docs_md,\n",
        "        }\n",
        "\n",
        "\n",
        "    # -----------------------------\n",
        "    # History Tab\n",
        "    # -----------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "516cbbd4",
      "metadata": {
        "id": "516cbbd4"
      },
      "source": [
        "## STEP 12 — History tab\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "f1388fa3",
      "metadata": {
        "id": "f1388fa3"
      },
      "outputs": [],
      "source": [
        "def build_history_tab(session_state):\n",
        "        \"\"\"\n",
        "        Simple history viewer using format_history_markdown.\n",
        "        \"\"\"\n",
        "        gr.Markdown(\"### Chat History (read-only)\")\n",
        "\n",
        "        with gr.Row():\n",
        "            history_user_filter = gr.Textbox(\n",
        "                label=\"Filter by User ID (optional)\",\n",
        "                placeholder=\"Leave blank for all users\",\n",
        "            )\n",
        "            history_cohort_filter = gr.Textbox(\n",
        "                label=\"Filter by Cohort (optional)\",\n",
        "                placeholder=\"Leave blank for all cohorts\",\n",
        "            )\n",
        "\n",
        "        with gr.Row():\n",
        "            use_current_user_btn = gr.Button(\"Use Current User\")\n",
        "            refresh_history_btn = gr.Button(\"Refresh History\")\n",
        "\n",
        "        history_md = gr.Markdown()\n",
        "\n",
        "        def _use_current_user(session_user):\n",
        "            return _extract_user_id(session_user) or \"\"\n",
        "\n",
        "        def _refresh_history(session_user, user_id: str, cohort_name: str):\n",
        "            try:\n",
        "                su = require_login(session_user, \"view chat history\")\n",
        "            except PermissionError as e:\n",
        "                return f\"❌ {e}\"\n",
        "\n",
        "            is_admin = su.is_admin\n",
        "\n",
        "            u_raw = (user_id or \"\").strip()\n",
        "            c_raw = (cohort_name or \"\").strip()\n",
        "\n",
        "            # Normalize common \"All\" sentinel values to None\n",
        "            if u_raw.lower() in (\"all users\", \"all\", \"*\"):\n",
        "                u_raw = \"\"\n",
        "            if c_raw.lower() in (\"all cohorts\", \"all\", \"*\"):\n",
        "                c_raw = \"\"\n",
        "\n",
        "            u = u_raw or None\n",
        "            c = c_raw or None\n",
        "\n",
        "            # Non-admins can only view their own history regardless of filter\n",
        "            if not is_admin:\n",
        "                u = su.username\n",
        "\n",
        "            try:\n",
        "                return format_history_markdown(u, c, limit=50)\n",
        "            except Exception as e:\n",
        "                trace_log(f\"HISTORY _refresh_history ERROR: {e}\")\n",
        "                return f\"❌ Error loading history: {e}\"\n",
        "\n",
        "\n",
        "        use_current_user_btn.click(\n",
        "            _use_current_user,\n",
        "            inputs=[session_state],\n",
        "            outputs=[history_user_filter],\n",
        "        )\n",
        "\n",
        "        refresh_history_btn.click(\n",
        "            _refresh_history,\n",
        "            inputs=[session_state, history_user_filter, history_cohort_filter],\n",
        "            outputs=[history_md],\n",
        "        )\n",
        "\n",
        "\n",
        "        return {\n",
        "            \"history_user_filter\": history_user_filter,\n",
        "            \"history_cohort_filter\": history_cohort_filter,\n",
        "            \"history_markdown\": history_md,\n",
        "        }\n",
        "\n",
        "\n",
        "    # -----------------------------\n",
        "    # Admin Tab\n",
        "    # -----------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9801c53d",
      "metadata": {
        "id": "9801c53d"
      },
      "source": [
        "## STEP 13 — Admin tab\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "fbd414ac",
      "metadata": {
        "id": "fbd414ac"
      },
      "outputs": [],
      "source": [
        "def build_admin_tab(session_state, session_memory_state):\n",
        "        \"\"\"\n",
        "        Admin tab showing DB stats, users, cohorts, and audit log.\n",
        "        Fix 3.3: prevent cross-button \"stacking\" by updating all 4 panes each click.\n",
        "        \"\"\"\n",
        "        admin_handles = {}\n",
        "        gr.Markdown(\"### Admin (requires admin role)\")\n",
        "        gr.Markdown(\"You must be logged in as an admin user to see details here.\")\n",
        "        gr.Markdown(\"#### Session Memory (EPIC 3.2 / 3.3)\")\n",
        "        session_mem_enabled_cb = gr.Checkbox(\n",
        "            label=\"Enable session memory (this browser session)\",\n",
        "            value=True,\n",
        "        )\n",
        "        session_mem_status_md = gr.Markdown(\"\")\n",
        "        # EPIC 3.3 — Additive controls (no layout refactor)\n",
        "        session_mem_max_turns = gr.Number(\n",
        "            label=\"Max session memory turns (EPIC 3.3)\",\n",
        "            value=SESSION_MEM_MAX_TURNS,\n",
        "            precision=0,\n",
        "        )\n",
        "        session_mem_scope_radio = gr.Radio(\n",
        "            choices=[\"cohort\", \"session\"],\n",
        "            value=\"cohort\",\n",
        "            label=\"Session memory scope (EPIC 3.3)\",\n",
        "            info=\"cohort = resets on cohort change; session = persists across cohorts\",\n",
        "        )\n",
        "\n",
        "        session_mem_clear_btn = gr.Button(\"Clear Session Memory (EPIC 3.3)\")\n",
        "        session_mem_refresh_btn = gr.Button(\"Refresh Memory Viewer (EPIC 3.3)\")\n",
        "        session_mem_view_md = gr.Markdown(\"_Session memory viewer will appear here._\")\n",
        "\n",
        "        def _format_session_mem_view(mem: dict) -> str:\n",
        "            if not isinstance(mem, dict):\n",
        "                return \"_(no session memory state)_\"\n",
        "            enabled = mem.get('enabled', True)\n",
        "            max_turns = int(mem.get('max_turns') or SESSION_MEM_MAX_TURNS)\n",
        "            turns = mem.get('turns') or []\n",
        "            if not enabled:\n",
        "                return f\"_(session memory OFF; turns cleared; max_turns={max_turns})_\"\n",
        "            if not turns:\n",
        "                return f\"_(session memory ON; empty; max_turns={max_turns})_\"\n",
        "            out = []\n",
        "            out.append(f\"**Turns stored:** {len(turns)}  •  **max_turns:** {max_turns}\")\n",
        "            out.append('')\n",
        "            for i, t in enumerate(turns[-max_turns:], 1):\n",
        "                q = (t.get('q') or '').strip()\n",
        "                a = (t.get('a') or '').strip()\n",
        "                if q:\n",
        "                    out.append(f\"**Q{i}:** {q}\")\n",
        "                if a:\n",
        "                    out.append(f\"**A{i}:** {a}\")\n",
        "                out.append('---')\n",
        "            return '\\n'.join(out).strip()\n",
        "\n",
        "        def _toggle_session_memory(session_user, enabled: bool, mem: dict, max_turns_val):  # EPIC 3.3\n",
        "            su, msg = _ensure_admin(session_user)\n",
        "            if not su:\n",
        "                mem = mem if isinstance(mem, dict) else init_session_memory_state()\n",
        "                return mem, f\"❌ {msg}\", _format_session_mem_view(mem), gr.update(value=int(mem.get(\"max_turns\") or SESSION_MEM_MAX_TURNS))\n",
        "            mem = mem if isinstance(mem, dict) else init_session_memory_state()\n",
        "            mem['enabled'] = bool(enabled)\n",
        "            try:\n",
        "                mem['max_turns'] = int(max_turns_val) if max_turns_val is not None else int(mem.get('max_turns') or SESSION_MEM_MAX_TURNS)\n",
        "            except Exception:\n",
        "                mem['max_turns'] = int(mem.get('max_turns') or SESSION_MEM_MAX_TURNS)\n",
        "            if not mem['enabled']:\n",
        "                mem['turns'] = []\n",
        "            else:\n",
        "                turns = mem.get('turns') or []\n",
        "                if len(turns) > mem['max_turns']:\n",
        "                    mem['turns'] = turns[-mem['max_turns']:]\n",
        "            state_txt = '✅ Session memory is **ON**.' if mem['enabled'] else '🛑 Session memory is **OFF** (cleared).'\n",
        "            return mem, state_txt, _format_session_mem_view(mem), gr.update(value=int(mem.get('max_turns') or SESSION_MEM_MAX_TURNS))\n",
        "\n",
        "        def _clear_session_memory(session_user, mem: dict):\n",
        "            su, msg = _ensure_admin(session_user)\n",
        "            if not su:\n",
        "                mem = mem if isinstance(mem, dict) else init_session_memory_state()\n",
        "                return mem, f\"❌ {msg}\", _format_session_mem_view(mem)\n",
        "            mem = mem if isinstance(mem, dict) else init_session_memory_state()\n",
        "            mem['turns'] = []\n",
        "            return mem, '🧹 Session memory cleared.', _format_session_mem_view(mem)\n",
        "\n",
        "        def _set_session_mem_max_turns(session_user, max_turns_val, mem: dict):\n",
        "            su, msg = _ensure_admin(session_user)\n",
        "            if not su:\n",
        "                mem = mem if isinstance(mem, dict) else init_session_memory_state()\n",
        "                return mem, f\"❌ {msg}\", _format_session_mem_view(mem), gr.update(value=int(mem.get(\"max_turns\") or SESSION_MEM_MAX_TURNS))\n",
        "            mem = mem if isinstance(mem, dict) else init_session_memory_state()\n",
        "            try:\n",
        "                mem['max_turns'] = int(max_turns_val)\n",
        "            except Exception:\n",
        "                mem['max_turns'] = int(mem.get('max_turns') or SESSION_MEM_MAX_TURNS)\n",
        "            turns = mem.get('turns') or []\n",
        "            if len(turns) > mem['max_turns']:\n",
        "                mem['turns'] = turns[-mem['max_turns']:]\n",
        "            return mem, '✅ Updated max_turns.', _format_session_mem_view(mem), gr.update(value=int(mem['max_turns']))\n",
        "\n",
        "        def _set_session_mem_scope(session_user, scope_val, mem: dict):\n",
        "            \"\"\"EPIC 3.3 — Admin can set memory scope (cohort vs session).\"\"\"\n",
        "            su, msg = _ensure_admin(session_user)\n",
        "            if not su:\n",
        "                mem = mem if isinstance(mem, dict) else init_session_memory_state()\n",
        "                return mem, f\"❌ {msg}\", _format_session_mem_view(mem), gr.update(value=(mem.get(\"scope\") or \"cohort\"))\n",
        "            mem = mem if isinstance(mem, dict) else init_session_memory_state()\n",
        "            scope_val = (scope_val or \"cohort\").lower()\n",
        "            if scope_val not in (\"cohort\", \"session\"):\n",
        "                scope_val = \"cohort\"\n",
        "            mem[\"scope\"] = scope_val\n",
        "            status = f\"✅ Session memory scope set to: {scope_val.upper()}.\"\n",
        "            return mem, status, _format_session_mem_view(mem), gr.update(value=scope_val)\n",
        "\n",
        "        def _refresh_session_mem_view(mem: dict):\n",
        "            mem = mem if isinstance(mem, dict) else init_session_memory_state()\n",
        "            return _format_session_mem_view(mem)\n",
        "\n",
        "        session_mem_enabled_cb.change(\n",
        "            _toggle_session_memory,\n",
        "            inputs=[session_state, session_mem_enabled_cb, session_memory_state, session_mem_max_turns],\n",
        "            outputs=[session_memory_state, session_mem_status_md, session_mem_view_md, session_mem_max_turns],\n",
        "            queue=False,\n",
        "            trigger_mode='once',\n",
        "        )\n",
        "\n",
        "        session_mem_clear_btn.click(\n",
        "            _clear_session_memory,\n",
        "            inputs=[session_state, session_memory_state],\n",
        "            outputs=[session_memory_state, session_mem_status_md, session_mem_view_md],\n",
        "            queue=False,\n",
        "        )\n",
        "\n",
        "        session_mem_max_turns.change(\n",
        "            _set_session_mem_max_turns,\n",
        "            inputs=[session_state, session_mem_max_turns, session_memory_state],\n",
        "            outputs=[session_memory_state, session_mem_status_md, session_mem_view_md, session_mem_max_turns],\n",
        "            queue=False,\n",
        "            trigger_mode='once',\n",
        "        )\n",
        "        # EPIC 3.3 — Scope toggle (admin)\n",
        "        session_mem_scope_radio.change(\n",
        "            _set_session_mem_scope,\n",
        "            inputs=[session_state, session_mem_scope_radio, session_memory_state],\n",
        "            outputs=[session_memory_state, session_mem_status_md, session_mem_view_md, session_mem_scope_radio],\n",
        "            queue=False,\n",
        "            trigger_mode='once',\n",
        "        )\n",
        "\n",
        "        session_mem_refresh_btn.click(\n",
        "            _refresh_session_mem_view,\n",
        "            inputs=[session_memory_state],\n",
        "            outputs=[session_mem_view_md],\n",
        "            queue=False,\n",
        "        )\n",
        "\n",
        "        with gr.Row():\n",
        "            refresh_stats_btn = gr.Button(\"DB Stats\")\n",
        "            refresh_users_btn = gr.Button(\"List Users\")\n",
        "            refresh_cohorts_btn = gr.Button(\"List Cohorts\")\n",
        "            refresh_audit_btn = gr.Button(\"Audit Log (last 50)\")\n",
        "\n",
        "        admin_stats_md = gr.Markdown()\n",
        "        admin_users_md = gr.Markdown()\n",
        "        admin_cohorts_md = gr.Markdown()\n",
        "        admin_audit_md = gr.Markdown()\n",
        "\n",
        "        gr.Markdown(\"#### Delete Cohort (admin only)\")\n",
        "        admin_delete_cohort_dropdown = gr.Dropdown(\n",
        "            label=\"Select Cohort to Delete\",\n",
        "            choices=[],\n",
        "        )\n",
        "        admin_delete_btn = gr.Button(\"Delete Selected Cohort\")\n",
        "        admin_delete_status_md = gr.Markdown()\n",
        "        # ============================================================\n",
        "                # ============================================================\n",
        "        # EPIC 5.2 — Admin Chat-Model Controls (enabled + default)\n",
        "        # ============================================================\n",
        "        gr.Markdown(\"### LLM Model Controls (EPIC 5.2)\")\n",
        "        gr.Markdown(\"Admins can enable/disable chat models and select the default model used on the Ask tab.\")\n",
        "\n",
        "        # Use registry if available; fall back to hardcoded list.\n",
        "        try:\n",
        "            all_models = list_all_chat_models()\n",
        "        except Exception:\n",
        "            all_models = [\"gpt-4.1-mini\", \"gpt-5\", \"gpt-5-mini\", \"\"]\n",
        "\n",
        "        try:\n",
        "            enabled_models = get_allowed_chat_models()\n",
        "        except Exception:\n",
        "            enabled_models = list(all_models)\n",
        "\n",
        "        try:\n",
        "            default_model_id = get_default_chat_model_id()\n",
        "        except Exception:\n",
        "            default_model_id = all_models[0] if all_models else \"gpt-4.1-mini\"\n",
        "\n",
        "\n",
        "\n",
        "        # ---- Sanitize model list (fix blank/phantom checkbox issue) ----\n",
        "        # Remove empty/whitespace IDs and de-duplicate while preserving order.\n",
        "        all_models = [m for m in (all_models or []) if isinstance(m, str) and m.strip()]\n",
        "        all_models = list(dict.fromkeys(all_models))\n",
        "\n",
        "        with gr.Row():\n",
        "            admin_chat_models_cbg = gr.CheckboxGroup(\n",
        "                label=\"Enabled chat models\",\n",
        "                choices=all_models,\n",
        "                value=[m for m in (enabled_models or []) if m in all_models],\n",
        "                interactive=True,\n",
        "            )\n",
        "\n",
        "        with gr.Row():\n",
        "            admin_default_chat_model_dd = gr.Dropdown(\n",
        "                label=\"Default chat model\",\n",
        "                choices=[m for m in all_models if m in ((enabled_models or []) or all_models)],\n",
        "                value=(default_model_id if default_model_id in all_models else (all_models[0] if all_models else None)),\n",
        "                interactive=True,\n",
        "            )\n",
        "            admin_models_save_btn = gr.Button(\"Save Model Settings\", variant=\"primary\")\n",
        "\n",
        "        admin_models_status_md = gr.Markdown(\"\")\n",
        "\n",
        "        def _admin_save_model_settings(session_user, enabled_list, default_id):\n",
        "            su, msg = _ensure_admin(session_user)\n",
        "            if not su:\n",
        "                return f\"❌ {msg}\"\n",
        "\n",
        "            enabled_list = enabled_list or []\n",
        "            # Ensure default is enabled\n",
        "            if default_id and default_id not in enabled_list:\n",
        "                enabled_list = list(enabled_list) + [default_id]\n",
        "\n",
        "            try:\n",
        "                ensure_model_registry_table()\n",
        "            except Exception as e:\n",
        "                trace_log(f\"ADMIN model settings ensure table ERROR: {e}\")\n",
        "\n",
        "            # Persist enable flags\n",
        "            for mid in all_models:\n",
        "                try:\n",
        "                    add_or_update_model(\n",
        "                        model_type=\"chat\",\n",
        "                        model_id=mid,\n",
        "                        enabled=(mid in enabled_list),\n",
        "                        is_default=False,\n",
        "                    )\n",
        "                except Exception as e:\n",
        "                    trace_log(f\"ADMIN add_or_update_model ERROR model={mid}: {e}\")\n",
        "\n",
        "            # Persist default\n",
        "            try:\n",
        "                if default_id:\n",
        "                    set_default_model(\"chat\", default_id)\n",
        "            except Exception as e:\n",
        "                trace_log(f\"ADMIN set_default_model ERROR: {e}\")\n",
        "\n",
        "            try:\n",
        "                log_audit(su.username, su.role, \"set_chat_models\", f\"enabled={enabled_list} default={default_id}\")\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "            # Return updated UI state\n",
        "            new_enabled = [m for m in enabled_list if m in all_models]\n",
        "            new_default_choices = [m for m in all_models if m in new_enabled] or list(all_models)\n",
        "            new_default = default_id if (default_id in new_default_choices) else (new_default_choices[0] if new_default_choices else None)\n",
        "\n",
        "            return (\n",
        "                \"✅ Model settings saved.\",\n",
        "                gr.update(value=new_enabled),\n",
        "                gr.update(choices=new_default_choices, value=new_default),\n",
        "            )\n",
        "\n",
        "        admin_models_save_btn.click(\n",
        "            _admin_save_model_settings,\n",
        "            inputs=[session_state, admin_chat_models_cbg, admin_default_chat_model_dd],\n",
        "            outputs=[admin_models_status_md, admin_chat_models_cbg, admin_default_chat_model_dd],\n",
        "            queue=False,\n",
        "        )\n",
        "\n",
        "# --- Demo Setup (v16.1) ---\n",
        "        gr.Markdown(\"### Demo Setup (admin)\")\n",
        "        gr.Markdown(\"Upload a prompt list to preload curated 'FAPs' for a cohort. These appear in the Ask tab as a dropdown when that cohort is selected.\")\n",
        "\n",
        "        with gr.Row():\n",
        "            demo_cohort_dropdown = gr.Dropdown(label=\"Cohort for FAPs\", choices=[], value=None)\n",
        "            demo_refresh_cohorts_btn = gr.Button(\"Refresh Cohorts\")\n",
        "\n",
        "        with gr.Row():\n",
        "            demo_prompts_file = gr.File(\n",
        "                label=\"Upload FAPs File (.txt, .csv, .json)\",\n",
        "                file_count=\"single\",\n",
        "                type=\"filepath\",\n",
        "            )\n",
        "            demo_replace_checkbox = gr.Checkbox(label=\"Replace existing FAPs\", value=True)\n",
        "\n",
        "        with gr.Row():\n",
        "            demo_load_btn = gr.Button(\"Load FAPs\", variant=\"primary\")\n",
        "            demo_view_btn = gr.Button(\"View FAPs\")\n",
        "            demo_clear_btn = gr.Button(\"Clear FAPs\")\n",
        "\n",
        "        demo_prompts_status = gr.Markdown(\"\")\n",
        "        demo_prompts_preview = gr.Markdown(\"\")\n",
        "        # --- Demo Prompt Notes (v17.0 / Epic 1.2) ---\n",
        "        gr.Markdown(\"#### Demo Prompt Notes (admin only)\")\n",
        "\n",
        "        with gr.Row():\n",
        "            demo_prompt_select_dd = gr.Dropdown(\n",
        "                label=\"Select a Demo Prompt to add notes\",\n",
        "                choices=[],\n",
        "                value=None,\n",
        "                interactive=True,\n",
        "            )\n",
        "\n",
        "        demo_prompt_note_tb = gr.Textbox(\n",
        "            label=\"Admin Note (Presenter Only)\",\n",
        "            lines=4,\n",
        "            placeholder=\"Only visible to admins in Demo Setup. Not shown in Ask tab.\",\n",
        "        )\n",
        "\n",
        "        with gr.Row():\n",
        "            demo_prompt_note_save_btn = gr.Button(\"Save Note\", variant=\"primary\")\n",
        "            demo_prompt_note_refresh_btn = gr.Button(\"Refresh Prompt List\")\n",
        "\n",
        "\n",
        "\n",
        "        def _demo_refresh_cohorts_dd(session_user):\n",
        "            su, msg = _ensure_admin(session_user)\n",
        "            if not su:\n",
        "                return gr.update(choices=[], value=None)\n",
        "            try:\n",
        "                rows = get_all_cohorts()\n",
        "                names = [r[0] for r in rows if r and r[0]]\n",
        "                return gr.update(choices=names, value=None)\n",
        "            except Exception as e:\n",
        "                trace_log(f\"ADMIN demo _demo_refresh_cohorts_dd ERROR: {e}\")\n",
        "                return gr.update(choices=[], value=None)\n",
        "\n",
        "        def _demo_load_prompts_cb(session_user, cohort_name: str, file_path: str, replace: bool):\n",
        "            su, msg = _ensure_admin(session_user)\n",
        "            if not su:\n",
        "                return msg or \"❌ Admin privileges required.\", \"\"\n",
        "            c = (cohort_name or \"\").strip()\n",
        "            if not c:\n",
        "                return \"❌ Please select a cohort.\", \"\"\n",
        "            if not file_path:\n",
        "                return \"❌ Please upload a .txt/.csv/.json prompt file.\", \"\"\n",
        "            prompts = parse_demo_prompts_file(file_path)\n",
        "            n = upsert_demo_prompts_for_cohort(c, prompts, replace=bool(replace))\n",
        "            try:\n",
        "                log_audit(su.username, su.role, \"demo_prompts_load\", f\"cohort={c} count={n} replace={bool(replace)}\")\n",
        "            except Exception as e:\n",
        "                trace_log(f\"log_audit demo_prompts_load ERROR: {e}\")\n",
        "            preview = list_demo_prompts_for_cohort(c)\n",
        "            preview_md = \"\\n\".join([f\"- {p}\" for p in preview[:50]]) if preview else \"_No FAPs found._\"\n",
        "            return f\"✅ Loaded {n} demo prompt(s) into cohort `{c}`.\", preview_md\n",
        "\n",
        "        def _demo_view_prompts_cb(session_user, cohort_name: str):\n",
        "            su, msg = _ensure_admin(session_user)\n",
        "            if not su:\n",
        "                return msg or \"❌ Admin privileges required.\", \"\"\n",
        "            c = (cohort_name or \"\").strip()\n",
        "            if not c:\n",
        "                return \"❌ Please select a cohort.\", \"\"\n",
        "            preview = list_demo_prompts_for_cohort(c)\n",
        "            if not preview:\n",
        "                return f\"_No FAPs found for `{c}`._\", \"\"\n",
        "            md = \"\\n\".join([f\"- {p}\" for p in preview[:200]])\n",
        "            return f\"**Demo prompts for `{c}` ({len(preview)}):**\", md\n",
        "\n",
        "        def _demo_clear_prompts_cb(session_user, cohort_name: str):\n",
        "            su, msg = _ensure_admin(session_user)\n",
        "            if not su:\n",
        "                return msg or \"❌ Admin privileges required.\", \"\"\n",
        "            c = (cohort_name or \"\").strip()\n",
        "            if not c:\n",
        "                return \"❌ Please select a cohort.\", \"\"\n",
        "            clear_demo_prompts_for_cohort(c)\n",
        "            try:\n",
        "                log_audit(su.username, su.role, \"demo_prompts_clear\", f\"cohort={c}\")\n",
        "            except Exception as e:\n",
        "                trace_log(f\"log_audit demo_prompts_clear ERROR: {e}\")\n",
        "            return f\"✅ Cleared FAPs for `{c}`.\", \"\"\n",
        "\n",
        "        def _demo_list_prompts_for_notes(session_user, cohort_name: str):\n",
        "          su, msg = _ensure_admin(session_user)\n",
        "          if not su:\n",
        "              return gr.update(choices=[], value=None)\n",
        "          c = (cohort_name or \"\").strip()\n",
        "          if not c:\n",
        "              return gr.update(choices=[], value=None)\n",
        "\n",
        "          rows = list_demo_prompts_with_notes(c)  # returns [(id, prompt_text, admin_note), ...]\n",
        "          # Dropdown values must be stable; use the ID as the value.\n",
        "          choices = [(r[1], str(r[0])) for r in rows]  # (label, value)\n",
        "          return gr.update(choices=choices, value=None)\n",
        "\n",
        "\n",
        "        def _demo_load_note_into_box(session_user, cohort_name: str, prompt_id: str):\n",
        "            su, msg = _ensure_admin(session_user)\n",
        "            if not su:\n",
        "                return \"\"\n",
        "            c = (cohort_name or \"\").strip()\n",
        "            if not c or not prompt_id:\n",
        "                return \"\"\n",
        "\n",
        "            pid = int(prompt_id)\n",
        "            rows = list_demo_prompts_with_notes(c)\n",
        "            for (rid, text, note) in rows:\n",
        "                if rid == pid:\n",
        "                    return note or \"\"\n",
        "            return \"\"\n",
        "\n",
        "\n",
        "        def _demo_save_note_cb(session_user, cohort_name: str, prompt_id: str, note_text: str):\n",
        "            su, msg = _ensure_admin(session_user)\n",
        "            if not su:\n",
        "                return msg or \"❌ Admin privileges required.\"\n",
        "\n",
        "            c = (cohort_name or \"\").strip()\n",
        "            if not c:\n",
        "                return \"❌ Please select a cohort.\"\n",
        "            if not prompt_id:\n",
        "                return \"❌ Please select a demo prompt.\"\n",
        "\n",
        "            try:\n",
        "                save_demo_prompt_note(int(prompt_id), note_text or \"\")\n",
        "                log_audit(su.username, su.role, \"demo_prompt_note_save\", f\"cohort={c} prompt_id={prompt_id}\")\n",
        "                return \"✅ Admin note saved.\"\n",
        "            except Exception as e:\n",
        "                trace_log(f\"ADMIN demo note save ERROR: {e}\")\n",
        "                return f\"❌ Error saving note: {e}\"\n",
        "\n",
        "\n",
        "        demo_refresh_cohorts_btn.click(\n",
        "            _demo_refresh_cohorts_dd,\n",
        "            inputs=[session_state],\n",
        "            outputs=[demo_cohort_dropdown],\n",
        "        )\n",
        "\n",
        "        demo_load_btn.click(\n",
        "            _demo_load_prompts_cb,\n",
        "            inputs=[session_state, demo_cohort_dropdown, demo_prompts_file, demo_replace_checkbox],\n",
        "            outputs=[demo_prompts_status, demo_prompts_preview],\n",
        "        )\n",
        "\n",
        "\n",
        "        demo_view_btn.click(\n",
        "            _demo_view_prompts_cb,\n",
        "            inputs=[session_state, demo_cohort_dropdown],\n",
        "            outputs=[demo_prompts_status, demo_prompts_preview],\n",
        "        )\n",
        "\n",
        "        demo_clear_btn.click(\n",
        "            _demo_clear_prompts_cb,\n",
        "            inputs=[session_state, demo_cohort_dropdown],\n",
        "            outputs=[demo_prompts_status, demo_prompts_preview],\n",
        "        )\n",
        "\n",
        "        # Refresh prompt list for selected cohort (for notes dropdown)\n",
        "        demo_prompt_note_refresh_btn.click(\n",
        "            _demo_list_prompts_for_notes,\n",
        "            inputs=[session_state, demo_cohort_dropdown],\n",
        "            outputs=[demo_prompt_select_dd],\n",
        "        )\n",
        "\n",
        "        # Also refresh prompt list when cohort changes\n",
        "        demo_cohort_dropdown.change(\n",
        "            _demo_list_prompts_for_notes,\n",
        "            inputs=[session_state, demo_cohort_dropdown],\n",
        "            outputs=[demo_prompt_select_dd],\n",
        "        )\n",
        "\n",
        "        # When an admin selects a prompt, load its note into the textbox\n",
        "        demo_prompt_select_dd.change(\n",
        "            _demo_load_note_into_box,\n",
        "            inputs=[session_state, demo_cohort_dropdown, demo_prompt_select_dd],\n",
        "            outputs=[demo_prompt_note_tb],\n",
        "        )\n",
        "\n",
        "        # Save note\n",
        "        demo_prompt_note_save_btn.click(\n",
        "            _demo_save_note_cb,\n",
        "            inputs=[session_state, demo_cohort_dropdown, demo_prompt_select_dd, demo_prompt_note_tb],\n",
        "            outputs=[demo_prompts_status],\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"admin_stats_md\": admin_stats_md,\n",
        "            \"admin_users_md\": admin_users_md,\n",
        "            \"admin_cohorts_md\": admin_cohorts_md,\n",
        "            \"admin_audit_md\": admin_audit_md,\n",
        "            \"admin_delete_cohort_dropdown\": admin_delete_cohort_dropdown,\n",
        "            \"admin_delete_status_md\": admin_delete_status_md,\n",
        "\n",
        "            \"demo_cohort_dropdown\": demo_cohort_dropdown,\n",
        "\n",
        "            # Epic 3.2 – Session Memory\n",
        "            \"session_mem_enabled_cb\": session_mem_enabled_cb,\n",
        "            \"session_mem_status_md\": session_mem_status_md,\n",
        "            \"session_mem_max_turns\": session_mem_max_turns,  # EPIC 3.3\n",
        "            \"session_mem_clear_btn\": session_mem_clear_btn,\n",
        "            \"session_mem_refresh_btn\": session_mem_refresh_btn,\n",
        "            \"session_mem_view_md\": session_mem_view_md,\n",
        "\n",
        "        }\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18512a32",
      "metadata": {
        "id": "18512a32"
      },
      "source": [
        "## STEP 14 — Interface assembly & launch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "ca440d65",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 619
        },
        "id": "ca440d65",
        "outputId": "c86e8d9b-5afc-43f8-ee2b-d4b4b6985bc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Note: opening Chrome Inspector may crash demo inside Colab notebooks.\n",
            "* To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "                        if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "                            return;\n",
              "                        }\n",
              "                        element.appendChild(document.createTextNode(''));\n",
              "                        const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "\n",
              "                        const external_link = document.createElement('div');\n",
              "                        external_link.innerHTML = `\n",
              "                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">\n",
              "                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">\n",
              "                                    https://localhost:${port}${path}\n",
              "                                </a>\n",
              "                            </div>\n",
              "                        `;\n",
              "                        element.appendChild(external_link);\n",
              "\n",
              "                        const iframe = document.createElement('iframe');\n",
              "                        iframe.src = new URL(path, url).toString();\n",
              "                        iframe.height = height;\n",
              "                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n",
              "                        iframe.width = width;\n",
              "                        iframe.style.border = 0;\n",
              "                        element.appendChild(iframe);\n",
              "                    })(7860, \"/\", \"100%\", 500, false, window.element)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n"
          ]
        }
      ],
      "source": [
        "ACTION_CREATE = \"Create new cohort from files\"\n",
        "ACTION_APPEND = \"Append files to existing cohort\"\n",
        "ACTION_CLONE  = \"Clone an existing cohort\"\n",
        "\n",
        "DEFAULT_ACTION = ACTION_CREATE\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Build Gradio Interface\n",
        "# -----------------------------\n",
        "def build_interface():\n",
        "    with gr.Blocks(title=\"Phase1 RAG MVP v18 EPIC5.1\") as demo:\n",
        "        gr.Markdown(\"## Phase1 RAG MVP v18 EPIC5.1\")\n",
        "\n",
        "        # Global session state\n",
        "        session_state = gr.State(value=None)       # {\"username\": ..., \"role\": ...} or None\n",
        "        role_state = gr.State(value=\"anonymous\")   # Optional, not heavily used yet\n",
        "        api_key_state = gr.State(value=\"\")         # Hidden backing store for API key\n",
        "\n",
        "        session_memory_state = gr.State(value=init_session_memory_state())\n",
        "\n",
        "        # ---- Login / Logout row ----\n",
        "        with gr.Row():\n",
        "            username_tb = gr.Textbox(label=\"Username\", scale=1)\n",
        "            password_tb = gr.Textbox(label=\"Password\", type=\"password\", scale=1)\n",
        "            api_key_tb = gr.Textbox(\n",
        "                label=\"OpenAI API Key\",\n",
        "                type=\"password\",\n",
        "                placeholder=\"sk-...\",\n",
        "                scale=2,\n",
        "            )\n",
        "\n",
        "        with gr.Row():\n",
        "            login_btn = gr.Button(\"Login\")\n",
        "            logout_btn = gr.Button(\"Logout\")\n",
        "\n",
        "        login_status = gr.Markdown(\"Not logged in.\")\n",
        "        current_user_label = gr.Markdown(\"**Current user:** (none)\")\n",
        "\n",
        "        # ---- Tabs ----\n",
        "        with gr.Tabs():\n",
        "            with gr.Tab(\"Setup & Cohorts\"):\n",
        "                setup_handles = build_setup_tab(session_state, api_key_state)\n",
        "            with gr.Tab(\"About\"):\n",
        "                gr.Markdown(ABOUT_TOOL_MD)\n",
        "\n",
        "            with gr.Tab(\"Ask\"):\n",
        "                ask_handles = build_ask_tab(session_state, api_key_state, session_memory_state)\n",
        "            with gr.Tab(\"History\"):\n",
        "                history_handles = build_history_tab(session_state)\n",
        "            # Admin tab: hidden by default\n",
        "            admin_tab = gr.Tab(\"Admin\", visible=False)\n",
        "            with admin_tab:\n",
        "                admin_handles = build_admin_tab(session_state, session_memory_state)\n",
        "\n",
        "        # Auto-refresh Admin dropdowns when the Admin tab is selected (v18 fix)\n",
        "        def _admin_tab_select_refresh(session_user):\n",
        "            # Populate Admin dropdowns on tab select.\n",
        "            # session_user is the session_state dict: {\"username\": ..., \"role\": ...}\n",
        "            if not session_user or session_user.get(\"role\") != \"admin\":\n",
        "                return gr.update(choices=[], value=None), gr.update(choices=[], value=None)\n",
        "            try:\n",
        "                names = [row[0] for row in get_all_cohorts()]\n",
        "            except Exception:\n",
        "                names = []\n",
        "            return gr.update(choices=names, value=None), gr.update(choices=names, value=None)\n",
        "\n",
        "        admin_tab.select(\n",
        "            _admin_tab_select_refresh,\n",
        "            inputs=[session_state],\n",
        "            outputs=[\n",
        "                admin_handles[\"admin_delete_cohort_dropdown\"],\n",
        "                admin_handles[\"demo_cohort_dropdown\"],\n",
        "            ],\n",
        "            queue=False,\n",
        "        )\n",
        "        # ---- Ask tab enable / disable JS (Fix Set 1) ----\n",
        "        ASK_DISABLE_JS = \"\"\"\n",
        "        (session_state, role_state, api_key_state, session_memory_state) => {\n",
        "          const setDisabled = (containerId, disabled) => {\n",
        "            const container = document.getElementById(containerId);\n",
        "            if (!container) return;\n",
        "            const el = container.querySelector(\"textarea, input, button, select\");\n",
        "            if (!el) return;\n",
        "            el.disabled = disabled;\n",
        "          };\n",
        "\n",
        "          setDisabled(\"ask_cohort_dropdown\", true);\n",
        "          setDisabled(\"ask_model_dropdown\", true);\n",
        "          setDisabled(\"ask_question_box\", true);\n",
        "          setDisabled(\"ask_improved_box\", true);\n",
        "          setDisabled(\"ask_improve_btn\", true);\n",
        "          setDisabled(\"ask_ask_btn\", true);\n",
        "          setDisabled(\"ask_refresh_btn\", true);\n",
        "              setDisabled(\"ask_refresh_models_btn\", true);\n",
        "\n",
        "          // CRITICAL: return inputs unchanged\n",
        "          return [session_state, role_state, api_key_state, session_memory_state];\n",
        "        }\n",
        "        \"\"\"\n",
        "\n",
        "        ASK_ENABLE_JS = \"\"\"\n",
        "        (username, password, api_key, session_state, role_state, api_key_state, session_memory_state) => {\n",
        "          const setDisabled = (containerId, disabled) => {\n",
        "            const container = document.getElementById(containerId);\n",
        "            if (!container) return;\n",
        "            const el = container.querySelector(\"textarea, input, button, select\");\n",
        "            if (!el) return;\n",
        "            el.disabled = disabled;\n",
        "          };\n",
        "\n",
        "          setDisabled(\"ask_cohort_dropdown\", false);\n",
        "          setDisabled(\"ask_model_dropdown\", false);\n",
        "          setDisabled(\"ask_question_box\", false);\n",
        "          setDisabled(\"ask_improved_box\", false);\n",
        "          setDisabled(\"ask_improve_btn\", false);\n",
        "          setDisabled(\"ask_ask_btn\", false);\n",
        "          setDisabled(\"ask_refresh_btn\", false);\n",
        "              setDisabled(\"ask_refresh_models_btn\", false);\n",
        "\n",
        "          // CRITICAL: return inputs unchanged so Python receives them\n",
        "          return [username, password, api_key, session_state, role_state, api_key_state, session_memory_state];\n",
        "        }\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "        # ---- Login / Logout Callbacks ----\n",
        "        # ---- Login / Logout Callbacks ----\n",
        "\n",
        "        def _login_cb(username, password, api_key, current_session, current_role, current_api_key, session_mem):\n",
        "            \"\"\"\n",
        "            Authenticate user and store session + API key in state.\n",
        "            Also auto-populates cohort dropdowns for this user.\n",
        "\n",
        "            POLICY:\n",
        "              - API key is stored only in api_key_state (memory), never left in a textbox.\n",
        "              - On ANY login attempt (success or failure), clear:\n",
        "                  Username, Password, API Key textbox, Question, Improved Prompt\n",
        "            \"\"\"\n",
        "            # print(\"LOGIN RAW:\", repr(username), repr(password), repr(api_key))\n",
        "\n",
        "            #print(\"DEBUG: login clicked\")\n",
        "\n",
        "            # Normalize inputs\n",
        "            username = (username or \"\").strip()\n",
        "            password = password or \"\"\n",
        "            api_key = (api_key or \"\").strip()\n",
        "\n",
        "            # Always-clear values for UI textboxes (return plain strings for reliability)\n",
        "            CLEAR = \"\"\n",
        "            clear_username = CLEAR\n",
        "            clear_password = CLEAR\n",
        "            clear_api_key_tb = CLEAR\n",
        "            clear_question = CLEAR\n",
        "            clear_improved = CLEAR\n",
        "            reset_session_mem = init_session_memory_state()\n",
        "\n",
        "            # Basic validation\n",
        "            if not username or not password:\n",
        "                return (\n",
        "                    \"❌ Username and password are required.\",  # login_status\n",
        "                    None,                                     # session_state\n",
        "                    \"anonymous\",                               # role_state\n",
        "                    \"\",                                       # api_key_state\n",
        "                    clear_username,                            # username_tb\n",
        "                    clear_password,                            # password_tb\n",
        "                    clear_api_key_tb,                          # api_key_tb\n",
        "                    \"**Current user:** (none)\",                # current_user_label\n",
        "                    gr.update(choices=[], value=None),         # setup existing cohort dropdown\n",
        "                    gr.update(choices=[], value=None),         # ask cohort dropdown\n",
        "                        gr.update(choices=list_chat_models(), value=get_default_chat_model_id()),         # ask model dropdown\n",
        "                    clear_question,                            # ask question_box\n",
        "                    clear_improved,                            # ask improved_box\n",
        "                    reset_session_mem,\n",
        "                )\n",
        "\n",
        "                            # EPIC 3.3.2 — Guard against unknown usernames to avoid downstream errors\n",
        "            if username and username not in USERS:\n",
        "                return (\n",
        "                    f\"❌ Unknown user `{username}`. Use `admin` or `demo1`.\",  # login_status\n",
        "                    None,                                     # session_state\n",
        "                    \"anonymous\",                               # role_state\n",
        "                    \"\",                                       # api_key_state\n",
        "                    clear_username,                            # username_tb\n",
        "                    clear_password,                            # password_tb\n",
        "                    clear_api_key_tb,                          # api_key_tb\n",
        "                    \"**Current user:** (none)\",                # current_user_label\n",
        "                    gr.update(choices=[], value=None),         # setup existing cohort dropdown\n",
        "                    gr.update(choices=[], value=None),         # ask cohort dropdown\n",
        "                        gr.update(choices=list_chat_models(), value=get_default_chat_model_id()),         # ask model dropdown\n",
        "                    clear_question,                            # ask question_box\n",
        "                    clear_improved,                            # ask improved_box\n",
        "                    gr.update(visible=False),                  # admin_tab\n",
        "                    reset_session_mem,                         # session_memory_state\n",
        "                )\n",
        "\n",
        "            # API key is required and must be valid BEFORE allowing login.\n",
        "            if not api_key or not str(api_key).strip():\n",
        "                return (\n",
        "                    \"❌ A valid OpenAI API key is required to log in.\",  # login_status\n",
        "                    None,                                              # session_state\n",
        "                    \"anonymous\",                                        # role_state\n",
        "                    \"\",                                                # api_key_state\n",
        "                    clear_username,                                     # username_tb\n",
        "                    clear_password,                                     # password_tb\n",
        "                    clear_api_key_tb,                                   # api_key_tb\n",
        "                    \"**Current user:** (none)\",                         # current_user_label\n",
        "                    gr.update(choices=[], value=None),                  # setup existing cohort dropdown\n",
        "                    gr.update(choices=[], value=None),                  # ask cohort dropdown\n",
        "                        gr.update(choices=list_chat_models(), value=get_default_chat_model_id()),         # ask model dropdown\n",
        "                    clear_question,                                     # ask question_box\n",
        "                    clear_improved,                                     # ask improved_box\n",
        "                    gr.update(visible=False),                           # admin_tab\n",
        "                    reset_session_mem,                                  # session_memory_state\n",
        "                )\n",
        "\n",
        "            # Validate key using a lightweight chat+embedding probe.\n",
        "            try:\n",
        "                _chat_model_for_validation = select_chat_model(\n",
        "                    task_type=\"rag_answer\",\n",
        "                    context_size=0,\n",
        "                    user_pref=None,\n",
        "                )\n",
        "                _embed_model_for_validation = DEFAULT_EMBED_MODEL\n",
        "                _key_status = validate_openai_key_and_models(\n",
        "                    api_key=api_key.strip(),\n",
        "                    chat_model=_chat_model_for_validation,\n",
        "                    embed_model=_embed_model_for_validation,\n",
        "                )\n",
        "            except Exception as e:\n",
        "                _key_status = f\"❌ Error validating key: {e}\"\n",
        "\n",
        "            if not str(_key_status).lstrip().startswith(\"✅\"):\n",
        "                return (\n",
        "                    f\"❌ Invalid OpenAI API key. {_key_status}\",         # login_status\n",
        "                    None,                                               # session_state\n",
        "                    \"anonymous\",                                         # role_state\n",
        "                    \"\",                                                 # api_key_state\n",
        "                    clear_username,                                      # username_tb\n",
        "                    clear_password,                                      # password_tb\n",
        "                    clear_api_key_tb,                                    # api_key_tb\n",
        "                    \"**Current user:** (none)\",                          # current_user_label\n",
        "                    gr.update(choices=[], value=None),                   # setup existing cohort dropdown\n",
        "                    gr.update(choices=[], value=None),                   # ask cohort dropdown\n",
        "                        gr.update(choices=list_chat_models(), value=get_default_chat_model_id()),         # ask model dropdown\n",
        "                    clear_question,                                      # ask question_box\n",
        "                    clear_improved,                                      # ask improved_box\n",
        "                    gr.update(visible=False),                            # admin_tab\n",
        "                    reset_session_mem,                                   # session_memory_state\n",
        "                )\n",
        "\n",
        "            user_dict = authenticate_credentials(username, password)\n",
        "            if not user_dict:\n",
        "                return (\n",
        "                    \"❌ Invalid username or password.\",         # login_status\n",
        "                    None,                                     # session_state\n",
        "                    \"anonymous\",                               # role_state\n",
        "                    \"\",                                       # api_key_state\n",
        "                    clear_username,                            # username_tb\n",
        "                    clear_password,                            # password_tb\n",
        "                    clear_api_key_tb,                          # api_key_tb\n",
        "                    \"**Current user:** (none)\",                # current_user_label\n",
        "                    gr.update(choices=[], value=None),         # setup existing cohort dropdown\n",
        "                    gr.update(choices=[], value=None),         # ask cohort dropdown\n",
        "                        gr.update(choices=list_chat_models(), value=get_default_chat_model_id()),         # ask model dropdown\n",
        "                    clear_question,                            # ask question_box\n",
        "                    clear_improved,                            # ask improved_box\n",
        "                    reset_session_mem,\n",
        "                    gr.update(visible=False)   # admin_tab\n",
        "\n",
        "                )\n",
        "\n",
        "            role = user_dict.get(\"role\", \"user\")\n",
        "            status = f\"✅ Logged in as `{username}` (role: `{role}`)\"\n",
        "            label = f\"**Current user:** `{username}` (role: `{role}`)\"\n",
        "\n",
        "            # Store API key ONLY in state (memory); do not leave it in textbox\n",
        "            new_api_key_state = api_key\n",
        "\n",
        "            # Populate cohorts for this user (respects sharing rules)\n",
        "            try:\n",
        "                cohort_names = get_cohorts_for_user(username)\n",
        "            except Exception as e:\n",
        "                trace_log(f\"LOGIN get_cohorts_for_user ERROR: {e}\")\n",
        "                try:\n",
        "                    cohort_names = list_cohorts()\n",
        "                except Exception:\n",
        "                    cohort_names = []\n",
        "\n",
        "            # Build \"name/owner\" labels for the Setup tab dropdown\n",
        "            try:\n",
        "                rows = get_all_cohorts()  # (cohort_name, owner_user_id, ...)\n",
        "                owner_by_name = {row[0]: (row[1] if len(row) > 1 else None) for row in rows}\n",
        "                setup_labels = []\n",
        "                for name in cohort_names:\n",
        "                    owner = owner_by_name.get(name)\n",
        "                    setup_labels.append(f\"{name}/{owner}\" if owner else name)\n",
        "            except Exception as e:\n",
        "                trace_log(f\"LOGIN building setup_labels ERROR: {e}\")\n",
        "                setup_labels = cohort_names\n",
        "\n",
        "            setup_existing_dd = gr.update(choices=setup_labels, value=None)\n",
        "            ask_cohort_dd = gr.update(choices=cohort_names, value=None)\n",
        "\n",
        "            # Populate chat models from registry (EPIC 5.2)\n",
        "            ask_model_dd = gr.update(choices=list_chat_models(), value=get_default_chat_model_id())\n",
        "\n",
        "            return (\n",
        "                status,                 # login_status\n",
        "                user_dict,              # session_state\n",
        "                role,                   # role_state\n",
        "                new_api_key_state,      # api_key_state\n",
        "                clear_username,         # username_tb (cleared after login)\n",
        "                clear_password,         # password_tb (cleared after login)\n",
        "                clear_api_key_tb,       # api_key_tb (cleared after login)\n",
        "                label,                  # current_user_label\n",
        "                setup_existing_dd,      # setup existing cohort dropdown\n",
        "                ask_cohort_dd,          # ask cohort dropdown\n",
        "                    ask_model_dd,          # ask model dropdown\n",
        "                clear_question,         # ask question_box (cleared after login)\n",
        "                clear_improved,# ask improved_box (cleared after login)\n",
        "                gr.update(visible=(role == \"admin\")),  # NEW admin_tab visibility\n",
        "                reset_session_mem,\n",
        "            )\n",
        "\n",
        "        login_btn.click(\n",
        "            _login_cb,\n",
        "            inputs=[\n",
        "                username_tb,\n",
        "                password_tb,\n",
        "                api_key_tb,\n",
        "                session_state,\n",
        "                role_state,\n",
        "                api_key_state,\n",
        "                session_memory_state,  # EPIC 3.1\n",
        "            ],\n",
        "            queue=False,\n",
        "            js=ASK_ENABLE_JS,\n",
        "\n",
        "            outputs=[\n",
        "                login_status,\n",
        "                session_state,\n",
        "                role_state,\n",
        "                api_key_state,\n",
        "                username_tb,\n",
        "                password_tb,\n",
        "                api_key_tb,\n",
        "                current_user_label,\n",
        "                setup_handles[\"existing_cohort_dropdown\"],\n",
        "                ask_handles[\"cohort_dropdown\"],\n",
        "                    ask_handles[\"model_dropdown\"],\n",
        "                ask_handles[\"question_box\"],\n",
        "                ask_handles[\"improved_box\"],\n",
        "                admin_tab,   # NEW\n",
        "                session_memory_state,      # EPIC 3.1\n",
        "            ],\n",
        "\n",
        "        )\n",
        "\n",
        "\n",
        "        def _logout_cb(_session, _role, _api_key, _session_mem):\n",
        "            \"\"\"Logout: clear UI + in-memory state (no persistence changes).\"\"\"\n",
        "            # Reset in-memory state\n",
        "            new_session = None\n",
        "            new_role = \"guest\"\n",
        "            new_api_key = \"\"\n",
        "            reset_session_mem = init_session_memory_state()\n",
        "\n",
        "            CLEAR = \"\"\n",
        "\n",
        "            # Setup tab resets\n",
        "            reset_action = gr.update(value=DEFAULT_ACTION)\n",
        "            reset_new_name = CLEAR\n",
        "            reset_share = gr.update(value=False)\n",
        "            reset_allow_clone = gr.update(value=False)\n",
        "            reset_existing_dd = gr.update(choices=[], value=None)\n",
        "            reset_files = gr.update(value=None)\n",
        "            reset_build_status = CLEAR\n",
        "\n",
        "            # Ask tab resets\n",
        "            reset_ask_cohort = gr.update(choices=[], value=None)\n",
        "            reset_model_dd = gr.update(choices=list_chat_models(), value=get_default_chat_model_id())\n",
        "            reset_question = CLEAR\n",
        "            reset_improved = CLEAR\n",
        "            reset_answer = CLEAR\n",
        "            reset_docs = CLEAR\n",
        "\n",
        "            # History resets\n",
        "            reset_hist_user = gr.update(value=\"\")\n",
        "            reset_hist_cohort = gr.update(value=\"\")\n",
        "            reset_hist_md = CLEAR\n",
        "\n",
        "            # Admin resets\n",
        "            reset_admin_md = CLEAR\n",
        "            reset_admin_dd = gr.update(choices=[], value=None)\n",
        "            reset_admin_status = CLEAR\n",
        "\n",
        "            return (\n",
        "                \"🔒 Logged out.\",          # login_status\n",
        "                new_session,              # session_state\n",
        "                new_role,                 # role_state\n",
        "                new_api_key,              # api_key_state\n",
        "                CLEAR,                    # username_tb\n",
        "                CLEAR,                    # password_tb\n",
        "                CLEAR,                    # api_key_tb\n",
        "                \"**Current user:** (none)\",\n",
        "\n",
        "                # Setup & Cohorts tab\n",
        "                reset_action,\n",
        "                reset_new_name,\n",
        "                reset_share,\n",
        "                reset_allow_clone,\n",
        "                reset_existing_dd,\n",
        "                reset_files,\n",
        "                reset_build_status,\n",
        "\n",
        "                # Ask tab\n",
        "                reset_ask_cohort,\n",
        "                reset_model_dd,\n",
        "                reset_question,\n",
        "                reset_improved,\n",
        "                reset_answer,\n",
        "                reset_docs,\n",
        "\n",
        "                # History tab\n",
        "                reset_hist_user,\n",
        "                reset_hist_cohort,\n",
        "                reset_hist_md,\n",
        "\n",
        "                # Admin tab panes + delete dropdown\n",
        "                reset_admin_md,\n",
        "                reset_admin_md,\n",
        "                reset_admin_md,\n",
        "                reset_admin_md,\n",
        "                reset_admin_dd,\n",
        "                reset_admin_status,\n",
        "\n",
        "                gr.update(visible=False),  # admin_tab\n",
        "                reset_session_mem,         # session_memory_state\n",
        "            )\n",
        "\n",
        "        logout_btn.click(\n",
        "            _logout_cb,\n",
        "            inputs=[session_state, role_state, api_key_state, session_memory_state],\n",
        "            outputs=[\n",
        "                login_status,\n",
        "                session_state,\n",
        "                role_state,\n",
        "                api_key_state,\n",
        "                username_tb,\n",
        "                password_tb,\n",
        "                api_key_tb,\n",
        "                current_user_label,\n",
        "\n",
        "                # Setup & Cohorts tab\n",
        "                setup_handles[\"action_radio\"],\n",
        "                setup_handles[\"new_cohort_name\"],\n",
        "                setup_handles[\"share_checkbox\"],\n",
        "                setup_handles[\"allow_clone_checkbox\"],\n",
        "                setup_handles[\"existing_cohort_dropdown\"],\n",
        "                setup_handles[\"file_uploader\"],\n",
        "                setup_handles[\"build_status\"],\n",
        "\n",
        "                # Ask tab\n",
        "                ask_handles[\"cohort_dropdown\"],\n",
        "                ask_handles[\"model_dropdown\"],\n",
        "                ask_handles[\"question_box\"],\n",
        "                ask_handles[\"improved_box\"],\n",
        "                ask_handles[\"answer_markdown\"],\n",
        "                ask_handles[\"docs_markdown\"],\n",
        "\n",
        "                # History tab\n",
        "                history_handles[\"history_user_filter\"],\n",
        "                history_handles[\"history_cohort_filter\"],\n",
        "                history_handles[\"history_markdown\"],\n",
        "\n",
        "                # Admin tab\n",
        "                admin_handles[\"admin_stats_md\"],\n",
        "                admin_handles[\"admin_users_md\"],\n",
        "                admin_handles[\"admin_cohorts_md\"],\n",
        "                admin_handles[\"admin_audit_md\"],\n",
        "                admin_handles[\"admin_delete_cohort_dropdown\"],\n",
        "                admin_handles[\"admin_delete_status_md\"],\n",
        "                admin_tab,\n",
        "                session_memory_state,\n",
        "            ],\n",
        "            queue=False,\n",
        "            js=ASK_DISABLE_JS,\n",
        "        )\n",
        "\n",
        "    return demo\n",
        "\n",
        "# ---- Create & Launch the App ----\n",
        "if __name__ == \"__main__\":\n",
        "    demo = build_interface()\n",
        "    # In Colab: share=False is usually fine; set to True if you want a public link.\n",
        "    demo.launch(share=False, debug=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}