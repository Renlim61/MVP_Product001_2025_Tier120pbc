{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Renlim61/MVP_Product001_2025_Tier120pbc/blob/main/Phase1_RAG_MVP_clean_v7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlRoM38Emjcu"
      },
      "source": [
        "# Phase 1 – GenAI RAG MVP (Colab, Clean v7) This version ran successfuly.\n",
        "\n",
        "**Goal:** Single-tenant RAG assistant (BYO OpenAI key). Ingest PDF/DOCX/TXT → chunk + embed → search with FAISS → answer with citations.\n",
        "\n",
        "### Highlights\n",
        "- Upload **.pdf / .docx / .txt**\n",
        "- OpenAI **embeddings** (fallback to large if small not available)\n",
        "- **FAISS** cosine similarity\n",
        "- **Gradio UI** with per-prompt document selection\n",
        "- **Optional**: Save/Load state to Google Drive\n",
        "\n",
        "**Instructions:** Run the next cell, paste your OpenAI key in the UI, upload files, click **Ingest**, then ask questions under **Ask**.\n"
      ],
      "id": "hlRoM38Emjcu"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 8 (updates)\n",
        "Core backend helpers consolidated: ingestion pipeline, chunking strategy, embedding calls, FAISS index handling, retrieval, RAG-answer assembly.\n",
        "Persistence helpers expanded: save/load state, directory conventions for storing indexes and metadata.\n",
        "Improved error handling, API sanity checks, and global state containers (DOCUMENTS, model defaults).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Cell 9 (updates / moved to cell9.py)\n",
        "Gradio UI and event handlers (ingest, ask, Drive mount/save/load) packaged as a standalone file; UI improvements for document selection and better logging.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Cell 10\n",
        "Document normalization: cleaned and standardized input text (Unicode, whitespace, simple OCR fixes) before chunking.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Cell 11\n",
        "Chunking enhancements: configurable chunk size/overlap, sentence-aware splits, and token-count based chunking for consistent embeddings.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Cell 12\n",
        "Per-document vector store management: create/load a separate FAISS index (and metadata files) per document or logical collection; folder layout conventions for indexes and metadata.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Cell 13\n",
        "Batch embedding & rate-control: batched embedding calls with retry/backoff logic and optional parallelism to improve throughput and handle API limits.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Cell 14\n",
        "Metadata and citation support: store source/file offsets, chunk IDs, and human-friendly citations for use in RAG outputs.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Cell 15\n",
        "Retrieval improvements: hybrid filtering (by document IDs), configurable Top-K, and basic scoring/post-processing to prefer higher-quality chunks.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Cell 16\n",
        "State management & export: functions to export/import full project state (indexes, docs, metadata) and lightweight snapshots suitable for Google Drive persistence.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Cell 17\n",
        "Utilities, testing & debug helpers: quick QA checks, ingestion summary reports, verbose logging toggle, and small diagnostic endpoints to inspect index contents and sample embeddings.\n",
        "\n",
        "\n",
        "\n",
        "Overall workflow impact\n",
        "\n",
        "More robust ingestion (clean → chunk → batch-embed → per-doc index).\n",
        "Better retrieval (document-scoped searches, metadata-aware citations).\n",
        "Reliable persistence (clear folder layout, save/load/export to Drive).\n",
        "Scalable embedding (batching, retries, rate control) and clearer debugging/logging."
      ],
      "metadata": {
        "id": "N6vCo_90y-tJ"
      },
      "id": "N6vCo_90y-tJ"
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 0 - INSTALL & IMPORTS\n",
        "# (Run once) Install / import required packages\n",
        "!pip install -q faiss-cpu openai gradio\n",
        "# Standard imports\n",
        "import osimport json\n",
        "import shutil\n",
        "import pickle\n",
        "import numpy as npfrom typing\n",
        "import List, Dict\n",
        "import faissimport gradio as gr\n",
        "# OpenAI client import placeholder (your notebook likely creates a client later)from openai import OpenAI\n",
        "# Defaults\n",
        "EMBED_MODEL_DEFAULT = \"text-embedding-3-small\"\n",
        "CHAT_MODEL_DEFAULT = \"gpt-4o-mini\""
      ],
      "metadata": {
        "id": "tBPiXZTjxsRu"
      },
      "id": "tBPiXZTjxsRu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1 — Config & Globals\n",
        "# Cell 1 — Config & Globals\n",
        "# Base paths and globals\n",
        "BASE_DRIVE = \"/content/drive/MyDrive/MVP_RAG\" # will be created when Drive mounted\n",
        "COHORTS_DIR = os.path.join(BASE_DRIVE, \"coherts\")"
      ],
      "metadata": {
        "id": "bNrY_ElAyPCQ"
      },
      "id": "bNrY_ElAyPCQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 2 — Readers (PDF/DOCX/TXT)\n",
        "# --- Readers ---\n",
        "def _read_pdf(file_bytes: bytes) -> str:\n",
        "    reader = PdfReader(io.BytesIO(file_bytes))\n",
        "    texts = []\n",
        "    for page in reader.pages:\n",
        "        try:\n",
        "            texts.append(page.extract_text() or \"\")\n",
        "        except Exception:\n",
        "            texts.append(\"\")\n",
        "    return \"\\n\".join(texts)\n",
        "\n",
        "def _read_docx(file_bytes: bytes) -> str:\n",
        "    bio = io.BytesIO(file_bytes)\n",
        "    doc = DocxDocument(bio)\n",
        "    return \"\\n\".join(p.text for p in doc.paragraphs)\n",
        "\n",
        "def _read_txt(file_bytes: bytes) -> str:\n",
        "    try:\n",
        "        return file_bytes.decode(\"utf-8\")\n",
        "    except Exception:\n",
        "        return file_bytes.decode(\"latin-1\", errors=\"ignore\")\n",
        "\n",
        "# Page-aware PDF reader (returns list of {\"page\": int, \"text\": str})\n",
        "def _read_pdf_pages(file_bytes: bytes):\n",
        "    reader = PdfReader(io.BytesIO(file_bytes))\n",
        "    pages = []\n",
        "    for i, page in enumerate(reader.pages, start=1):\n",
        "        try:\n",
        "            text = page.extract_text() or \"\"\n",
        "        except Exception:\n",
        "            text = \"\"\n",
        "        pages.append({\"page\": i, \"text\": text})\n",
        "    return pages\n",
        "\n",
        "\n",
        "def load_file(file_obj) -> Dict[str, Any]:\n",
        "    \"\"\"Return dict with keys: name, text, filetype, pages? (for PDFs).\"\"\"\n",
        "    if isinstance(file_obj, str):\n",
        "        path = file_obj\n",
        "        name = os.path.basename(path)\n",
        "        with open(path, 'rb') as f:\n",
        "            content = f.read()\n",
        "    else:\n",
        "        name = getattr(file_obj, 'orig_name', None) or getattr(file_obj, 'name', 'uploaded_file')\n",
        "        if hasattr(file_obj, 'read'):\n",
        "            content = file_obj.read()\n",
        "        else:\n",
        "            path = getattr(file_obj, 'path', None)\n",
        "            if not path:\n",
        "                raise ValueError(\"Unsupported file object received from Gradio upload.\")\n",
        "            with open(path, 'rb') as f:\n",
        "                content = f.read()\n",
        "\n",
        "    if not content:\n",
        "        raise ValueError(f\"{name}: file is empty.\")\n",
        "\n",
        "    lower = name.lower()\n",
        "    meta = {\"name\": os.path.basename(name)}\n",
        "\n",
        "    if lower.endswith('.pdf'):\n",
        "        pages = _read_pdf_pages(content)\n",
        "        full_text = \"\\n\".join(p[\"text\"] for p in pages)\n",
        "        meta.update({\"text\": (full_text or \"\").strip(), \"filetype\": \"pdf\", \"pages\": pages})\n",
        "    elif lower.endswith('.docx'):\n",
        "        text = _read_docx(content)\n",
        "        meta.update({\"text\": (text or \"\").strip(), \"filetype\": \"docx\"})\n",
        "    elif lower.endswith('.txt'):\n",
        "        text = _read_txt(content)\n",
        "        meta.update({\"text\": (text or \"\").strip(), \"filetype\": \"txt\"})\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported file type for {name}. Use PDF/DOCX/TXT.\")\n",
        "\n",
        "    if not meta[\"text\"]:\n",
        "        raise ValueError(f\"{meta['name']}: no extractable text found (scanned PDF or empty file?).\")\n",
        "\n",
        "    return meta"
      ],
      "metadata": {
        "id": "p4BL2kxdydqq"
      },
      "id": "p4BL2kxdydqq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 3 — Chunking\n",
        "# --- Chunking ---\n",
        "def chunk_text(text: str, chunk_size: int = CHUNK_SIZE, overlap: int = CHUNK_OVERLAP) -> List[str]:\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    n = len(text)\n",
        "    while start < n:\n",
        "        end = min(start + chunk_size, n)\n",
        "        chunk = text[start:end]\n",
        "        chunks.append(chunk)\n",
        "        if end == n:\n",
        "            break\n",
        "        start = end - overlap\n",
        "        if start < 0:\n",
        "            start = 0\n",
        "    return [c.strip() for c in chunks if c.strip()]"
      ],
      "metadata": {
        "id": "vB0P-5Tty_po"
      },
      "id": "vB0P-5Tty_po",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 4 — Embeddings\n",
        "# --- Embeddings ---\n",
        "def embed_texts(client: OpenAI, texts: List[str], model: str = EMBED_MODEL_DEFAULT, batch_size: int = 128) -> np.ndarray:\n",
        "    vectors = []\n",
        "    def _call(batch, mdl):\n",
        "        return client.embeddings.create(model=mdl, input=batch)\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        try:\n",
        "            resp = _call(batch, model)\n",
        "        except Exception as e1:\n",
        "            if model == \"text-embedding-3-small\":\n",
        "                try:\n",
        "                    resp = _call(batch, \"text-embedding-3-large\")\n",
        "                except Exception as e2:\n",
        "                    raise RuntimeError(f\"Embedding failed on both models: small-> {e1}; large-> {e2}\")\n",
        "            else:\n",
        "                raise\n",
        "        for d in resp.data:\n",
        "            vectors.append(d.embedding)\n",
        "    return np.array(vectors, dtype=np.float32)\n",
        "\n",
        "def normalize(vecs: np.ndarray) -> np.ndarray:\n",
        "    norms = np.linalg.norm(vecs, axis=1, keepdims=True) + 1e-12\n",
        "    return vecs / norms\n"
      ],
      "metadata": {
        "id": "xUvHIDmCzOyF"
      },
      "id": "xUvHIDmCzOyF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CELL 5 — FAISS (Index/Search)\n",
        "# --- FAISS ---\n",
        "def build_faiss_index(embs: np.ndarray) -> faiss.IndexFlatIP:\n",
        "    index = faiss.IndexFlatIP(embs.shape[1])\n",
        "    index.add(embs)\n",
        "    return index\n",
        "\n",
        "def search_faiss(index: faiss.IndexFlatIP, query_vec: np.ndarray, k: int = 5):\n",
        "    D, I = index.search(query_vec, k)\n",
        "    return D, I"
      ],
      "metadata": {
        "id": "AZiIYUR3zbG9"
      },
      "id": "AZiIYUR3zbG9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cell 6 — Persistence (Optional: Google Drive)\n",
        "# --- Persistence (Drive optional) ---\n",
        "def mount_drive() -> str:\n",
        "    if not IN_COLAB:\n",
        "        return \"\"\n",
        "    colab_drive.mount('/content/drive', force_remount=False)\n",
        "    save_dir = '/content/drive/MyDrive/RAG_MVP_Phase1'\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    return save_dir\n",
        "\n",
        "def save_state(save_dir: str):\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    state_path = os.path.join(save_dir, 'documents_state.pkl')\n",
        "    with open(state_path, 'wb') as f:\n",
        "        pickle.dump(DOCUMENTS, f)\n",
        "    return state_path\n",
        "\n",
        "def load_state(save_dir: str):\n",
        "    state_path = os.path.join(save_dir, 'documents_state.pkl')\n",
        "    if not os.path.exists(state_path):\n",
        "        raise FileNotFoundError(f\"No saved state at {state_path}\")\n",
        "    with open(state_path, 'rb') as f:\n",
        "        loaded = pickle.load(f)\n",
        "    DOCUMENTS.clear()\n",
        "    DOCUMENTS.update(loaded)\n",
        "    return True"
      ],
      "metadata": {
        "id": "vc9LXjMxzprt"
      },
      "id": "vc9LXjMxzprt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 7 - Ingestion\n",
        "def ingest_files(api_key: str, embed_model: str, files) -> Dict:\n",
        "    # quick API sanity check\n",
        "    try:\n",
        "        _ = OpenAI(api_key=api_key).embeddings.create(model=embed_model, input=[\"sanity check\"]).data[0].embedding\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"OpenAI key/model check failed: {type(e).__name__}: {e}\")\n",
        "\n",
        "    client = build_openai_client(api_key)\n",
        "    added = []\n",
        "\n",
        "    for f in files:\n",
        "        try:\n",
        "            meta = load_file(f)  # {name, text, filetype, pages?}\n",
        "            name = meta[\"name\"]\n",
        "            filetype = meta.get(\"filetype\", \"txt\")\n",
        "\n",
        "            all_chunks = []\n",
        "            all_metas = []  # each item: {\"page\": int|None}\n",
        "\n",
        "            if filetype == \"pdf\" and \"pages\" in meta:\n",
        "                # page-aware chunking\n",
        "                for page_entry in meta[\"pages\"]:\n",
        "                    page_no = page_entry[\"page\"]\n",
        "                    page_text = (page_entry[\"text\"] or \"\").strip()\n",
        "                    if not page_text:\n",
        "                        continue\n",
        "                    page_chunks = chunk_text(page_text, CHUNK_SIZE, CHUNK_OVERLAP)\n",
        "                    for ch in page_chunks:\n",
        "                        all_chunks.append(ch)\n",
        "                        all_metas.append({\"page\": page_no})\n",
        "            else:\n",
        "                # docx/txt\n",
        "                text = meta[\"text\"]\n",
        "                chunks = chunk_text(text, CHUNK_SIZE, CHUNK_OVERLAP)\n",
        "                for ch in chunks:\n",
        "                    all_chunks.append(ch)\n",
        "                    all_metas.append({\"page\": None})\n",
        "\n",
        "            if not all_chunks:\n",
        "                continue\n",
        "\n",
        "            embs = embed_texts(client, all_chunks, model=embed_model)\n",
        "            embs = normalize(embs)\n",
        "            index = build_faiss_index(embs)\n",
        "\n",
        "            doc_id = f\"doc_{int(time.time()*1000)}_{len(DOCUMENTS)+1}\"\n",
        "            DOCUMENTS[doc_id] = {\n",
        "                \"name\": name,\n",
        "                \"filetype\": filetype,\n",
        "                \"chunks\": all_chunks,\n",
        "                \"meta\": all_metas,  # <-- page info here\n",
        "                \"embs\": embs,\n",
        "                \"index\": index,\n",
        "            }\n",
        "            added.append({\"doc_id\": doc_id, \"name\": name, \"chunks\": len(all_chunks)})\n",
        "        except Exception as e:\n",
        "            print(f\"[ingest warning] {type(e).__name__}: {e}\\n\", traceback.format_exc())\n",
        "\n",
        "    return {\n",
        "        \"message\": f\"Ingested {len(added)} file(s).\",\n",
        "        \"docs\": [{\"doc_id\": k, \"name\": v[\"name\"], \"chunks\": len(v[\"chunks\"])} for k, v in DOCUMENTS.items()]\n",
        "    }\n"
      ],
      "metadata": {
        "id": "oYBBTQbr0Bur"
      },
      "id": "oYBBTQbr0Bur",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 8 - Retrieval + Answering (updated to support cohort querying via \"cohort:<name>\")\n",
        "def assemble_subset_index(selected_doc_ids: List[str]):\n",
        "    texts, metas, all_embs = [], [], []\n",
        "    for did in selected_doc_ids:\n",
        "        rec = DOCUMENTS.get(did)\n",
        "        if not rec:\n",
        "          continue\n",
        "        k = len(rec[\"chunks\"])\n",
        "        texts.extend(rec[\"chunks\"])\n",
        "        # keep (doc_id, chunk_idx, page)\n",
        "        metas.extend([(did, i, rec.get(\"meta\", [{}]*k)[i].get(\"page\")) for i in range(k)])\n",
        "        all_embs.append(rec[\"embs\"])\n",
        "\n",
        "        if not all_embs:\n",
        "           return None, None, None\n",
        "        embs = np.vstack(all_embs)\n",
        "        index = build_faiss_index(embs)\n",
        "        return index, texts, meta\n",
        "\n",
        "def _format_cohort_hits(cohort_results: List[Dict], scores: List[float]):\n",
        "   # cohort metadata entries expected to have keys like: \"chunk\", \"source_file\", optional \"page\"\n",
        "   hits = []\n",
        "   for rank, (meta, score) in enumerate(zip(cohort_results, scores), start=1:\n",
        "       src = meta.get(\"source_file\", \"cohort\")\n",
        "       chunk_text = meta.get(\"chunk\", \"\")\n",
        "       page_no = meta.get(\"page\")\n",
        "       hits.append({\n",
        "           \"rank\": rank,\n",
        "           \"doc_id\": src,\n",
        "           \"doc_name\": src,\n",
        "           \"chunk_idx\": 0,\n",
        "           \"page\": page_no,\n",
        "           \"score\": score,\n",
        "           \"text\": chunk_text\n",
        "           })\n",
        "  return hits\n",
        "\n",
        "def retrieve(api_key: str, query: str, embed_model: str, selected_doc_ids: List[str], k: int = 5):\n",
        "    client = build_openai_client(api_key)\n",
        "    # Special case: query an entire cohort by passing selected_doc_ids = [\"cohort:NAME\"]\n",
        "    if isinstance(selected_doc_ids, (list, tuple)) and len(selected_doc_ids) == 1 and isinstance(selected_doc_ids[0], str) and selected_doc_ids[0].startswith(\"cohort:\"):\n",
        "       cohort_name = selected_doc_ids[0].split(\"cohort:\", 1)[1]        try:\n",
        "           cohort_results, scores = query_cohort(cohort_name, client, query, k=k)\n",
        "       except Exception as e:\n",
        "           # return empty on error (or optionally return the exception message)\n",
        "           print(\"cohort query error:\", e)\n",
        "           return []\n",
        "       return _format_cohort_hits(cohort_results, scores)\n",
        "\n",
        "    # Default: assemble subset index from DOCUMENTS\n",
        "    index, texts, metas = assemble_subset_index(selected_doc_ids)\n",
        "    if index is None:\n",
        "       return []\n",
        "\n",
        "    # embed query using your existing client pattern\n",
        "    q_vec = client.embeddings.create(model=embed_model, input=[query]).data[0].embedding\n",
        "    q_vec = np.array(q_vec, dtype=np.float32)[None, :]\n",
        "    q_vec = normalize(q_vec)\n",
        "    D, I = search_faiss(index, q_vec, k)\n",
        "    hits = []\n",
        "    for rank, idx in enumerate(I[0].tolist()):\n",
        "        sim = float(D[0][rank])\n",
        "        did, chunk_idx, page_no = metas[idx]\n",
        "        doc = DOCUMENTS[did]\n",
        "        hits.append({\n",
        "            \"rank\": rank+1,\n",
        "            \"doc_id\": did,\n",
        "            \"doc_name\": doc[\"name\"],\n",
        "            \"chunk_idx\": chunk_idx,\n",
        "            \"page\": page_no,\n",
        "            \"score\": sim,\n",
        "            \"text\": texts[idx]\n",
        "            })\n",
        "    return hits\n",
        "\n",
        "def make_context_with_citations(hits: List[Dict], max_chars: int = 4000) -> str:\n",
        "    ctx_parts, citations, total = [], [], 0\n",
        "    for i, h in enumerate(hits, start=1):\n",
        "        chunk = (h[\"text\"] or \"\").strip().replace(\"\\n\", \" \")\n",
        "        page_str = f\" · p.{h['page']}\" if h.get(\"page\") else \"\"\n",
        "        prefix = f\"[Source {i}: {h['doc_name']}{page_str} · chunk {h['chunk_idx']}]\\n\"\n",
        "        part = prefix + chunk + \"\\n\\n\"\n",
        "        if total + len(part) > max_chars:\n",
        "            break\n",
        "        ctx_parts.append(part)\n",
        "        if h.get(\"page\"):\n",
        "            citations.append(f\"[{i}] {h['doc_name']} (p.{h['page']}, chunk {h['chunk_idx']})\")\n",
        "            else:\n",
        "            citations.append(f\"[{i}] {h['doc_name']} (chunk {h['chunk_idx']})\")\n",
        "            total += len(part)\n",
        "    ctx = \"\".join(ctx_parts)\n",
        "    return ctx, citations\n",
        "\n",
        "def answer_with_rag(api_key: str, chat_model: str, query: str, hits: List[Dict]):\n",
        "    client = build_openai_client(api_key)\n",
        "    ctx, citations = make_context_with_citations(hits)\n",
        "    system_prompt = (\n",
        "        \"You are a helpful assistant. Use the provided sources to answer succinctly. \"\n",
        "        \"When you rely on a source, include bracketed reference numbers like [1], [2]. If the sources don't contain the answer, say so.\"\n",
        "        )\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "             {\"role\": \"user\", \"content\": f\"Question: {query}\\n\\nSources:\\n{ctx}\"}\n",
        "        ]\n",
        "        resp = client.chat.completions.create(model=chat_model, messages=messages, temperature=0.2)\n",
        "        answer = resp.choices[0].message.content\n",
        "        if citations:\n",
        "            answer = answer + \"\\n\\nSources:\\n\" + \"\\n\".join(citations)\n",
        "        return answer"
      ],
      "metadata": {
        "id": "Hjdkj8e51v5K"
      },
      "id": "Hjdkj8e51v5K",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 9 - Gradio UI & Launch# --- Gradio App (defined and launched in this cell) ---\n",
        "def ui_list_docs():\n",
        "    return [f\"{v['name']} — {k}\" for k, v in DOCUMENTS.items()]\n",
        "\n",
        "def _ids_from_labels(labels: List[str]) -> List[str]:\n",
        "    ids = []\n",
        "    for lab in labels or []:\n",
        "        if '—' in lab:\n",
        "           ids.append(lab.split('—')[-1].strip())\n",
        "    return ids\n",
        "\n",
        "def on_ingest(api_key, embed_model, files):\n",
        "    if not api_key:\n",
        "        return gr.update(value=\"Please enter your OpenAI API key.\"), gr.update(choices=ui_list_docs(), value=[])\n",
        "    if not files:\n",
        "        return gr.update(value=\"No files selected.\"), gr.update(choices=ui_list_docs(), value=[])\n",
        "\n",
        "    # quick API sanity check\n",
        "    try:\n",
        "        _ = OpenAI(api_key=api_key).embeddings.create(model=embed_model, input=[\"sanity check\"]).data[0].embedding\n",
        "    except Exception as e:\n",
        "        return f\"OpenAI key/model check failed: {type(e).__name__}: {e}\", gr.update(choices=ui_list_docs(), value=[])\n",
        "    try:\n",
        "        res = ingest_files(api_key, embed_model, files)\n",
        "        msg = res[\"message\"] + \"\\n\" + json.dumps(res[\"docs\"], indent=2)        return msg, gr.update(choices=ui_list_docs(), value=ui_list_docs())\n",
        "    except Exception as e:\n",
        "        tb = traceback.format_exc()\n",
        "        return f\"Ingest failed: {type(e).__name__}: {e}\\n\\nTraceback:\\n{tb}\", gr.update(choices=ui_list_docs(), value=[])\n",
        "\n",
        "def on_ask(api_key, chat_model, embed_model, query, selected_labels, top_k):\n",
        "    try:\n",
        "        selected_ids = _ids_from_labels(selected_labels)\n",
        "        if not selected_ids:\n",
        "            return \"Please select at least one ingested file.\"\n",
        "        hits = retrieve(api_key, query, embed_model, selected_ids, k=top_k)\n",
        "        if not hits:\n",
        "            return \"No results found. Try ingesting files or broadening your question.\"\n",
        "        answer = answer_with_rag(api_key, chat_model, query, hits)\n",
        "        return answer\n",
        "    except Exception as e:\n",
        "        tb = traceback.format_exc()\n",
        "        return f\"Error: {type(e).__name__}: {e}\\n\\nTraceback:\\n{tb}\"\n",
        "\n",
        "def on_mount_drive():\n",
        "    if not IN_COLAB:\n",
        "        return \"This action is only available in Google Colab.\", \"\"\n",
        "    save_dir = mount_drive()\n",
        "    return f\"Drive mounted. Save dir: {save_dir}\", save_dir\n",
        "\n",
        "def on_save(save_dir):\n",
        "    if not save_dir:\n",
        "        return \"Provide a Google Drive folder path first.\"\n",
        "    path = save_state(save_dir)\n",
        "    return f\"Saved state to: {path}\"\n",
        "\n",
        "def on_load(save_dir):\n",
        "    try:\n",
        "        load_state(save_dir)\n",
        "        return f\"Loaded state from: {save_dir}\", gr.update(choices=ui_list_docs(), value=ui_list_docs())\n",
        "    except Exception as e:\n",
        "      return f\"Load failed: {e}\", gr.update()\n",
        "\n",
        "with gr.Blocks(title=\"Phase 1 – RAG MVP\") as demo:\n",
        "    gr.Markdown(\"\"\"\n",
        "    # Phase 1 – RAG MVP\n",
        "    **Bring Your Own OpenAI Key**. Ingest PDF/DOCX/TXT → chunk + embed → FAISS → ask with citations.\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Row():\n",
        "        api_key = gr.Textbox(label=\"OpenAI API Key\", type=\"password\", placeholder=\"sk-...\", show_label=True)\n",
        "        chat_model = gr.Dropdown(choices=[\"gpt-4o-mini\", \"gpt-4o\", \"gpt-4.1-mini\"], value=CHAT_MODEL_DEFAULT, label=\"Chat Model\")        embed_model = gr.Dropdown(choices=[\"text-embedding-3-small\", \"text-embedding-3-large\"], value=EMBED_MODEL_DEFAULT, label=\"Embedding Model\")\n",
        "    # Shared document selector across tabs\n",
        "    doc_selector = gr.CheckboxGroup(label=\"Available Documents (shared)\", choices=[])\n",
        "\n",
        "    with gr.Tab(\"Ingest\"):\n",
        "      files = gr.File(label=\"Upload files (PDF, DOCX, TXT)\", file_count=\"multiple\", type=\"filepath\", file_types=[\".pdf\", \".docx\", \".txt\"])\n",
        "      ingest_btn = gr.Button(\"Ingest\")\n",
        "      ingest_log = gr.Textbox(label=\"Ingestion Log\", lines=10)        ingest_btn.click(on_ingest, inputs=[api_key, embed_model, files], outputs=[ingest_log, doc_selector])\n",
        "\n",
        "      with gr.Tab(\"Ask\"):\n",
        "          with gr.Row():\n",
        "              query = gr.Textbox(label=\"Your question\", placeholder=\"Ask me about your documents...\", lines=3)\n",
        "          with gr.Row():\n",
        "              top_k = gr.Slider(1, 10, value=5, step=1, label=\"Top-K Chunks\")\n",
        "          ask_btn = gr.Button(\"Ask\")\n",
        "          answer_out = gr.Markdown()\n",
        "          ask_btn.click(on_ask, inputs=[api_key, chat_model, embed_model, query, doc_selector, top_k], outputs=[answer_out,])\n",
        "\n",
        "          with gr.Tab(\"Google Drive (Optional)\"):\n",
        "              drive_status = gr.Textbox(label=\"Status\")\n",
        "              save_dir = gr.Textbox(label=\"Save folder (e.g., /content/drive/MyDrive/RAG_MVP_Phase1)\")\n",
        "\n",
        "              with gr.Row():\n",
        "                  mount_btn = gr.Button(\"Mount Drive (Colab)\")            mount_btn.click(on_mount_drive, inputs=[], outputs=[drive_status, save_dir])\n",
        "              with gr.Row():\n",
        "                  save_btn = gr.Button(\"Save State\")\n",
        "                  load_btn = gr.Button(\"Load State\")\n",
        "\n",
        "              save_btn.click(on_save, inputs=[save_dir], outputs=[drive_status])\n",
        "\n",
        "              load_btn.click(on_load, inputs=[save_dir], outputs=[drive_status, doc_selector])\n",
        "\n",
        "        gr.Markdown(\"Built for fast iteration. ⚡️\")\n",
        "\n",
        "# Launch immediately to avoid ordering issues\n",
        "demo.launch()"
      ],
      "metadata": {
        "id": "8bGx6i3IoMr5"
      },
      "id": "8bGx6i3IoMr5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 10 - Mount Drive & pathsfrom google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "import os, jsonBASE_DRIVE = \"/content/drive/MyDrive/MVP_RAG\"\n",
        "COHORTS_DIR = os.path.join(BASE_DRIVE, \"cohorts\")\n",
        "os.makedirs(COHORTS_DIR, exist_ok=True)\n",
        "\n",
        "print(\"Cohorts root:\", COHORTS_DIR)"
      ],
      "metadata": {
        "id": "UBS__10YLh4p"
      },
      "id": "UBS__10YLh4p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 11 - Imports + detect embedding dim\n",
        "import faiss, pickle, shutil\n",
        "import numpy as np\n",
        "from typing import List, Dict\n",
        "EMBED_MODEL_DEFAULT = \"text-embedding-3-small\"  # ensure matches Cell 0\n",
        "\n",
        "def detect_embed_dim(client) -> int:\n",
        "      # safe tiny probe to detect embedding dimension at runtime\n",
        "      sample = [\"test\"]\n",
        "      vecs = embed_texts(client, sample, model=EMBED_MODEL_DEFAULT)\n",
        "      return len(vecs[0])"
      ],
      "metadata": {
        "id": "vwZw_NxuLstz"
      },
      "id": "vwZw_NxuLstz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 12 - Cohort filesystem helpers\n",
        "def cohort_path(name: str) -> str:\n",
        "  return os.path.join(COHORTS_DIR, name)\n",
        "\n",
        "def ensure_cohort_dirs(name: str):\n",
        "  p = cohort_path(name)\n",
        "  os.makedirs(os.path.join(p, \"files\"), exist_ok=True)\n",
        "  os.makedirs(os.path.join(p, \"chunks\"), exist_ok=True)\n",
        "\n",
        "  def list_cohorts() -> List[str]:\n",
        "    if not os.path.exists(COHORTS_DIR):\n",
        "      return []\n",
        "    return sorted([d for d in os.listdir(COHORTS_DIR) if os.path.isdir(cohort_path(d))])"
      ],
      "metadata": {
        "id": "YcOb-uBSLyGY"
      },
      "id": "YcOb-uBSLyGY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 13 - Create, load, persist cohort\n",
        "\n",
        "def create_cohort(name: str, client, description: str = \"\") -> Dict:\n",
        "  p = cohort_path(name)\n",
        "  if os.path.exists(p):\n",
        "    raise FileExistsError(f\"Cohort '{name}' exists\")\n",
        "    ensure_cohort_dirs(name)\n",
        "    dim = detect_embed_dim(client)\n",
        "    index = faiss.IndexFlatIP(dim)  # use inner-product on normalized vectors\n",
        "    faiss.write_index(index, os.path.join(p, \"index.faiss\"))\n",
        "    with open(os.path.join(p, \"metadata.pkl\"), \"wb\") as f:\n",
        "      pickle.dump([], f)\n",
        "    manifest = {\"description\": description, \"embed_model\": EMBED_MODEL_DEFAULT, \"dim\": dim}\n",
        "    with open(os.path.join(p, \"manifest.json\"), \"w\") as f:\n",
        "      json.dump(manifest, f)\n",
        "    return manifest\n",
        "\n",
        "  def load_cohort(name: str):\n",
        "    p = cohort_path(name)\n",
        "    if not os.path.exists(p):\n",
        "      raise FileNotFoundError(name)\n",
        "    idx_path = os.path.join(p, \"index.faiss\")\n",
        "    meta_path = os.path.join(p, \"metadata.pkl\")\n",
        "    manifest_path = os.path.join(p, \"manifest.json\")\n",
        "    index = faiss.read_index(idx_path)\n",
        "    with open(meta_path, \"rb\") as f:\n",
        "      metadata = pickle.load(f)\n",
        "    with open(manifest_path, \"r\") as f:\n",
        "      manifest = json.load(f)\n",
        "    return {\"index\": index, \"metadata\": metadata, \"manifest\": manifest, \"path\": p}\n",
        "\n",
        "  def persist_cohort(name: str, index, metadata, manifest=None):\n",
        "    p = cohort_path(name)\n",
        "    faiss.write_index(index, os.path.join(p, \"index.faiss\"))\n",
        "    with open(os.path.join(p, \"metadata.pkl\"), \"wb\") as f:\n",
        "      pickle.dump(metadata, f)\n",
        "    if manifest is not None:\n",
        "      with open(os.path.join(p, \"manifest.json\"), \"w\") as f:\n",
        "        json.dump(manifest, f)"
      ],
      "metadata": {
        "id": "XAh8Mc0LL2_a"
      },
      "id": "XAh8Mc0LL2_a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 14 - Ingest files into cohort (relies on chunk_text and embed_texts)\n",
        "\n",
        "def _normalize_vectors(np_arr: np.ndarray) -> np.ndarray:\n",
        "    norms = np.linalg.norm(np_arr, axis=1, keepdims=True)\n",
        "    norms[norms == 0] = 1.0\n",
        "    return np_arr / norms\n",
        "\n",
        "def ingest_file_to_cohort(name: str, client, local_file_path: str, verbose=False) -> int:\n",
        "      cpath = cohort_path(name)\n",
        "      if not os.path.exists(cpath):\n",
        "        raise FileNotFoundError(name)\n",
        "      files_dir = os.path.join(cpath, \"files\")\n",
        "      os.makedirs(files_dir, exist_ok=True)\n",
        "      dest = os.path.join(files_dir, os.path.basename(local_file_path))\n",
        "      shutil.copy(local_file_path, dest)\n",
        "\n",
        "      # your chunk_text should return list of (chunk_text, meta_dict)\n",
        "      chunks = chunk_text(dest)\n",
        "      texts = [t for t, meta in chunks]\n",
        "      if not texts:\n",
        "         return 0\n",
        "\n",
        "      embeddings = embed_texts(client, texts, model=EMBED_MODEL_DEFAULT)   emb_np = np.array(embeddings, dtype=np.float32)\n",
        "      emb_np = _normalize_vectors(emb_np)\n",
        "\n",
        "      cohort = load_cohort(name)\n",
        "      index = cohort[\"index\"]\n",
        "      metadata = cohort[\"metadata\"]\n",
        "\n",
        "      index.add(emb_np)\n",
        "      for i, (txt, meta) in enumerate(chunks):\n",
        "          entry = {\"chunk\": txt, \"source_file\": os.path.basename(dest)}\n",
        "          if isinstance(meta, dict):\n",
        "              entry.update(meta)\n",
        "          metadata.append(entry)\n",
        "\n",
        "      persist_cohort(name, index, metadata, manifest=cohort[\"manifest\"])\n",
        "      if verbose:\n",
        "          print(f\"Added {len(texts)} chunks to cohort '{name}'\")\n",
        "      return len(texts)"
      ],
      "metadata": {
        "id": "1HEPKlgcL7l_"
      },
      "id": "1HEPKlgcL7l_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 15 - Delete file from cohort (rebuild index)\n",
        "\n",
        "def delete_file_from_cohort(name: str, filename: str, client) -> bool:\n",
        "    cohort = load_cohort(name)\n",
        "    p = cohort[\"path\"]\n",
        "    files_dir = os.path.join(p, \"files\")\n",
        "    target = os.path.join(files_dir, filename)\n",
        "    if os.path.exists(target):\n",
        "        os.remove(target)\n",
        "\n",
        "    remaining = [m for m in cohort[\"metadata\"] if m.get(\"source_file\") != filename]\n",
        "    texts = [m[\"chunk\"] for m in remaining]\n",
        "    if texts:\n",
        "       embeddings = embed_texts(client, texts, model=cohort[\"manifest\"].get(\"embed_model\",EMBED_MODEL_DEFAULT))\n",
        "       emb_np = np.array(embeddings, dtype=np.float32)\n",
        "       emb_np = _normalize_vectors(emb_np)\n",
        "       dim = emb_np.shape[1]\n",
        "       new_index = faiss.IndexFlatIP(dim)\n",
        "       new_index.add(emb_np)\n",
        "    else:\n",
        "       dim = cohort[\"manifest\"].get(\"dim\", detect_embed_dim(client))        new_index = faiss.IndexFlatIP(dim)\n",
        "\n",
        "  persist_cohort(name, new_index, remaining, manifest=cohort[\"manifest\"])    return True"
      ],
      "metadata": {
        "id": "9I1usztqMARP"
      },
      "id": "9I1usztqMARP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 16 - Query cohort\n",
        "\n",
        "def query_cohort(name: str, client, query: str, k: int = 5):\n",
        "  cohort = load_cohort(name)\n",
        "  index = cohort[\"index\"]\n",
        "  metadata = cohort[\"metadata\"]\n",
        "  if len(metadata) == 0:\n",
        "    return [], []\n",
        "\n",
        "    q_emb = embed_texts(client, [query], model=cohort[\"manifest\"].get(\"embed_model\", EMBED_MODEL_DEFAULT))\n",
        "    q_np = np.array(q_emb, dtype=np.float32)\n",
        "    q_np = _normalize_vectors(q_np)\n",
        "\n",
        "    D, I = index.search(q_np, k)\n",
        "    results = []\n",
        "    scores = []\n",
        "    for idx, score in zip(I[0], D[0]):\n",
        "      if idx < len(metadata):\n",
        "        results.append(metadata[idx])\n",
        "        scores.append(float(score))\n",
        "    return results, scores"
      ],
      "metadata": {
        "id": "PnGHmYDsMFZC"
      },
      "id": "PnGHmYDsMFZC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 17 - Minimal Gradio wiring for cohorts (adapt to your existing UI variables)import gradio as gr\n",
        "def ui_create_cohort(name, desc, api_key):\n",
        "    from openai import OpenAI\n",
        "    client_local = OpenAI(api_key=api_key)\n",
        "    create_cohort(name, client_local, description=desc)\n",
        "    return gr.update(choices=list_cohorts()), f\"Created cohort '{name}'\"\n",
        "\n",
        "def ui_ingest_files(selected_cohort, uploaded_files, api_key):\n",
        "    from openai import OpenAI\n",
        "    client_local = OpenAI(api_key=api_key)\n",
        "    added = 0\n",
        "    for f in uploaded_files:\n",
        "      path = f if isinstance(f, str) else f.name\n",
        "      added += ingest_file_to_cohort(selected_cohort, client_local, path)   return f\"Added {added} chunks to '{selected_cohort}'\"\n",
        "\n",
        "def ui_list_files(selected_cohort):\n",
        "    p = cohort_path(selected_cohort)\n",
        "    files_dir = os.path.join(p, \"files\")\n",
        "    if not os.path.exists(files_dir):\n",
        "      return []\n",
        "    return sorted(os.listdir(files_dir))\n",
        "\n",
        "def ui_delete_file(selected_cohort, filename, api_key):\n",
        "    from openai import OpenAI\n",
        "    client_local = OpenAI(api_key=api_key)\n",
        "    delete_file_from_cohort(selected_cohort, filename, client_local)\n",
        "    return f\"Deleted {filename}\"\n",
        "\n",
        "def ui_delete_cohort(name):\n",
        "    p = cohort_path(name)\n",
        "    if os.path.exists(p):\n",
        "      shutil.rmtree(p)\n",
        "    return gr.update(choices=list_cohorts()), f\"Deleted cohort '{name}'\"\n",
        "\n"
      ],
      "metadata": {
        "id": "UlhgVhfiMKmq"
      },
      "id": "UlhgVhfiMKmq",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}