{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Phase 1 \u2013 GenAI RAG MVP (Colab, Clean v5)\n", "\n", "**Goal:** Single-tenant RAG assistant (BYO OpenAI key). Ingest PDF/DOCX/TXT \u2192 chunk + embed \u2192 search with FAISS \u2192 answer with citations.\n", "\n", "### Highlights\n", "- Upload **.pdf / .docx / .txt**\n", "- OpenAI **embeddings** (fallback to large if small not available)\n", "- **FAISS** cosine similarity\n", "- **Gradio UI** with per-prompt document selection\n", "- **Optional**: Save/Load state to Google Drive\n", "\n", "**Instructions:** Run the next cell, paste your OpenAI key in the UI, upload files, click **Ingest**, then ask questions under **Ask**.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# %%capture\n", "!pip -q install openai gradio faiss-cpu pypdf python-docx\n", "\n", "import os, io, json, pickle, time, traceback\n", "import numpy as np\n", "from typing import List, Dict, Any\n", "import gradio as gr\n", "from pypdf import PdfReader\n", "from docx import Document as DocxDocument\n", "import faiss\n", "from dataclasses import dataclass\n", "\n", "try:\n", "    from google.colab import drive as colab_drive\n", "    IN_COLAB = True\n", "except Exception:\n", "    IN_COLAB = False\n", "\n", "try:\n", "    from openai import OpenAI\n", "except Exception:\n", "    raise RuntimeError(\"The 'openai' package failed to import. Make sure the install cell ran successfully.\")\n", "\n", "# --- Config ---\n", "EMBED_MODEL_DEFAULT = \"text-embedding-3-small\"\n", "CHAT_MODEL_DEFAULT = \"gpt-4o-mini\"\n", "CHUNK_SIZE = 1200\n", "CHUNK_OVERLAP = 200\n", "\n", "# In-memory documents\n", "DOCUMENTS: Dict[str, Dict[str, Any]] = {}\n", "\n", "def build_openai_client(api_key: str):\n", "    if not api_key:\n", "        raise ValueError(\"OpenAI API key is required.\")\n", "    os.environ[\"OPENAI_API_KEY\"] = api_key\n", "    return OpenAI(api_key=api_key)\n", "\n", "# --- Readers ---\n", "def _read_pdf(file_bytes: bytes) -> str:\n", "    reader = PdfReader(io.BytesIO(file_bytes))\n", "    texts = []\n", "    for page in reader.pages:\n", "        try:\n", "            texts.append(page.extract_text() or \"\")\n", "        except Exception:\n", "            texts.append(\"\")\n", "    return \"\\n\".join(texts)\n", "\n", "def _read_docx(file_bytes: bytes) -> str:\n", "    bio = io.BytesIO(file_bytes)\n", "    doc = DocxDocument(bio)\n", "    return \"\\n\".join(p.text for p in doc.paragraphs)\n", "\n", "def _read_txt(file_bytes: bytes) -> str:\n", "    try:\n", "        return file_bytes.decode(\"utf-8\")\n", "    except Exception:\n", "        return file_bytes.decode(\"latin-1\", errors=\"ignore\")\n", "\n", "def load_file(file_obj) -> Dict[str, Any]:\n", "    \"\"\"Return dict with keys: name, text. Accepts Gradio filepath strings or file-like objects.\"\"\"\n", "    if isinstance(file_obj, str):\n", "        path = file_obj\n", "        name = os.path.basename(path)\n", "        with open(path, 'rb') as f:\n", "            content = f.read()\n", "    else:\n", "        name = getattr(file_obj, 'orig_name', None) or getattr(file_obj, 'name', 'uploaded_file')\n", "        if hasattr(file_obj, 'read'):\n", "            content = file_obj.read()\n", "        else:\n", "            path = getattr(file_obj, 'path', None)\n", "            if not path:\n", "                raise ValueError(\"Unsupported file object received from Gradio upload.\")\n", "            with open(path, 'rb') as f:\n", "                content = f.read()\n", "\n", "    if not content:\n", "        raise ValueError(f\"{name}: file is empty.\")\n", "\n", "    lower = name.lower()\n", "    if lower.endswith('.pdf'):\n", "        text = _read_pdf(content)\n", "    elif lower.endswith('.docx'):\n", "        text = _read_docx(content)\n", "    elif lower.endswith('.txt'):\n", "        text = _read_txt(content)\n", "    else:\n", "        raise ValueError(f\"Unsupported file type for {name}. Use PDF/DOCX/TXT.\")\n", "\n", "    text = (text or \"\").strip()\n", "    if not text:\n", "        raise ValueError(f\"{name}: no extractable text found (is it a scanned PDF or empty file?).\")\n", "    return {\"name\": os.path.basename(name), \"text\": text}\n", "\n", "# --- Chunking ---\n", "def chunk_text(text: str, chunk_size: int = CHUNK_SIZE, overlap: int = CHUNK_OVERLAP) -> List[str]:\n", "    chunks = []\n", "    start = 0\n", "    n = len(text)\n", "    while start < n:\n", "        end = min(start + chunk_size, n)\n", "        chunk = text[start:end]\n", "        chunks.append(chunk)\n", "        if end == n:\n", "            break\n", "        start = end - overlap\n", "        if start < 0:\n", "            start = 0\n", "    return [c.strip() for c in chunks if c.strip()]\n", "\n", "# --- Embeddings ---\n", "def embed_texts(client: OpenAI, texts: List[str], model: str = EMBED_MODEL_DEFAULT, batch_size: int = 128) -> np.ndarray:\n", "    vectors = []\n", "    def _call(batch, mdl):\n", "        return client.embeddings.create(model=mdl, input=batch)\n", "    for i in range(0, len(texts), batch_size):\n", "        batch = texts[i:i+batch_size]\n", "        try:\n", "            resp = _call(batch, model)\n", "        except Exception as e1:\n", "            if model == \"text-embedding-3-small\":\n", "                try:\n", "                    resp = _call(batch, \"text-embedding-3-large\")\n", "                except Exception as e2:\n", "                    raise RuntimeError(f\"Embedding failed on both models: small-> {e1}; large-> {e2}\")\n", "            else:\n", "                raise\n", "        for d in resp.data:\n", "            vectors.append(d.embedding)\n", "    return np.array(vectors, dtype=np.float32)\n", "\n", "def normalize(vecs: np.ndarray) -> np.ndarray:\n", "    norms = np.linalg.norm(vecs, axis=1, keepdims=True) + 1e-12\n", "    return vecs / norms\n", "\n", "# --- FAISS ---\n", "def build_faiss_index(embs: np.ndarray) -> faiss.IndexFlatIP:\n", "    index = faiss.IndexFlatIP(embs.shape[1])\n", "    index.add(embs)\n", "    return index\n", "\n", "def search_faiss(index: faiss.IndexFlatIP, query_vec: np.ndarray, k: int = 5):\n", "    D, I = index.search(query_vec, k)\n", "    return D, I\n", "\n", "# --- Persistence (Drive optional) ---\n", "def mount_drive() -> str:\n", "    if not IN_COLAB:\n", "        return \"\"\n", "    colab_drive.mount('/content/drive', force_remount=False)\n", "    save_dir = '/content/drive/MyDrive/RAG_MVP_Phase1'\n", "    os.makedirs(save_dir, exist_ok=True)\n", "    return save_dir\n", "\n", "def save_state(save_dir: str):\n", "    os.makedirs(save_dir, exist_ok=True)\n", "    state_path = os.path.join(save_dir, 'documents_state.pkl')\n", "    with open(state_path, 'wb') as f:\n", "        pickle.dump(DOCUMENTS, f)\n", "    return state_path\n", "\n", "def load_state(save_dir: str):\n", "    state_path = os.path.join(save_dir, 'documents_state.pkl')\n", "    if not os.path.exists(state_path):\n", "        raise FileNotFoundError(f\"No saved state at {state_path}\")\n", "    with open(state_path, 'rb') as f:\n", "        loaded = pickle.load(f)\n", "    DOCUMENTS.clear()\n", "    DOCUMENTS.update(loaded)\n", "    return True\n", "\n", "# --- Retrieval + Answering ---\n", "def ingest_files(api_key: str, embed_model: str, files) -> Dict:\n", "    client = build_openai_client(api_key)\n", "    added = []\n", "    for f in files:\n", "        meta = load_file(f)\n", "        text = meta[\"text\"]\n", "        name = meta[\"name\"]\n", "        chunks = chunk_text(text, CHUNK_SIZE, CHUNK_OVERLAP)\n", "        if not chunks:\n", "            continue\n", "        embs = embed_texts(client, chunks, model=embed_model)\n", "        embs = normalize(embs)\n", "        index = build_faiss_index(embs)\n", "        doc_id = f\"doc_{int(time.time()*1000)}_{len(DOCUMENTS)+1}\"\n", "        DOCUMENTS[doc_id] = {\n", "            \"name\": name,\n", "            \"chunks\": chunks,\n", "            \"embs\": embs,\n", "            \"index\": index,\n", "        }\n", "        added.append({\"doc_id\": doc_id, \"name\": name, \"chunks\": len(chunks)})\n", "    return {\n", "        \"message\": f\"Ingested {len(added)} file(s).\",\n", "        \"docs\": [{\"doc_id\": k, \"name\": v[\"name\"], \"chunks\": len(v[\"chunks\"]) } for k, v in DOCUMENTS.items()]\n", "    }\n", "\n", "def assemble_subset_index(selected_doc_ids: List[str]):\n", "    texts, metas, all_embs = [], [], []\n", "    for did in selected_doc_ids:\n", "        rec = DOCUMENTS.get(did)\n", "        if not rec:\n", "            continue\n", "        k = len(rec[\"chunks\"])\n", "        texts.extend(rec[\"chunks\"])\n", "        metas.extend([(did, i) for i in range(k)])\n", "        all_embs.append(rec[\"embs\"])\n", "    if not all_embs:\n", "        return None, None, None\n", "    embs = np.vstack(all_embs)\n", "    index = build_faiss_index(embs)\n", "    return index, texts, metas\n", "\n", "def retrieve(api_key: str, query: str, embed_model: str, selected_doc_ids: List[str], k: int = 5):\n", "    client = build_openai_client(api_key)\n", "    index, texts, metas = assemble_subset_index(selected_doc_ids)\n", "    if index is None:\n", "        return []\n", "    q_vec = client.embeddings.create(model=embed_model, input=[query]).data[0].embedding\n", "    q_vec = np.array(q_vec, dtype=np.float32)[None, :]\n", "    q_vec = normalize(q_vec)\n", "    D, I = search_faiss(index, q_vec, k)\n", "    hits = []\n", "    for rank, idx in enumerate(I[0].tolist()):\n", "        sim = float(D[0][rank])\n", "        did, chunk_idx = metas[idx]\n", "        doc = DOCUMENTS[did]\n", "        hits.append({\n", "            \"rank\": rank+1,\n", "            \"doc_id\": did,\n", "            \"doc_name\": doc[\"name\"],\n", "            \"chunk_idx\": chunk_idx,\n", "            \"score\": sim,\n", "            \"text\": texts[idx]\n", "        })\n", "    return hits\n", "\n", "def make_context_with_citations(hits: List[Dict], max_chars: int = 4000) -> str:\n", "    ctx_parts, citations, total = [], [], 0\n", "    for i, h in enumerate(hits, start=1):\n", "        chunk = h[\"text\"].strip().replace(\"\\n\", \" \")\n", "        prefix = f\"[Source {i}: {h['doc_name']} \u00b7 chunk {h['chunk_idx']}]\\n\"\n", "        part = prefix + chunk + \"\\n\\n\"\n", "        if total + len(part) > max_chars:\n", "            break\n", "        ctx_parts.append(part)\n", "        citations.append(f\"[{i}] {h['doc_name']} (chunk {h['chunk_idx']})\")\n", "        total += len(part)\n", "    ctx = \"\".join(ctx_parts)\n", "    return ctx, citations\n", "\n", "def answer_with_rag(api_key: str, chat_model: str, query: str, hits: List[Dict]):\n", "    client = build_openai_client(api_key)\n", "    ctx, citations = make_context_with_citations(hits)\n", "    system_prompt = (\n", "        \"You are a helpful assistant. Use the provided sources to answer succinctly. \"\n", "        \"When you rely on a source, include bracketed reference numbers like [1], [2]. If the sources don't contain the answer, say so.\"\n", "    )\n", "    messages = [\n", "        {\"role\": \"system\", \"content\": system_prompt},\n", "        {\"role\": \"user\", \"content\": f\"Question: {query}\\n\\nSources:\\n{ctx}\"}\n", "    ]\n", "    resp = client.chat.completions.create(model=chat_model, messages=messages, temperature=0.2)\n", "    answer = resp.choices[0].message.content\n", "    if citations:\n", "        answer = answer + \"\\n\\nSources:\\n\" + \"\\n\".join(citations)\n", "    return answer\n", "\n", "# --- Gradio App (defined and launched in this cell) ---\n", "def ui_list_docs():\n", "    return [f\"{v['name']} \u2014 {k}\" for k, v in DOCUMENTS.items()]\n", "\n", "def _ids_from_labels(labels: List[str]) -> List[str]:\n", "    ids = []\n", "    for lab in labels or []:\n", "        if '\u2014' in lab:\n", "            ids.append(lab.split('\u2014')[-1].strip())\n", "    return ids\n", "\n", "def on_ingest(api_key, embed_model, files):\n", "    if not api_key:\n", "        return gr.update(value=\"Please enter your OpenAI API key.\"), gr.update(choices=ui_list_docs(), value=[])\n", "    if not files:\n", "        return gr.update(value=\"No files selected.\"), gr.update(choices=ui_list_docs(), value=[])\n", "    # quick API sanity check\n", "    try:\n", "        _ = OpenAI(api_key=api_key).embeddings.create(model=embed_model, input=[\"sanity check\"]).data[0].embedding\n", "    except Exception as e:\n", "        return f\"OpenAI key/model check failed: {type(e).__name__}: {e}\", gr.update(choices=ui_list_docs(), value=[])\n", "    try:\n", "        res = ingest_files(api_key, embed_model, files)\n", "        msg = res[\"message\"] + \"\\n\" + json.dumps(res[\"docs\"], indent=2)\n", "        return msg, gr.update(choices=ui_list_docs(), value=ui_list_docs())\n", "    except Exception as e:\n", "        tb = traceback.format_exc()\n", "        return f\"Ingest failed: {type(e).__name__}: {e}\\n\\nTraceback:\\n{tb}\", gr.update(choices=ui_list_docs(), value=[])\n", "\n", "def on_ask(api_key, chat_model, embed_model, query, selected_labels, top_k):\n", "    try:\n", "        selected_ids = _ids_from_labels(selected_labels)\n", "        if not selected_ids:\n", "            return \"Please select at least one ingested file.\"\n", "        hits = retrieve(api_key, query, embed_model, selected_ids, k=top_k)\n", "        if not hits:\n", "            return \"No results found. Try ingesting files or broadening your question.\"\n", "        answer = answer_with_rag(api_key, chat_model, query, hits)\n", "        return answer\n", "    except Exception as e:\n", "        tb = traceback.format_exc()\n", "        return f\"Error: {type(e).__name__}: {e}\\n\\nTraceback:\\n{tb}\"\n", "\n", "def on_mount_drive():\n", "    if not IN_COLAB:\n", "        return \"This action is only available in Google Colab.\", \"\"\n", "    save_dir = mount_drive()\n", "    return f\"Drive mounted. Save dir: {save_dir}\", save_dir\n", "\n", "def on_save(save_dir):\n", "    if not save_dir:\n", "        return \"Provide a Google Drive folder path first.\"\n", "    path = save_state(save_dir)\n", "    return f\"Saved state to: {path}\"\n", "\n", "def on_load(save_dir):\n", "    try:\n", "        load_state(save_dir)\n", "        return f\"Loaded state from: {save_dir}\", gr.update(choices=ui_list_docs(), value=ui_list_docs())\n", "    except Exception as e:\n", "        return f\"Load failed: {e}\", gr.update()\n", "\n", "with gr.Blocks(title=\"Phase 1 \u2013 RAG MVP\") as demo:\n", "    gr.Markdown(\"\"\"\n", "    # Phase 1 \u2013 RAG MVP\n", "    **Bring Your Own OpenAI Key**. Ingest PDF/DOCX/TXT \u2192 chunk + embed \u2192 FAISS \u2192 ask with citations.\n", "    \"\"\")\n", "\n", "    with gr.Row():\n", "        api_key = gr.Textbox(label=\"OpenAI API Key\", type=\"password\", placeholder=\"sk-...\", show_label=True)\n", "        chat_model = gr.Dropdown(choices=[\"gpt-4o-mini\", \"gpt-4o\", \"gpt-4.1-mini\"], value=CHAT_MODEL_DEFAULT, label=\"Chat Model\")\n", "        embed_model = gr.Dropdown(choices=[\"text-embedding-3-small\", \"text-embedding-3-large\"], value=EMBED_MODEL_DEFAULT, label=\"Embedding Model\")\n", "\n", "    # Shared document selector across tabs\n", "    doc_selector = gr.CheckboxGroup(label=\"Available Documents (shared)\", choices=[])\n", "\n", "    with gr.Tab(\"Ingest\"):\n", "        files = gr.File(label=\"Upload files (PDF, DOCX, TXT)\", file_count=\"multiple\", type=\"filepath\", file_types=[\".pdf\", \".docx\", \".txt\"])\n", "        ingest_btn = gr.Button(\"Ingest\")\n", "        ingest_log = gr.Textbox(label=\"Ingestion Log\", lines=10)\n", "        ingest_btn.click(on_ingest, inputs=[api_key, embed_model, files], outputs=[ingest_log, doc_selector])\n", "\n", "    with gr.Tab(\"Ask\"):\n", "        with gr.Row():\n", "            query = gr.Textbox(label=\"Your question\", placeholder=\"Ask me about your documents...\", lines=3)\n", "        with gr.Row():\n", "            top_k = gr.Slider(1, 10, value=5, step=1, label=\"Top-K Chunks\")\n", "        ask_btn = gr.Button(\"Ask\")\n", "        answer_out = gr.Markdown()\n", "        ask_btn.click(on_ask, inputs=[api_key, chat_model, embed_model, query, doc_selector, top_k], outputs=[answer_out,])\n", "\n", "    with gr.Tab(\"Google Drive (Optional)\"):\n", "        drive_status = gr.Textbox(label=\"Status\")\n", "        save_dir = gr.Textbox(label=\"Save folder (e.g., /content/drive/MyDrive/RAG_MVP_Phase1)\")\n", "        with gr.Row():\n", "            mount_btn = gr.Button(\"Mount Drive (Colab)\")\n", "            mount_btn.click(on_mount_drive, inputs=[], outputs=[drive_status, save_dir])\n", "        with gr.Row():\n", "            save_btn = gr.Button(\"Save State\")\n", "            load_btn = gr.Button(\"Load State\")\n", "        save_btn.click(on_save, inputs=[save_dir], outputs=[drive_status])\n", "        load_btn.click(on_load, inputs=[save_dir], outputs=[drive_status, doc_selector])\n", "\n", "    gr.Markdown(\"Built for fast iteration. \u26a1\ufe0f\")\n", "\n", "# Launch immediately to avoid ordering issues\n", "demo.launch()\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.10"}, "colab": {"name": "Phase1_RAG_MVP_clean_v5.ipynb"}}, "nbformat": 4, "nbformat_minor": 5}