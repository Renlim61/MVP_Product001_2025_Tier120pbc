{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Renlim61/MVP_Product001_2025_Tier120pbc/blob/main/Phase1_RAG_MVP_clean_v6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlRoM38Emjcu"
      },
      "source": [
        "# Phase 1 – GenAI RAG MVP (Colab, Clean v6) This version ran successfuly.\n",
        "\n",
        "**Goal:** Single-tenant RAG assistant (BYO OpenAI key). Ingest PDF/DOCX/TXT → chunk + embed → search with FAISS → answer with citations.\n",
        "\n",
        "### Highlights\n",
        "- Upload **.pdf / .docx / .txt**\n",
        "- OpenAI **embeddings** (fallback to large if small not available)\n",
        "- **FAISS** cosine similarity\n",
        "- **Gradio UI** with per-prompt document selection\n",
        "- **Optional**: Save/Load state to Google Drive\n",
        "\n",
        "**Instructions:** Run the next cell, paste your OpenAI key in the UI, upload files, click **Ingest**, then ask questions under **Ask**.\n"
      ],
      "id": "hlRoM38Emjcu"
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 0 - INSTALL & IMPORTS\n",
        "# %%capture\n",
        "!pip -q install openai gradio faiss-cpu pypdf python-docx\n",
        "\n",
        "import os, io, json, pickle, time, traceback\n",
        "import numpy as np\n",
        "from typing import List, Dict, Any\n",
        "import gradio as gr\n",
        "from pypdf import PdfReader\n",
        "from docx import Document as DocxDocument\n",
        "import faiss\n",
        "from dataclasses import dataclass\n",
        "\n",
        "try:\n",
        "    from google.colab import drive as colab_drive\n",
        "    IN_COLAB = True\n",
        "except Exception:\n",
        "    IN_COLAB = False\n",
        "\n",
        "try:\n",
        "    from openai import OpenAI\n",
        "except Exception:\n",
        "    raise RuntimeError(\"The 'openai' package failed to import. Make sure the install cell ran successfully.\")"
      ],
      "metadata": {
        "id": "tBPiXZTjxsRu"
      },
      "id": "tBPiXZTjxsRu",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1 — Config & Globals\n",
        "# --- Config ---\n",
        "EMBED_MODEL_DEFAULT = \"text-embedding-3-small\"\n",
        "CHAT_MODEL_DEFAULT = \"gpt-4o-mini\"\n",
        "CHUNK_SIZE = 1200\n",
        "CHUNK_OVERLAP = 200\n",
        "\n",
        "# In-memory documents\n",
        "DOCUMENTS: Dict[str, Dict[str, Any]] = {}\n",
        "\n",
        "def build_openai_client(api_key: str):\n",
        "    if not api_key:\n",
        "        raise ValueError(\"OpenAI API key is required.\")\n",
        "    os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "    return OpenAI(api_key=api_key)"
      ],
      "metadata": {
        "id": "bNrY_ElAyPCQ"
      },
      "id": "bNrY_ElAyPCQ",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 2 — Readers (PDF/DOCX/TXT)\n",
        "# --- Readers ---\n",
        "def _read_pdf(file_bytes: bytes) -> str:\n",
        "    reader = PdfReader(io.BytesIO(file_bytes))\n",
        "    texts = []\n",
        "    for page in reader.pages:\n",
        "        try:\n",
        "            texts.append(page.extract_text() or \"\")\n",
        "        except Exception:\n",
        "            texts.append(\"\")\n",
        "    return \"\\n\".join(texts)\n",
        "\n",
        "def _read_docx(file_bytes: bytes) -> str:\n",
        "    bio = io.BytesIO(file_bytes)\n",
        "    doc = DocxDocument(bio)\n",
        "    return \"\\n\".join(p.text for p in doc.paragraphs)\n",
        "\n",
        "def _read_txt(file_bytes: bytes) -> str:\n",
        "    try:\n",
        "        return file_bytes.decode(\"utf-8\")\n",
        "    except Exception:\n",
        "        return file_bytes.decode(\"latin-1\", errors=\"ignore\")\n",
        "\n",
        "# Page-aware PDF reader (returns list of {\"page\": int, \"text\": str})\n",
        "def _read_pdf_pages(file_bytes: bytes):\n",
        "    reader = PdfReader(io.BytesIO(file_bytes))\n",
        "    pages = []\n",
        "    for i, page in enumerate(reader.pages, start=1):\n",
        "        try:\n",
        "            text = page.extract_text() or \"\"\n",
        "        except Exception:\n",
        "            text = \"\"\n",
        "        pages.append({\"page\": i, \"text\": text})\n",
        "    return pages\n",
        "\n",
        "\n",
        "def load_file(file_obj) -> Dict[str, Any]:\n",
        "    \"\"\"Return dict with keys: name, text, filetype, pages? (for PDFs).\"\"\"\n",
        "    if isinstance(file_obj, str):\n",
        "        path = file_obj\n",
        "        name = os.path.basename(path)\n",
        "        with open(path, 'rb') as f:\n",
        "            content = f.read()\n",
        "    else:\n",
        "        name = getattr(file_obj, 'orig_name', None) or getattr(file_obj, 'name', 'uploaded_file')\n",
        "        if hasattr(file_obj, 'read'):\n",
        "            content = file_obj.read()\n",
        "        else:\n",
        "            path = getattr(file_obj, 'path', None)\n",
        "            if not path:\n",
        "                raise ValueError(\"Unsupported file object received from Gradio upload.\")\n",
        "            with open(path, 'rb') as f:\n",
        "                content = f.read()\n",
        "\n",
        "    if not content:\n",
        "        raise ValueError(f\"{name}: file is empty.\")\n",
        "\n",
        "    lower = name.lower()\n",
        "    meta = {\"name\": os.path.basename(name)}\n",
        "\n",
        "    if lower.endswith('.pdf'):\n",
        "        pages = _read_pdf_pages(content)\n",
        "        full_text = \"\\n\".join(p[\"text\"] for p in pages)\n",
        "        meta.update({\"text\": (full_text or \"\").strip(), \"filetype\": \"pdf\", \"pages\": pages})\n",
        "    elif lower.endswith('.docx'):\n",
        "        text = _read_docx(content)\n",
        "        meta.update({\"text\": (text or \"\").strip(), \"filetype\": \"docx\"})\n",
        "    elif lower.endswith('.txt'):\n",
        "        text = _read_txt(content)\n",
        "        meta.update({\"text\": (text or \"\").strip(), \"filetype\": \"txt\"})\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported file type for {name}. Use PDF/DOCX/TXT.\")\n",
        "\n",
        "    if not meta[\"text\"]:\n",
        "        raise ValueError(f\"{meta['name']}: no extractable text found (scanned PDF or empty file?).\")\n",
        "\n",
        "    return meta"
      ],
      "metadata": {
        "id": "p4BL2kxdydqq"
      },
      "id": "p4BL2kxdydqq",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 3 — Chunking\n",
        "# --- Chunking ---\n",
        "def chunk_text(text: str, chunk_size: int = CHUNK_SIZE, overlap: int = CHUNK_OVERLAP) -> List[str]:\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    n = len(text)\n",
        "    while start < n:\n",
        "        end = min(start + chunk_size, n)\n",
        "        chunk = text[start:end]\n",
        "        chunks.append(chunk)\n",
        "        if end == n:\n",
        "            break\n",
        "        start = end - overlap\n",
        "        if start < 0:\n",
        "            start = 0\n",
        "    return [c.strip() for c in chunks if c.strip()]"
      ],
      "metadata": {
        "id": "vB0P-5Tty_po"
      },
      "id": "vB0P-5Tty_po",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 4 — Embeddings\n",
        "# --- Embeddings ---\n",
        "def embed_texts(client: OpenAI, texts: List[str], model: str = EMBED_MODEL_DEFAULT, batch_size: int = 128) -> np.ndarray:\n",
        "    vectors = []\n",
        "    def _call(batch, mdl):\n",
        "        return client.embeddings.create(model=mdl, input=batch)\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        try:\n",
        "            resp = _call(batch, model)\n",
        "        except Exception as e1:\n",
        "            if model == \"text-embedding-3-small\":\n",
        "                try:\n",
        "                    resp = _call(batch, \"text-embedding-3-large\")\n",
        "                except Exception as e2:\n",
        "                    raise RuntimeError(f\"Embedding failed on both models: small-> {e1}; large-> {e2}\")\n",
        "            else:\n",
        "                raise\n",
        "        for d in resp.data:\n",
        "            vectors.append(d.embedding)\n",
        "    return np.array(vectors, dtype=np.float32)\n",
        "\n",
        "def normalize(vecs: np.ndarray) -> np.ndarray:\n",
        "    norms = np.linalg.norm(vecs, axis=1, keepdims=True) + 1e-12\n",
        "    return vecs / norms\n"
      ],
      "metadata": {
        "id": "xUvHIDmCzOyF"
      },
      "id": "xUvHIDmCzOyF",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CELL 5 — FAISS (Index/Search)\n",
        "# --- FAISS ---\n",
        "def build_faiss_index(embs: np.ndarray) -> faiss.IndexFlatIP:\n",
        "    index = faiss.IndexFlatIP(embs.shape[1])\n",
        "    index.add(embs)\n",
        "    return index\n",
        "\n",
        "def search_faiss(index: faiss.IndexFlatIP, query_vec: np.ndarray, k: int = 5):\n",
        "    D, I = index.search(query_vec, k)\n",
        "    return D, I"
      ],
      "metadata": {
        "id": "AZiIYUR3zbG9"
      },
      "id": "AZiIYUR3zbG9",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cell 6 — Persistence (Optional: Google Drive)\n",
        "# --- Persistence (Drive optional) ---\n",
        "def mount_drive() -> str:\n",
        "    if not IN_COLAB:\n",
        "        return \"\"\n",
        "    colab_drive.mount('/content/drive', force_remount=False)\n",
        "    save_dir = '/content/drive/MyDrive/RAG_MVP_Phase1'\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    return save_dir\n",
        "\n",
        "def save_state(save_dir: str):\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    state_path = os.path.join(save_dir, 'documents_state.pkl')\n",
        "    with open(state_path, 'wb') as f:\n",
        "        pickle.dump(DOCUMENTS, f)\n",
        "    return state_path\n",
        "\n",
        "def load_state(save_dir: str):\n",
        "    state_path = os.path.join(save_dir, 'documents_state.pkl')\n",
        "    if not os.path.exists(state_path):\n",
        "        raise FileNotFoundError(f\"No saved state at {state_path}\")\n",
        "    with open(state_path, 'rb') as f:\n",
        "        loaded = pickle.load(f)\n",
        "    DOCUMENTS.clear()\n",
        "    DOCUMENTS.update(loaded)\n",
        "    return True"
      ],
      "metadata": {
        "id": "vc9LXjMxzprt"
      },
      "id": "vc9LXjMxzprt",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 7 - Ingestion\n",
        "def ingest_files(api_key: str, embed_model: str, files) -> Dict:\n",
        "    # quick API sanity check\n",
        "    try:\n",
        "        _ = OpenAI(api_key=api_key).embeddings.create(model=embed_model, input=[\"sanity check\"]).data[0].embedding\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"OpenAI key/model check failed: {type(e).__name__}: {e}\")\n",
        "\n",
        "    client = build_openai_client(api_key)\n",
        "    added = []\n",
        "\n",
        "    for f in files:\n",
        "        try:\n",
        "            meta = load_file(f)  # {name, text, filetype, pages?}\n",
        "            name = meta[\"name\"]\n",
        "            filetype = meta.get(\"filetype\", \"txt\")\n",
        "\n",
        "            all_chunks = []\n",
        "            all_metas = []  # each item: {\"page\": int|None}\n",
        "\n",
        "            if filetype == \"pdf\" and \"pages\" in meta:\n",
        "                # page-aware chunking\n",
        "                for page_entry in meta[\"pages\"]:\n",
        "                    page_no = page_entry[\"page\"]\n",
        "                    page_text = (page_entry[\"text\"] or \"\").strip()\n",
        "                    if not page_text:\n",
        "                        continue\n",
        "                    page_chunks = chunk_text(page_text, CHUNK_SIZE, CHUNK_OVERLAP)\n",
        "                    for ch in page_chunks:\n",
        "                        all_chunks.append(ch)\n",
        "                        all_metas.append({\"page\": page_no})\n",
        "            else:\n",
        "                # docx/txt\n",
        "                text = meta[\"text\"]\n",
        "                chunks = chunk_text(text, CHUNK_SIZE, CHUNK_OVERLAP)\n",
        "                for ch in chunks:\n",
        "                    all_chunks.append(ch)\n",
        "                    all_metas.append({\"page\": None})\n",
        "\n",
        "            if not all_chunks:\n",
        "                continue\n",
        "\n",
        "            embs = embed_texts(client, all_chunks, model=embed_model)\n",
        "            embs = normalize(embs)\n",
        "            index = build_faiss_index(embs)\n",
        "\n",
        "            doc_id = f\"doc_{int(time.time()*1000)}_{len(DOCUMENTS)+1}\"\n",
        "            DOCUMENTS[doc_id] = {\n",
        "                \"name\": name,\n",
        "                \"filetype\": filetype,\n",
        "                \"chunks\": all_chunks,\n",
        "                \"meta\": all_metas,  # <-- page info here\n",
        "                \"embs\": embs,\n",
        "                \"index\": index,\n",
        "            }\n",
        "            added.append({\"doc_id\": doc_id, \"name\": name, \"chunks\": len(all_chunks)})\n",
        "        except Exception as e:\n",
        "            print(f\"[ingest warning] {type(e).__name__}: {e}\\n\", traceback.format_exc())\n",
        "\n",
        "    return {\n",
        "        \"message\": f\"Ingested {len(added)} file(s).\",\n",
        "        \"docs\": [{\"doc_id\": k, \"name\": v[\"name\"], \"chunks\": len(v[\"chunks\"])} for k, v in DOCUMENTS.items()]\n",
        "    }\n"
      ],
      "metadata": {
        "id": "oYBBTQbr0Bur"
      },
      "id": "oYBBTQbr0Bur",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 8 - Retrieval + Answering\n",
        "def assemble_subset_index(selected_doc_ids: List[str]):\n",
        "    texts, metas, all_embs = [], [], []\n",
        "    for did in selected_doc_ids:\n",
        "        rec = DOCUMENTS.get(did)\n",
        "        if not rec:\n",
        "            continue\n",
        "        k = len(rec[\"chunks\"])\n",
        "        texts.extend(rec[\"chunks\"])\n",
        "        # keep (doc_id, chunk_idx, page)\n",
        "        metas.extend([(did, i, rec.get(\"meta\", [{}]*k)[i].get(\"page\")) for i in range(k)])\n",
        "        all_embs.append(rec[\"embs\"])\n",
        "    if not all_embs:\n",
        "        return None, None, None\n",
        "    embs = np.vstack(all_embs)\n",
        "    index = build_faiss_index(embs)\n",
        "    return index, texts, metas\n",
        "\n",
        "def retrieve(api_key: str, query: str, embed_model: str, selected_doc_ids: List[str], k: int = 5):\n",
        "    client = build_openai_client(api_key)\n",
        "    index, texts, metas = assemble_subset_index(selected_doc_ids)\n",
        "    if index is None:\n",
        "        return []\n",
        "    q_vec = client.embeddings.create(model=embed_model, input=[query]).data[0].embedding\n",
        "    q_vec = np.array(q_vec, dtype=np.float32)[None, :]\n",
        "    q_vec = normalize(q_vec)\n",
        "    D, I = search_faiss(index, q_vec, k)\n",
        "    hits = []\n",
        "    for rank, idx in enumerate(I[0].tolist()):\n",
        "        sim = float(D[0][rank])\n",
        "        did, chunk_idx, page_no = metas[idx]\n",
        "        doc = DOCUMENTS[did]\n",
        "        hits.append({\n",
        "            \"rank\": rank+1,\n",
        "            \"doc_id\": did,\n",
        "            \"doc_name\": doc[\"name\"],\n",
        "            \"chunk_idx\": chunk_idx,\n",
        "            \"page\": page_no,\n",
        "            \"score\": sim,\n",
        "            \"text\": texts[idx]\n",
        "        })\n",
        "    return hits\n",
        "\n",
        "def make_context_with_citations(hits: List[Dict], max_chars: int = 4000) -> str:\n",
        "    ctx_parts, citations, total = [], [], 0\n",
        "    for i, h in enumerate(hits, start=1):\n",
        "        chunk = (h[\"text\"] or \"\").strip().replace(\"\\n\", \" \")\n",
        "        page_str = f\" · p.{h['page']}\" if h.get(\"page\") else \"\"\n",
        "        prefix = f\"[Source {i}: {h['doc_name']}{page_str} · chunk {h['chunk_idx']}]\\n\"\n",
        "        part = prefix + chunk + \"\\n\\n\"\n",
        "        if total + len(part) > max_chars:\n",
        "            break\n",
        "        ctx_parts.append(part)\n",
        "        if h.get(\"page\"):\n",
        "            citations.append(f\"[{i}] {h['doc_name']} (p.{h['page']}, chunk {h['chunk_idx']})\")\n",
        "        else:\n",
        "            citations.append(f\"[{i}] {h['doc_name']} (chunk {h['chunk_idx']})\")\n",
        "        total += len(part)\n",
        "    ctx = \"\".join(ctx_parts)\n",
        "    return ctx, citations\n",
        "\n",
        "def answer_with_rag(api_key: str, chat_model: str, query: str, hits: List[Dict]):\n",
        "    client = build_openai_client(api_key)\n",
        "    ctx, citations = make_context_with_citations(hits)\n",
        "    system_prompt = (\n",
        "        \"You are a helpful assistant. Use the provided sources to answer succinctly. \"\n",
        "        \"When you rely on a source, include bracketed reference numbers like [1], [2]. If the sources don't contain the answer, say so.\"\n",
        "    )\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": f\"Question: {query}\\n\\nSources:\\n{ctx}\"}\n",
        "    ]\n",
        "    resp = client.chat.completions.create(model=chat_model, messages=messages, temperature=0.2)\n",
        "    answer = resp.choices[0].message.content\n",
        "    if citations:\n",
        "        answer = answer + \"\\n\\nSources:\\n\" + \"\\n\".join(citations)\n",
        "    return answer"
      ],
      "metadata": {
        "id": "Hjdkj8e51v5K"
      },
      "id": "Hjdkj8e51v5K",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CELL 9 - Gradio UI & Launch\n",
        "# --- Gradio App (defined and launched in this cell) ---\n",
        "def ui_list_docs():\n",
        "    return [f\"{v['name']} — {k}\" for k, v in DOCUMENTS.items()]\n",
        "\n",
        "def _ids_from_labels(labels: List[str]) -> List[str]:\n",
        "    ids = []\n",
        "    for lab in labels or []:\n",
        "        if '—' in lab:\n",
        "            ids.append(lab.split('—')[-1].strip())\n",
        "    return ids\n",
        "\n",
        "def on_ingest(api_key, embed_model, files):\n",
        "    if not api_key:\n",
        "        return gr.update(value=\"Please enter your OpenAI API key.\"), gr.update(choices=ui_list_docs(), value=[])\n",
        "    if not files:\n",
        "        return gr.update(value=\"No files selected.\"), gr.update(choices=ui_list_docs(), value=[])\n",
        "    # quick API sanity check\n",
        "    try:\n",
        "        _ = OpenAI(api_key=api_key).embeddings.create(model=embed_model, input=[\"sanity check\"]).data[0].embedding\n",
        "    except Exception as e:\n",
        "        return f\"OpenAI key/model check failed: {type(e).__name__}: {e}\", gr.update(choices=ui_list_docs(), value=[])\n",
        "    try:\n",
        "        res = ingest_files(api_key, embed_model, files)\n",
        "        msg = res[\"message\"] + \"\\n\" + json.dumps(res[\"docs\"], indent=2)\n",
        "        return msg, gr.update(choices=ui_list_docs(), value=ui_list_docs())\n",
        "    except Exception as e:\n",
        "        tb = traceback.format_exc()\n",
        "        return f\"Ingest failed: {type(e).__name__}: {e}\\n\\nTraceback:\\n{tb}\", gr.update(choices=ui_list_docs(), value=[])\n",
        "\n",
        "def on_ask(api_key, chat_model, embed_model, query, selected_labels, top_k):\n",
        "    try:\n",
        "        selected_ids = _ids_from_labels(selected_labels)\n",
        "        if not selected_ids:\n",
        "            return \"Please select at least one ingested file.\"\n",
        "        hits = retrieve(api_key, query, embed_model, selected_ids, k=top_k)\n",
        "        if not hits:\n",
        "            return \"No results found. Try ingesting files or broadening your question.\"\n",
        "        answer = answer_with_rag(api_key, chat_model, query, hits)\n",
        "        return answer\n",
        "    except Exception as e:\n",
        "        tb = traceback.format_exc()\n",
        "        return f\"Error: {type(e).__name__}: {e}\\n\\nTraceback:\\n{tb}\"\n",
        "\n",
        "def on_mount_drive():\n",
        "    if not IN_COLAB:\n",
        "        return \"This action is only available in Google Colab.\", \"\"\n",
        "    save_dir = mount_drive()\n",
        "    return f\"Drive mounted. Save dir: {save_dir}\", save_dir\n",
        "\n",
        "def on_save(save_dir):\n",
        "    if not save_dir:\n",
        "        return \"Provide a Google Drive folder path first.\"\n",
        "    path = save_state(save_dir)\n",
        "    return f\"Saved state to: {path}\"\n",
        "\n",
        "def on_load(save_dir):\n",
        "    try:\n",
        "        load_state(save_dir)\n",
        "        return f\"Loaded state from: {save_dir}\", gr.update(choices=ui_list_docs(), value=ui_list_docs())\n",
        "    except Exception as e:\n",
        "        return f\"Load failed: {e}\", gr.update()\n",
        "\n",
        "with gr.Blocks(title=\"Phase 1 – RAG MVP\") as demo:\n",
        "    gr.Markdown(\"\"\"\n",
        "    # Phase 1 – RAG MVP\n",
        "    **Bring Your Own OpenAI Key**. Ingest PDF/DOCX/TXT → chunk + embed → FAISS → ask with citations.\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Row():\n",
        "        api_key = gr.Textbox(label=\"OpenAI API Key\", type=\"password\", placeholder=\"sk-...\", show_label=True)\n",
        "        chat_model = gr.Dropdown(choices=[\"gpt-4o-mini\", \"gpt-4o\", \"gpt-4.1-mini\"], value=CHAT_MODEL_DEFAULT, label=\"Chat Model\")\n",
        "        embed_model = gr.Dropdown(choices=[\"text-embedding-3-small\", \"text-embedding-3-large\"], value=EMBED_MODEL_DEFAULT, label=\"Embedding Model\")\n",
        "\n",
        "    # Shared document selector across tabs\n",
        "    doc_selector = gr.CheckboxGroup(label=\"Available Documents (shared)\", choices=[])\n",
        "\n",
        "    with gr.Tab(\"Ingest\"):\n",
        "        files = gr.File(label=\"Upload files (PDF, DOCX, TXT)\", file_count=\"multiple\", type=\"filepath\", file_types=[\".pdf\", \".docx\", \".txt\"])\n",
        "        ingest_btn = gr.Button(\"Ingest\")\n",
        "        ingest_log = gr.Textbox(label=\"Ingestion Log\", lines=10)\n",
        "        ingest_btn.click(on_ingest, inputs=[api_key, embed_model, files], outputs=[ingest_log, doc_selector])\n",
        "\n",
        "    with gr.Tab(\"Ask\"):\n",
        "        with gr.Row():\n",
        "            query = gr.Textbox(label=\"Your question\", placeholder=\"Ask me about your documents...\", lines=3)\n",
        "        with gr.Row():\n",
        "            top_k = gr.Slider(1, 10, value=5, step=1, label=\"Top-K Chunks\")\n",
        "        ask_btn = gr.Button(\"Ask\")\n",
        "        answer_out = gr.Markdown()\n",
        "        ask_btn.click(on_ask, inputs=[api_key, chat_model, embed_model, query, doc_selector, top_k], outputs=[answer_out,])\n",
        "\n",
        "    with gr.Tab(\"Google Drive (Optional)\"):\n",
        "        drive_status = gr.Textbox(label=\"Status\")\n",
        "        save_dir = gr.Textbox(label=\"Save folder (e.g., /content/drive/MyDrive/RAG_MVP_Phase1)\")\n",
        "        with gr.Row():\n",
        "            mount_btn = gr.Button(\"Mount Drive (Colab)\")\n",
        "            mount_btn.click(on_mount_drive, inputs=[], outputs=[drive_status, save_dir])\n",
        "        with gr.Row():\n",
        "            save_btn = gr.Button(\"Save State\")\n",
        "            load_btn = gr.Button(\"Load State\")\n",
        "        save_btn.click(on_save, inputs=[save_dir], outputs=[drive_status])\n",
        "        load_btn.click(on_load, inputs=[save_dir], outputs=[drive_status, doc_selector])\n",
        "\n",
        "    gr.Markdown(\"Built for fast iteration. ⚡️\")\n",
        "\n",
        "# Launch immediately to avoid ordering issues\n",
        "demo.launch()"
      ],
      "metadata": {
        "id": "M2lV1ypi2FGW",
        "outputId": "9e45fede-13f3-428a-bd2e-e22243c9f9d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        }
      },
      "id": "M2lV1ypi2FGW",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://06498520080b700987.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://06498520080b700987.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}